,title,summary,url,date,article_content
0,Tencent gains approval to sell mutual funds to WeChat users,"Tencent has been granted a licence from the China Securities Regulatory Commission to sell third-party mutual funds directly. The development will allow the company to make better use of its popular messaging app WeChat to sell investment products, analysts have noted. WeChat has close to one billion users. The firm will now have more control over wealth management services offered via WeChat, rather than relying on third-party partners, analysts added. It's also predicted Tencent could seek to turn its payment service, Tenpay, into a full financial service platform. The company previously obtained licences for mobile payments, insurance and micro-financing. 
",http://www.scmp.com/business/companies/article/2126876/tencent-granted-licence-sell-mutual-funds,2018-01-04 11:49:58.900000,Traditional finance houses now being seriously challenged in the wealth management market by emerging internet financial companies
1,"India testing blockchains in education, health, corruption fight","India is testing blockchain applications in education, health and agriculture, among other sectors of the economy, and is working on a proof-of-concept platform, according to an anonymous senior government official. Government think tank Niti Aayog co-hosted a blockchain hackathon alongside start-up Proffer in November. Reports the same month revealed the think tank was also developing a fraud-resistant transaction platform called IndiaChain, which is expected to be linked to national digital identification database IndiaStack.
",https://www.vccircle.com/niti-aayog-explores-blockchain-usage-in-education-health-and-agriculture/,2018-01-04 11:33:08.257000,"The Indian government's policy think tank, Niti Aayog, is testing waters to employ blockchain technology in education, health and agriculture, several media reports stated.

The top government think tank is developing a proof of concept to take advantage of the new technology in key sectors, a senior government official told The Economic Times on condition of anonymity.

The think tank along with blockchain startup Proffer, which was founded by MIT and Harvard graduates, held a blockchain hackathon from 10 November to 13 November 2017 at IIT Delhi, a report in YourStory said in November last year.

About 1,900 students from the IITs, MIT, Harvard University, UC Berkeley College of Engineering, and top engineering institutions around the world participated in the event. AgroChain, a blockchain-based marketplace that helps farmers and consumers through co-operative farming, bagged the first prize at the competition, the report added. The marketplace was developed by students from the Indian Institute of Information Technology and Management-Kerala (IIITM-K).

Niti Aayog has also been working on developing a country-wide blockchain network called IndiaChain which looks to reduce corruption and frauds, maximise transparency of transactions, report in November 2017 in technology news website Factor Daily had stated. The think tank is also expected to connect the blockchain infra to IndiaStack—the country's digital identification database, the report added.

Blockchain technology uses cryptographic tools to create an open and decentralised body of data, which can include banks transactions and the like. The data record can be verified by anyone involved in the transaction and information can be tracked via a secure network."
2,Higher living wage risks robot takeover of low-paid jobs: report,"The UK Institute for Fiscal Studies has warned that a higher National Living Wage brings with it the possibility of more jobs being automated, as employers look to keep costs down. The current £7.50 ($10.18) hourly rate is set to rise to £7.83 in April and could rise to £8.56 by 2020. IFS research economist Agnes Norris Keiller said there was a ""negligible impact on employment with the 2015 hourly rate of £6.70 and added: ""beyond some point, a higher minimum must start affecting employment, and we do not know where that point is"".
",https://news.sky.com/story/ifs-living-wage-increases-are-automation-threat-to-jobs-11194890,2018-01-04 11:32:10.223000,"Increases in minimum wage levels risk raising the pace of mechanisation in the workplace, according to the Institute for Fiscal Studies (IFS).

A report for the economic think tank urges ""extremely careful"" monitoring of wage rates over the coming years as statutory pay goes up and employers look to cut costs.

The National Living Wage, which applies to workers aged 25 and over, is set to rise to £7.83 per hour from April from a current rate of £7.50.

Current ambitions will see it hit 60% of median wages in 2020 - around £8.56 if forecasts prove accurate.

Please use Chrome browser for a more accessible video player Could robots replace human workers?

The IFS said those set to be brought within the minimum wage net in 2020 are more than twice as likely to be in the 10% most ""routine"" occupations as those who were directly affected by the minimum wage in 2015.

Advertisement

It said that leaves roles such as retail cashiers and receptionists at the mercy of potential automation. However, the report admitted the future was ""uncertain"" and that the loss of some jobs to automation could open other opportunities.

The report was released after a separate study in the summer warned that 15 million UK jobs ""could disappear due to technological disruption"".

IFS research economist Agnes Norris Keiller, who wrote the study, said: ""The fact that there seemed to be a negligible employment impact of a minimum at £6.70 per hour - the 2015 rate - does not mean that the same will be true of the rate of over £8.50 per hour that is set to apply in 2020.

""Beyond some point, a higher minimum must start affecting employment, and we do not know where that point is.""

She added: ""Meanwhile even higher rates, as proposed for example by the Labour Party, would bring even more employees in more automatable jobs into the minimum wage net.""

Shadow business secretary Rebecca Long Bailey said: ""Technological change, if harnessed effectively, could bring about immense benefits - transforming jobs and workplaces and driving up productivity and living standards.

""All workers should be paid a full and fair wage, which is why Labour has pledged to introduce a Real Living Wage of at least £10 an hour by 2020, as well as support for smaller businesses to pay it.

""Labour will invest in our country's future, new technologies, our businesses, our infrastructure and people. Higher wages, good jobs, greater investment in skills and technology to boost productivity and high employment all goes

together. They are complements, not trade-offs.""

A Government spokesman responded: ""The National Living Wage is creating a stronger economy and a fairer society, having delivered the fastest pay rise for the lowest earners in 20 years.

""But we also want to create highly-skilled, well-paid jobs for the future, backing innovation and supporting the development of new skills through our Industrial Strategy.

""That's why the Government is working with industry to ensure the benefits of new technologies are felt across different sectors and regions."""
3,Regus WeWork may seek stock-market flotation this year,"Co-working start-up WeWork may go public this year after its valuation rose to $21bn from $18bn in 2017. WeWork acquired a number of smaller companies in the past year, including Meetup and Spacemob, and also bought the former Lord & Taylor building on New York's Fifth Avenue for $850m to serve as its headquarters. Co-founder Adam Neumann confirmed last year that the company intends to go public, and with the stock market performing well for tech companies, 2018 appears a good time.
",https://www.fool.com/investing/2018/01/03/will-wework-stock-go-public-in-2018.aspx,2018-01-04 11:31:59.617000,"Depending on whom you ask, WeWork is either a brilliant company that is re-imagining office space and the modern workplace, or a glorified, overvalued real estate play with no sustainable competitive advantage.

One thing is clear: The start-up is making waves in the business world and real estate market as it now commands a valuation of $21 billion, up from $18 billion earlier last year, just eight years after its founding in 2010. It's received investments from the likes of Softbank, Fidelity Investments, and JPMorgan Chase along the way.

Last year was a big one for the co-working specialist as the company made a number of acquisitions, taking over Meetup, The Flatiron School, and Spacemob, among others, and making a splashy real estate play with its $850 million purchase of the Lord & Taylor flagship building on Manhattan's Fifth Avenue, which will become the company's new headquarters.

With WeWork's valuation north of $20 billion, revenue on track for over $1 billion last year, and a pedigree as a disruptor, it's not a surprise that investors would be anticipating the company's IPO (initial product offering). Co-founder Adam Neumann reiterated last summer that the company plans to go public.

Before we explore whether WeWork will debut on the market this year, let's take a look at how far the company has come.

What is WeWork?

Like Airbnb or Uber, WeWork is a company distinctly of the mobile and digital era. The internet has rewritten the norms of white-collar work, allowing those in industries like media, design, and tech to work from anywhere with an internet connection.

WeWork capitalized on this shift by creating workspaces that cater to freelancers, entrepreneurs, and small businesses looking for customizable office space and the community that a WeWork hub fosters. The company leases office space and redesigns it to attract millennials in the knowledge economy as well as others looking for office space.

Founders Neumann and Miguel McKelvey started a similar business in 2008 in Brooklyn called Green Desk, as an eco-friendly co-working space, which paved the way for WeWork after they sold Green Desk to their landlord in 2010. Making community the central focus, the pair opened the first WeWork in New York in April 2011. Today, WeWork is growing quickly, with its locations nearly tripling over the last year, and the company now has 275 locations across 59 cities around the world. WeWork is even experimenting with living spaces with WeLive, which has two locations, one in New York and one in Washington, D.C., and offers short- or long-term housing in community-focused microapartments based on the WeWork model.

Will 2018 be the year?

While the company has given no indication of when it would issue an IPO, there's good reason to think that WeWork could go public this year. The company said last year it would reach the key milestone of $1 billion in annual revenue, nearly doubling sales from a year ago. The stock market -- especially tech stocks -- is strong, as the Nasdaq jumped 28% last year and the valuation of the S&P 500 is as high as it's been since the tech bubble. Not all IPOs have been winners lately, as Snap Inc and Blue Apron were duds, but WeWork looks like the type of company -- the clear leader in a new industry and demonstrably profitable on a unit business -- that would generate excitement among investors. When it opens a new location, WeWork says it takes a loss for the first few months, but locations are generally profitable nine months after opening.

Having just scored a $4.4 billion investment from Softbank in August, WeWork may not necessarily need the cash that would come with an IPO, but the company has been expanding fast and spending money to boot with its $850 million purchase of the Lord & Taylor building and its $200 million acquisition of Meetup in November 2017. If the company seeks more such acquisitions, tapping the public markets may be its best option for a cash influx.

WeWork may also want to be mindful of the lesson in Uber's travails as a private company; the ride-hailing specialist may have waited too long to go public as it's now saddled with a slew of image problems after a number of scandals, not to mention regulatory challenges. WeWork's reputation in that area remains spotless for now, but it needs to avoid mistakes that would potentially damage that reputation.

With the market roaring, investors hungry for new tech IPOs, and a strong economy pushing WeWork's growth, 2018 could be a great year for WeWork stock to go public."
4,AMD poised to gain market share as Intel pounded by chip flaws,"Intel has seen more than $11bn wiped off its market value following a report by The Register of two vulnerabilities in its microprocessors that could lead malware to stored password data. In contrast, shares in rival Advanced Micro Devices (AMD) rose 10% after the company said there was a ""near zero risk of exploitation"" despite its chips being affected by one of the vulnerabilities. Computers using Intel chips from the past 10 years could be affected, including those running Microsoft Windows and Apple OS X. AMD now has a chance to eat into Intel's 99% share of the data-centre market.
",https://www.cnbc.com/2018/01/05/amd-is-big-winner-from-chip-flaw-fiasco.html,2018-01-04 11:17:13.277000,"AMD is big winner from chip flaw fiasco as more than $11 billion in Intel stock value is wiped out 11:43 AM ET Fri, 5 Jan 2018 | 00:43

Investors are piling into AMD shares and selling Intel stock after major chip security vulnerabilities were revealed earlier this week, and it totally makes sense.

Enterprises will likely diversify their chip security architecture risk for mission-critical applications by buying more AMD server chips. The company's ""architecture differences"" have proven immune to the more problematic one of the two disclosed vulnerabilities.

British tech website The Register reported Tuesday that some Intel processors have a ""fundamental design flaw"" and security issue, which spurred the company to confirm the problem later in the day.

AMD shares are up 10.4 percent in the two days through Thursday following the report, while Intel's stock declined 5.2 percent in the period, wiping out $11.3 billion of shareholder value.

One of the two vulnerabilities, called Meltdown, affects Intel processors. The other, named Spectre, could affect chips from Intel, AMD and Arm.

Intel said Wednesday that performance degradation after security updates for Meltdown ""should not be significant"" for the average user. But on a call with investors, the company admitted a decrease in performance of up to 30 percent was possible after fixes under some ""synthetic workloads.""

Bank of America Merrill Lynch told clients the big Intel performance hits were ""likely for enterprise and server workloads.""

On the flip side, AMD said any performance hits will be ""negligible"" after Spectre-related security software updates and there is ""near zero risk of exploitation."" The company also confirmed it is not affected by Meltdown due to processor ""architecture differences.""

Researchers and Apple said Spectre is more difficult to exploit.

Multiple Wall Street analysts predicted AMD will take advantage of the Intel's security issues.

AMD could use it as ""a marketing edge given differing architectures and no vulnerability yet,"" Mizuho Securities analyst Vijay Rakesh wrote in a note to clients Wednesday.

Intel's high-profit data-center business, which sells server chips to cloud computing providers and enterprises, is the chipmaker's crown jewel.

Rakesh noted that Intel had 99 percent market share of the data-center market, representing a huge opportunity for AMD.

Analysts estimate that Intel's data-center group will generate $18.5 billion in sales and $7.4 billion in operating profit in 2017, according to FactSet.

""Longer-term customers could be more motivated to find alternatives at AMD and possibly ARM (CAVM benefits) to diversify the architectural risks,"" Bank of America Merrill Lynch analyst Vivek Arya wrote Thursday. ""AMD appears poised to be the most direct beneficiary.""

An AMD gain of significant market share in the server market is not unprecedented. The company hit 25 percent share in 2006. If AMD is able to reach 10 percent or 15 percent market share of the data-center business, it could add billions in revenue to the company's financial results.

Any increase will be a boon for AMD because the Wall Street consensus for the company's 2017 estimated sales is just $5.25 billion.

One leading tech industry analyst says the chipmaker will do just that.

""The news of Intel's processor security issue and the potential performance degradation to correct it comes at an inopportune time as Intel currently faces heavy competitive pressure from its long-time nemesis, AMD,"" Fred Hickey, editor of High Tech Strategist, wrote in an email Thursday. ""AMD's new line of chips is a significant challenger for the first time in many years (since AMD's Opteron chip days).""

AMD launched new line Epyc data-center processors to much fanfare last June with design wins at cloud computing providers Microsoft Azure and Baidu.



""For Intel, it likely means loss of market share (lower revenues) as well as loss of pricing power (lower gross margins) as the advantage shifts to the buyers and away from Intel, which has totally dominated the PC/computer server processor market in recent years,"" Hickey said. ""AMD's new processor chips already had momentum and that momentum will likely be propelled further by the recent security issue disclosures."""
5,Demand for cyber coverage rises 50% in India,"Demand for cyber insurance among Indian companies has spiked by roughly 50%, with around 250 top companies opting for coverage. Indian underwriters reportedly began offering such policies only three years ago. Increased popularity of this type of insurance comes following a 2017 hack on public-sector lender Union Bank of India. The attack resulted in $171m being debited from the bank's account without authorisation. 
",https://economictimes.indiatimes.com/industry/banking/finance/insure/demand-for-cyber-cover-jumps-50/articleshow/62360112.cms,2018-01-04 11:10:54.450000,"MUMBAI: About 250 companies, including some of the top staterun banks , have bought cyber insurance cover, which is 50% more than what was sold in the past year. With rising attacks, insurers expect robust future demand for cyber risk insurance in India.Today, cyber risk is the most discussed risk topic-area in board rooms. Marsh India, a Mumbai-based insurance broking firm with a large share in the cyber segment, saw a 50% increase in companies buying cyber security cover in 2017 compared with 2016.Cyber liability insurance has been around in the international markets for more than a decade. However, Indian insurers have started writing this business for only three years now. It covers losses arising from a cyber attack or incident of data breach. Mostly banks and ecommerce companies have been buying large covers.The size of cyber insurance premium is Rs 200 crore. It is expected to grow to Rs 400 crore in the next couple of years. There is huge demand for cyber insurance policies after the telecom revolution and various initiatives have pushed increasing digitisation of the economy. “Cyber insurance is going through a similar phase of active dialogue which we saw for Directors’ and Officers’ liability insurance in India 15 years ago,” said Sanjay Kedia, country head, Marsh India.“One major difference though is that the cyber risk incident is on the rise at a much faster pace, and the nature of risk is far more dynamic and possibly explosive in many situations.” In 2017, public sector lender Union Bank of India was hacked and a whopping $171 million was debited from the bank’s account through Society for Worldwide Interbank Financial Telecommunication or SWIFT payments without the bank’s authorisation.The money was retrieved after the bank acted with the help of government agencies. In 2016, card data of 3.2 million customers was stolen from a network of Yes Bank ATMs managed by Hitachi Payment Services. Banks in India had to reissue cards and faced a combined loss of more than $2 million after hackers allegedly penetrated the system that carried out the processing of ATM transactions. There have been cases of mobile phone applications of banks being hacked by cyber-criminals. The pilferage of personal information and money in digital wallets has cost banks crores of rupees."
6,Terror insurance spending up four-fold in UK public sector,"The UK's public sector has increased its spending on insurance against terrorist attacks four-fold over the past 12 months. Roughly £56m ($76m) of contracts included terrorism coverage were underwritten in 2017, up from £14m in 2016. Recent attacks are said to have caused less physical damage than previous incidents. Business interruption from these attacks, however, has been notable, with insurers criticised for failing to get payouts to businesses impacted by last year's attack at Borough Market, losses from which are estimated to have been £1.4m. 
",http://www.telegraph.co.uk/business/2018/01/02/public-sector-quadruples-spending-terror-insurance/,2018-01-04 11:09:15.640000,"Public sector organisations have quadrupled the amount of money they spend on insuring against terror attacks over the past year, in a bid to protect against disruption to their services.

Around £56m of contract awards for insurance services that include terror cover were issued in 2017, up from £14m the previous year, according to data company Tussell, which tracks public contracts.

Typically, terrorism cover will insure an organisation against damage to its property in the event of a terror attack.

One of the most recent contracts awarded was to Fidelis Underwriting to insure Southwark Council’s headquarters building at 160 Tooley Street, which is close to the site of the London Bridge attack in June. The contract, worth £34,000, means the insurer would pay out for damage to the building in the event of another attack.

There have been moves from the insurance industry to try to broaden coverage, in the wake of attacks this year. Current rules were established during the Nineties, when the major threat was from the IRA.

More recent attacks have been less damaging to physical assets but have resulted in major business disruption. Neil Coyle, MP for Bermondsey and Old Southwark, hit out at insurers in the wake of the Borough Market attack after traders struggled to get payouts, suggesting losses were around £1.4m."
7,Land values for logistics and data centre hubs doubled in 2017,"The average price for large industrial plots of land of between 50 and 100 acres doubled last year from $50,000 to $100,000 per acre, thanks to increased demand for data hubs and distribution centres, according to a survey by CBRE. In an examination of 10 US markets, plots of between five and 10 acres, suitable for ""last-mile"" depots, cost $250,000 per acre by the end of last year, an increase of $50,000 on 2016. David Egan, the global head of CBRE's Industrial & Logistics Research division, said that demand is not likely to drop in the near future.",https://www.cnbc.com/2018/01/02/internet-giants-fuel-warehouse-demand-as-industrial-land-prices-surge.html?lipi=urn%3Ali%3Apage%3Ad_flagship3_feed%3Bw%2B4z9KvYQU6glzxbgyCA%2Bg%3D%3D,2018-01-04 10:59:01.237000,"A worker pulls carts full of customer orders along the floor inside the million-square foot Amazon distribution warehouse that opened last fall in Fall River, MA.

Land fit for future fulfillment centers for the likes of Amazon and Walmart saw huge spikes in prices last year, according to real estate services firm CBRE.

In a trend largely stemming from the growth of e-commerce players across the U.S., some plots of land now cost twice the amount they did a year ago, the group found. This is especially true in major markets, including Atlanta and Houston.

In surveying 10 U.S. markets, CBRE found the average price for ""large industrial parcels"" (50 to 100 acres) now sits at more than $100,000 per acre, up from about $50,000 a year ago.

Industrial land plots of five to 10 acres, which typically house infill distribution centers for completing ""last-mile"" deliveries, watched their prices soar to more than $250,000 per acre by the end of 2017, up from roughly $200,000 a year ago, according to CBRE. Located in more bustling metropolitan settings, these warehouses must help retailers serve consumers closer to their homes.

To be sure, industry experts say that despite an uptick in construction of late, there's still a long way to go before supply aligns with demand."
8,Regus Birmingham breaks 1 million sq ft office take-up for first time,"Office take-up in Birmingham during 2017 broke the one million sq ft barrier for the first time, in spite of a slow first half to the year, according to data from the city's Office Market Forum. The year saw a total of 130 deals and beating the previous record of 970,458 sq ft in 2015. The figures were given a fillip by demand linked to the planned HS2 high-speed rail link, and the UK government committing to the city's biggest pre-let in 10 years.
",http://www.commercialnewsmedia.com/archives/69509,2018-01-04 10:25:48.097000,"Office take up in quarter four, 2017 in the central Birmingham office market totalled 354,530 sq ft in 49 deals as scheduled in the table below, compiled by the Birmingham Office Market Forum.

When added to the 81 deals totalling 650,542 sq ft recorded in the first three quarters of the year, the 2017 year-end total take up amounts to 1,005,072 sq ft in 130 deals.

This compares with:

692,729 sq ft in 139 deals for 2016

970,458 sq ft in 132 deals for 2015

713,460 sq ft in 148 deals for 2014

664,147 sq ft in 128 deals for 2013

The 2017 outcome is a record take up year, beating the previous high seen in 2015.

Office take up was boosted by the emergence of HS2 related demand together with the Government committing to the largest prelet seen in the city for a decade. In addition, key larger transactions were concluded in the Professional Services and the Serviced Office sectors.

Breaking 1 million sq ft office take up for the first time is extremely positive for Birmingham during the current period of unprecedented development activity and further regeneration, visible across the BOMF area. It is also particularly encouraging bearing in mind the slow first half of the year following on from the dip seen in the previous year’s total.

For further information please contact the author of the report Jonathan Carmalt, Director, Office Agency, JLL on 0121 214 9935 or email [email protected]

Birmingham Office Market Forum was established in 2007 to present a co-ordinated voice to investors, developers and occupiers about Birmingham’s city centre office market.

The Forum brings together the city’s leading office agents and Business Birmingham.

For a list of member firms or further information visit their website"
9,UK insurers may cover health claims based on chatbot diagnosis,"Machine learning chatbots – such as Ada, which helps UK residents make preliminary assessments about their health – could lead to insurance companies offering coverage for virtual consultations. Industry insiders have suggested that a chatbot's ability to personalise consultations makes them less likely to make mistakes, leading to less risk for insurers. The country's National Health Service is using a chatbot assessment service in an attempt to reduce A&E patient numbers.
",https://venturebeat.com/2018/01/03/insurers-in-the-uk-could-be-the-first-to-cover-chatbot-consultations/,2018-01-04 10:14:10.870000,"Elevate your enterprise data technology and strategy at Transform 2021.

People use chatbots to find homes, interact with their favorite brands, and schedule appointments. Many consumers are onboard with using chatbots to gather instant, personalized information.

In many cases, chatbots are the first point of contact for individuals who feel unwell and need to decide whether to head to the doctor. As this technology becomes more prominent, people understandably begin to wonder if insurance companies will cover sessions with chatbot doctors. Given their innovative use of chatbots in the health care sector, it’s looking like insurers and health organizations in the U.K. could be the first to establish insurance coverage for health consultations with chatbots.

How do chatbot doctors work?

People who have yet to interact with a chatbot doctor might wonder how they work. Could a bot know as much as physicians who have completed years of medical school and relevant work experience?

Sometimes, the chatbot makes a preliminary assessment about a person’s health depending on the responses the individual gives to targeted questions. One such chatbot called Ada is available to residents in the United Kingdom. The assessment is free, and the bot avoids providing a set-in-stone diagnosis. Ada uses artificial intelligence to get continually smarter with ongoing use.

The developers know Ada won’t replace doctors but hope the bot will help more patients understand what their symptoms might mean. However, a person can also supplement the assessment portion by talking to an actual doctor. That option is offered for a fee and includes receiving a prescription if needed.

Individuals frequently head to sites such as WebMD and end up with a questionable self-diagnosis. This is why it makes sense that insurers would be open to the idea of paying for patient interactions that begin with chatbots. A bot’s ability to personalize its conversations with patients could theoretically yield a smaller margin of error and be less likely to provide misleading information.

Reducing emergency room visits

The U.K.’s National Health Service (NHS) is also trying out a chatbot that asks people a series of questions when they dial the nationwide emergency number to indicate whether or not they need an ambulance. Representatives hope the service will reduce the number of people dispatchers send to emergency rooms.

When too many callers are sent to the emergency room, patients wait in hospitals for several hours or even longer before receiving treatment. Those in favor of chatbots say the technology could keep emergency room visits at more manageable levels. In contrast, critics assert that the NHS previously used a symptom checker app that made highly publicized and dangerous blunders when advising some patients, and they think the same problems could occur with the chatbot.

The NHS provides free health care to legal residents of the United Kingdom that covers most needs, including emergency care and visits with general practitioners. However, the NHS assists over 1 million patients in England every 36 hours. People who rely on the NHS for health care often deal with long waits. To compensate, those who can afford them often subscribe to private insurance plans.

It will be interesting to gauge the outcome of this NHS program, especially considering how many people rely on the NHS. If things go well, the positive result could prompt insurance companies to accept claims from customers who receive advice from chatbots regarding their well-being.

How much are chatbots worth?

It remains to be seen if insurance companies in the United Kingdom and elsewhere will cover chatbot doctors. The evidence to suggest their willingness — or lack thereof — to do so is not available yet because the technology is too new.

One reason insurers might balk at the idea of paying for this kind of coverage is the need to determine the fair market value of compensation for such services. In some areas, health facilities use telehealth providers to reduce the need for on-call personnel. However, the relevant valuators in those locations must remain aware of federal and state laws surrounding telemedicine. If they fail to do so, over- or undercompensation could occur during the billing and collection processes.

Some telemedicine laws have not been in place for very long. Not surprisingly, many locations have not even considered chatbots in the equation.

Helping people understand their coverage

Although insurance companies haven’t made it clear whether they’re completely onboard with covering chatbots, it seems promising that some are already using chatbots to help customers achieve a higher level of understanding about their coverage packages.

For example, Now Health International is a company that provides health insurance for expatriates. The headquarters is in Hong Kong, and the establishment has other branches in Asia and the United Kingdom.

This summer, the insurance provider launched a chatbot through Facebook Messenger. Regardless of whether users are existing customers or are only thinking about purchasing coverage packages, they can use the chatbot to find physicians in the Now Health International network or get questions answered about filing claims and receiving quotes.

The chatbot can also recognize keywords entered by a user into the chat window. When it picks up on those words, the technology automatically directs the person to the proper area of the website for further information.

The increasingly widespread use of chatbots for health — including those provided by insurance companies — indicates that some insurers are laying the necessary foundation for covering chatbots in the future. However, it’s likely that before that happens, we must pass legislation clarifying the valuation-related questions that could arise during claims, billing, and other aspects.

As consumers become more comfortable with using chatbots to ask questions about their health or insurance, the increased prevalence of the technology in the marketplace could push officials to iron out the details. That would pave the way for insurance companies to clearly mention they accept claims related to treatment that involves chatbot doctors on their websites or insurance documents. Insurers may also stipulate that they will not offer coverage to patients who did not speak to actual doctors during their chatbot-driven conversations and only used the chatbots for preliminary advice.

Chatbots could drastically change how people manage their health care needs. Similarly, they could alter how doctors treat patients and how insurers handle the claims. Since the technology is in its infancy, however, it’s only possible to perform research and make educated speculations.

Kayla Matthews is senior writer for MakeUseOf. Her work has also appeared on Vice, The Next Web, The Week, and TechnoBuffalo."
10,ETF shop Reality Shares adds blockchain experts to advisory board,"Asset management company Reality Shares Advisors has appointed six blockchain and cryptocurrency executives to its advisory board to ""infuse its investment products and decisions with the knowledge and research of credible thought leaders"", according to CEO Eric Ervin. The appointees include Jeff Garzik, the CEO of blockchain enterprise software company Bloq, Derin Cag, co-founder of research centre Blockchain Age, and Steve Beauregard, former CEO of payment processor GoCoin. Reality Shares Advisors focuses on ETF and index investments.
",https://www.crowdfundinsider.com/2018/01/126604-california-asset-management-firm-reality-shares-announces-advisory-board-consisted-six-blockchain-cryptocurrency-executives/,2018-01-04 09:18:37.177000,"California-based asset management firm Reality Share Advisors announced on Wednesday its advisory board now includes six blockchain and cryptocurrency executives. The members are the following:

Erik Voorhees: The founder of Coinapult and CEO of ShapeShift.

The founder of Coinapult and CEO of ShapeShift. Dr. Garrick Hileman: A research fellow at the University of Cambridge and researcher at the London School of Economics.

A research fellow at the University of Cambridge and researcher at the London School of Economics. Jeff Garzik: The co-founder and CEO of Bloq, a blockchain enterprise software company.

The co-founder and CEO of Bloq, a blockchain enterprise software company. Matthew Roszak: The founding partner of Tally Capital, a private investment firm focused on blockchain-enabled technology.

The founding partner of Tally Capital, a private investment firm focused on blockchain-enabled technology. Steve Beauregard: The founder and former CEO of leading blockchain payment processor GoCoin.

The founder and former CEO of leading blockchain payment processor GoCoin. Derin Cag: The founder of Richtopia and co-founder of Blockchain Age, a research center and digital data consultancy for blockchain technology.

While sharing more details about the board, Eric Ervin, CEO of Reality Shares, stated:

“In recognizing the tremendous growth potential for blockchain technology while still in its infancy, this advisory board seeks to infuse our investment products and decisions with the knowledge and research of credible thought leaders in the space.”

Ervin then added:

“Our newly-formed advisory board is comprised of well-regarded influencers at the forefront of blockchain innovation who are deeply entrenched in the disruptive technologies and ideas propelling the distributed ledger and cryptocurrency revolution.”

Founded in 2011, Reality Shares is described as an innovative asset management firm, ETF issuer, and index provider. The firm noted that its goal is democratize the world’s best investing ideas, using systematic quantitative methods to deliver products and solutions that support a range of investing objectives, such as diversification, lower correlation, risk mitigation, or unique market exposures."
11,General Motors sales surge in China as US registers declines,"Surging sales of General Motors cars in China meant the company sold 70% more vehicles there than in the US in November. China sales rose 13.1% last month, YoY, to 491,702 vehicles, having jumped 13% in November. US sales fell 3.3% to 245,387 last month after a 3% decline in November. The most popular GM brand in China is the Baojun, sales of which rocketed 52% in November to 113,711. However, GM only owns 44% of Baojun, with China's SAIC holding a large part of the deal. The Wall Street Journal warns that this ""leaves GM vulnerable to the whims of its powerful Chinese partner"".
",http://www.businessinsider.com/gms-business-is-booming-in-china-2017-12/?IR=T&mc_cid=28d3be2ef0&mc_eid=a37072368a,2018-01-04 08:18:54.380000,"General Motors sells 70% more cars in China than in the US.

China's auto market growth has outpaced the US in the last decade, but is slowing.



In China, General Motors is hot. In November, its 10 joint ventures and two wholly-owned foreign enterprises sold 418,225 new vehicles in China, up 13% from a year ago. It was the best November ever, GM said. SUV sales soared 73%.

By comparison, in the US, GM sold 245,387 new vehicles in November, it reported a few days ago, down nearly 3% from a year ago. In other words, in November, GM sold 70% more vehicles in China than in the US.

China became the world’s largest new vehicle market for the first time in 2009, when sales in the US plunged. For years, growth rates in the Chinese market blew the doors off the US market. But the hectic pace has recently subsided. For 2017, deliveries are expected to rise only 2%, and competition from local automakers is getting tougher.

Buick is still hot in China – though it’s just about moribund in the US, where deliveries fell 3.5% year-over-year in November to just 16,833 vehicles, accounting for only 7% of GM’s total sales in the US. Of them, 2,228 were the China-made compact SUVs, the Buick Envision.

In China, Buick is still the fourth largest auto brand with a market share of just under 5% so far this year, behind Volkswagen, Honda, and Toyota. In November, GM sold 112,738 Buicks, or 27% of its total sales. But for the month, Buick was already outsold by GM’s Baojun.

Baojun is the hottest brand GM has in China. Sales of the bargain-priced vehicles soared 52% in November to 113,711 units, accounting for over a quarter of GM’s total sales in China.

GM launched Baojun in 2010, after the “New GM” had emerged from Chapter 11 bankruptcy in the US in July 2009, with Debtor in Possession (DIP) financing and equity investments from the US taxpayer. This support helped GM go on an investment spree in China, and, along with its joint-venture partner SAIC, plow $2.4 billion in the Baojun factory in Liuzhou, even as many former GM plants in the US had been shuttered and were disposed of in bankruptcy.

But GM owns only 44% of Baojun. With technology transfer to SAIC being a big part of the deal, this is a risky proposition. The Wall Street Journal:

Other foreign auto makers “are consistently taken aback by GM’s apparently generous technology sharing” when it comes to Baojun, said Michael Dunne, a former GM executive and now president of Dunne Automotive, a consultancy. “The open approach has engendered considerable goodwill but it also leaves GM vulnerable to the whims of its powerful Chinese partner.”

GM was less proud of its brand Wuling. It sold 113,919 units. GM does not brag about the year-over-year change in deliveries, as it does with Baojun. In fact, in the press release, there is no mention of this year-over-year change. Turns out, a year ago, GM had reported 121,566 sales in November. In other words, Wuling sales dropped 6.3% year-over-year.

Nevertheless GM gushed, while purposefully leaving out Wuling: “GM’s performance was strong across its brands. Baojun deliveries reached an all-time monthly high, while Buick, Cadillac and Chevrolet set November sales records.”

China-sold Buicks, Cadillacs, and Chevrolets are built by a 50-50 GM-SAIC joint venture. These vehicles tend to be more upscale than GM’s Chinese brands.

For the first 11 months this year, GM sold 32% more vehicles in China than in the US, with 3,549,087 units in China, up 3.3% year-over-year, versus 2,691,493 units in the US, down 1.2%, according to Autodata. 2017 will be the sixth year in a row when GM’s vehicle sales in China exceeded those in the US.

But it’s complicated. These are joint ventures, margins in China are thin, and in terms of profits, GM’s China operations don’t contribute all that much. For the year 2016, GM booked global profits of $9.4 billion, of which GM attributed only $2 billion to “equity income” at its joint ventures in China despite all the massive in vestments in China. Most of the remainder of its global profits came from its sales in the US, and mostly from the fat profit margins on pickups and SUVs.

Then there’s Tesla, with a market capitalization not much behind GM’s despite minuscule vehicle sales. This is where hype goes to die."
12,Intervals between coral bleaching events falls from 25 to 6 years,"Tropical coral reefs across the world, on which millions of livelihoods depend and which are home to a third of all marine biodiversity, are under threat from repeated deadly bouts of warmer water, according to new research. The study of 100 reefs reveals that the interval between bleaching events, when unusually warm water causes coral to eject algae with often fatal consequences, has fallen from once in every 25-30 years in the 1980s, to once in every six years. The researchers have called for greater efforts to reduce the emissions of greenhouse gases to combat the warming.
",http://www.bbc.co.uk/news/science-environment-42571484,2018-01-04 00:00:00,"""Coral reefs cover less than 0.1% of the world's oceans and yet they house a third of all marine biodiversity. And the oceans cover 70% of our planet so they're housing a huge amount of the biodiversity of our planet. So, anyone who cares about extinction, about biodiversity, needs to worry about the future of coral reefs."""
13,Intervals between coral bleaching events falls from 25 to 6 years,"Tropical coral reefs across the world, on which millions of livelihoods depend and which are home to a third of all marine biodiversity, are under threat from repeated deadly bouts of warmer water, according to new research. The study of 100 reefs reveals that the interval between bleaching events, when unusually warm water causes coral to eject algae with often fatal consequences, has fallen from once in every 25-30 years in the 1980s, to once in every six years. The researchers have called for greater efforts to reduce the emissions of greenhouse gases to combat the warming.
",http://science.sciencemag.org/content/359/6371/80,2018-01-04 00:00:00,"Not enough time for recovery Coral bleaching occurs when stressful conditions result in the expulsion of the algal partner from the coral. Before anthropogenic climate warming, such events were relatively rare, allowing for recovery of the reef between events. Hughes et al. looked at 100 reefs globally and found that the average interval between bleaching events is now less than half what it was before. Such narrow recovery windows do not allow for full recovery. Furthermore, warming events such as El Niño are warmer than previously, as are general ocean conditions. Such changes are likely to make it more and more difficult for reefs to recover between stressful events. Science, this issue p. 80

Abstract Tropical reef systems are transitioning to a new era in which the interval between recurrent bouts of coral bleaching is too short for a full recovery of mature assemblages. We analyzed bleaching records at 100 globally distributed reef locations from 1980 to 2016. The median return time between pairs of severe bleaching events has diminished steadily since 1980 and is now only 6 years. As global warming has progressed, tropical sea surface temperatures are warmer now during current La Niña conditions than they were during El Niño events three decades ago. Consequently, as we transition to the Anthropocene, coral bleaching is occurring more frequently in all El Niño–Southern Oscillation phases, increasing the likelihood of annual bleaching in the coming decades.

The average surface temperature of Earth has risen by close to 1°C as of the 1880s (1), and global temperatures in 2015 and 2016 were the warmest since instrumental record keeping began in the 19th century (2). Recurrent regional-scale (>1000 km) bleaching and mortality of corals is a modern phenomenon caused by anthropogenic global warming (3–10). Bleaching before the 1980s was recorded only at a local scale of a few tens of kilometers because of small-scale stressors such as freshwater inundation, sedimentation, or unusually cold or hot weather (3–5). The modern emergence of regional-scale bleaching is also evident from the growth bands of old Caribbean corals: synchronous distortions of skeletal deposition (stress bands) along a 400-km stretch of the Mesoamerican Reef have only been found after recent hot conditions, confirming that regional-scale heat stress is a modern phenomenon caused by anthropogenic global warming (10). Bleaching occurs when the density of algal symbionts, or zooxanthellae (Symbiodinium spp.), in the tissues of a coral host diminishes as a result of environmental stress, revealing the underlying white skeleton of the coral (8). Bleached corals are physiologically and nutritionally compromised, and prolonged bleaching over several months leads to high levels of coral mortality (11, 12). Global climate modeling and satellite observations also indicate that the thermal conditions for coral bleaching are becoming more prevalent (13, 14), leading to predictions that localities now considered to be thermal refugia could disappear by midcentury (15).

Although several global databases of bleaching records are available (notably ReefBase, reefbase.org), they suffer from intermittent or lapsed maintenance and from uneven sampling effort across both years and locations (7). The time spans of five earlier global studies of coral bleaching range from 1870 to 1990 (3), 1960 to 2002 (4), 1973 to 2006 (5), 1980 to 2005 (6), and 1985 to 2010 (7). Here we compiled de novo the history of recurrent bleaching from 1980 to 2016 for 100 globally distributed coral reef locations in 54 countries using a standardized protocol to examine patterns in the timing, recurrence, and intensity of bleaching episodes, including the latest global bleaching event from 2015 to 2016 (table S1). This approach avoids the bias of the continuous addition of new sites in open-access databases and retains the same range of spatial scales through time (fig. S1). A bleaching record in our analysis consists of three elements: the location, from 1 to 100; the year; and the binary presence or absence of bleaching. Our findings reveal that coral reefs have entered the distinctive human-dominated era characterized as the Anthropocene (16–18), in which the frequency and intensity of bleaching events is rapidly approaching unsustainable levels. At the spatial scale we examined (fig. S1), the number of years between recurrent severe bleaching events has diminished fivefold in the past four decades, from once every 25 to 30 years in the early 1980s to once every 5.9 years in 2016. Across the 100 locations, we scored 300 bleaching episodes as severe, i.e., >30% of corals bleached at a scale of tens to hundreds of kilometers, and a further 312 as moderate (<30% of corals bleached). Our analysis indicates that coral reefs have moved from a period before 1980 when regional-scale bleaching was exceedingly rare or absent (3–5) to an intermediary phase beginning in the 1980s when global warming increased the thermal stress of strong El Niño events, leading to global bleaching events. Finally, in the past two decades, many additional regional-scale bleaching events have also occurred outside of El Niño conditions, affecting more and more former spatial refuges and threatening the future viability of coral reefs.

Increasingly, climate-driven bleaching is occurring in all El Niño–Southern Oscillation (ENSO) phases, because as global warming progresses, average tropical sea surface temperatures are warmer today under La Niña conditions than they were during El Niño events only three decades ago (Fig. 1). Since 1980, 58% of severe bleaching events have been recorded during four strong El Niño periods (1982–1983, 1997–1998, 2009–2010, and 2015–2016) (Fig. 2A), with the remaining 42% occurring during hot summers in other ENSO phases. Inevitably, the link between El Niño as the predominant trigger of mass bleaching (3–5) is diminishing as global warming continues (Fig. 1) and as summer temperature thresholds for bleaching are increasingly exceeded throughout all ENSO phases.

Fig. 1 Global warming throughout ENSO cycles. Sea surface temperature anomalies from 1871 to 2016, relative to a 1961–1990 baseline, averaged across 1670 1° latitude–by–1° longitude boxes containing coral reefs between latitudes of 31°N and 31°S. Data points differentiate El Niño (red triangles), La Niña (blue triangles), and ENSO neutral periods (black squares). Ninety-five percent confidence intervals are shown for nonlinear regression fits for years with El Niño and La Niña conditions (red and blue shading, respectively; overlap is shown in purple).

Fig. 2 Temporal patterns of recurrent coral bleaching. (A) Number of 100 pantropical locations that have bleached each year from 1980 to 2016. Black bars indicate severe bleaching affecting >30% of corals, and white bars depict moderate bleaching of <30% of corals. (B) Cumulative number of severe and total bleaching events since 1980 (red; right axis) and the depletion of locations that remain free of any bleaching or severe bleaching over time (blue; left axis). (C) Frequency distribution of the number of severe (black) and total bleaching events (red) per location. (D) Frequency distribution of return times (number of years) between successive severe bleaching events from 1980 to 1999 (white bars) and 2000 to 2016 (black bars).

The 2015–2016 bleaching event affected 75% of the globally distributed locations we examined (Figs. 2A and 3) and is therefore comparable in scale to the then-unprecedented 1997–1998 event, when 74% of the same 100 locations bleached. In both periods, sea surface temperatures were the warmest on record in all major coral reef regions (2, 19). As the geographic footprint of recurrent bleaching spreads, fewer and fewer potential refuges from global warming remain untouched (Fig. 2B), and only 6 of the 100 locations we examined have escaped severe bleaching so far (Fig. 2B and table S1). This result is conservative because of type 2 errors (false negatives) in our analyses, where bleaching could have occurred but was not recorded.

Fig. 3 The global extent of mass bleaching of corals in 2015 and 2016. Symbols show 100 reef locations that were assessed: red circles, severe bleaching affecting >30% of corals; orange circles, moderate bleaching affecting <30% of corals; and blue circles, no substantial bleaching recorded. See table S1 for further details.

After the extreme bleaching recorded from 2015 to 2016, the median number of severe bleaching events experienced across our study locations since 1980 is now three (Fig. 2C). Eighty-eight percent of the locations that bleached from 1997 to 1998 have bleached severely at least once again. As of 1980, 31% of reef locations have experienced four or more (up to nine) severe bleaching events (Fig. 2C), as well as many moderate episodes (table S1). Globally, the annual risk of bleaching (both severe and more moderate events) has increased by a rate of approximately 3.9% per annum (fig. S2), from an expected 8% of locations in the early 1980s to 31% in 2016. Similarly, the annual risk of severe bleaching has also increased, at a slightly faster rate of 4.3% per annum, from an expected 4% of locations in the early 1980s to 17% in 2016 (fig. S2). This trend corresponds to a 4.6-fold reduction in estimated return times of severe events, from once every 27 years in the early 1980s to once every 5.9 years in 2016. Thirty-three percent of return times between recurrent severe bleaching events since 2000 have been just 1, 2, or 3 years (Fig. 2D).

Our analysis also reveals strong geographic patterns in the timing, severity, and return times of mass bleaching (Fig. 4). The Western Atlantic, which has warmed earlier than elsewhere (13, 19), began to experience regular bleaching sooner, with an average of 4.1 events per location before 1998, compared with 0.4 to 1.6 in other regions (Fig. 4 and fig. S2). Furthermore, widespread bleaching (affecting >50% of locations) has now occurred seven times since 1980 in the Western Atlantic, compared to three times for both Australasia and the Indian Ocean, and only twice in the Pacific. Over the entire period, the number of bleaching events has been highest in the Western Atlantic, with an average of 10 events per location, two to three times more than in other regions (Fig. 4).

Fig. 4 Geographic variation in the timing and intensity of coral bleaching from 1980 to 2016. (A) Australasia (32 locations). (B) Indian Ocean (24 locations). (C) Pacific Ocean (22 locations). (D) Western Atlantic (22 locations). For each region, black bars indicate the percentage of locations that experienced severe bleaching, affecting >30% of corals. White bars indicate the percentage of locations per region with additional moderate bleaching affecting <30% of corals.

In the 1980s, bleaching risk was highest in the Western Atlantic followed by the Pacific, with the Indian Ocean and Australasia having the lowest bleaching risk. However, bleaching risk increased most strongly over time in Australasia and the Middle East, at an intermediate rate in the Pacific, and slowly in the Western Atlantic (Fig. 4, fig. S3B, and tables S2 and S3). The return times between pairs of severe bleaching events are declining in all regions (fig. S3C), with the exception of the Western Atlantic, where most locations have escaped a major bleaching event from 2010 to 2016 (Fig. 2D).

We tested the hypothesis that the number of bleaching events that have occurred so far at each location is positively related to the level of postindustrial warming of sea surface temperatures that has been experienced there (fig. S4). However, we found no significant relationship for any of the four geographic regions, consistent with each bleaching event being caused by a short-lived episode of extreme heat (12, 19, 20) that is superimposed on much smaller long-term warming trends. Hence, the long-term predictions of future average warming of sea surface temperatures (13) are also unlikely to provide an accurate projection of bleaching risk or the location of spatial refuges over the next century.

In the coming years and decades, climate change will inevitably continue to increase the number of extreme heating events on coral reefs and further drive down the return times between them. Our analysis indicates that we are already approaching a scenario in which every hot summer, with or without an El Niño event, has the potential to cause bleaching and mortality at a regional scale. The time between recurrent events is increasingly too short to allow a full recovery of mature coral assemblages, which generally takes from 10 to 15 years for the fastest growing species and far longer for the full complement of life histories and morphologies of older assemblages (21–24). Areas that have so far escaped severe bleaching are likely to decline further in number (Fig. 2B), and the size of spatial refuges will diminish. These impacts are already underway, with an increase in average global temperature of close to 1°C. Hence, 1.5° or 2°C of warming above preindustrial conditions will inevitably contribute to further degradation of the world’s coral reefs (14). The future condition of reefs, and the ecosystem services they provide to people, will depend critically on the trajectory of global emissions and on our diminishing capacity to build resilience to recurrent high-frequency bleaching through management of local stressors (18) before the next bleaching event occurs.

Supplementary Materials www.sciencemag.org/content/359/6371/80/suppl/DC1 Materials and Methods Figs. S1 to S4 Tables S1 to S3 References (25–29)

http://www.sciencemag.org/about/science-licenses-journal-article-reuse This is an article distributed under the terms of the Science Journals Default License.

Acknowledgments: Major funding for this research was provided by the Australian Research Council’s Centre of Excellence Program (CE140100020). The contents of this manuscript are solely the opinions of the authors and do not constitute a statement of policy, decision, or position on behalf of the National Oceanic and Atmospheric Administration or the U.S. government. Data reported in this paper are tabulated in the supplementary materials."
14,Liam Fox 'not ruling out' possibility of UK joining TPP,"According to the UK's International Trade Secretary, Liam Fox, the UK could feasibly join the Trans-Pacific Partnership (TPP), saying ""it would be foolish to rule anything out"". The organisation is made up of Australia, Mexico, New Zealand, Canada, Chile, Japan, Singapore, Brunei, Peru, Vietnam and Malaysia - with Donald Trump pulling the US out last year - and is currently in renegotiation under the new name of the Comprehensive and Progressive Agreement for Trans-Pacific Partnership. Its aims are to lower both non-tariff and tariff barriers to trade and to provide a forum to settle international disputes.",http://www.bbc.co.uk/news/uk-politics-42552877,2018-01-04 00:00:00,"""But, on the other hand, we would be foolish to rule anything out. We know that Asia-Pacific will be a very important market and we know a lot of the global growth in the future will come from there."""
15,Ocean dead zones have quadrupled in size since 1950,"Ocean dead zones, which contain no oxygen, have become four times larger since 1950, while the number of areas with very low oxygen close to coasts has increased tenfold, according to the first comprehensive analysis of these areas. Most marine species cannot exist in such conditions, and the continuation of such trends would result in mass extinction, endangering the livelihood of millions of people. Large-scale deoxygenation is the result of climate change caused by burning fossil fuels; as waters warm, they contain less oxygen.
",https://www.theguardian.com/environment/2018/jan/04/oceans-suffocating-dead-zones-oxygen-starved,2018-01-04 00:00:00,"Ocean dead zones with zero oxygen have quadrupled in size since 1950, scientists have warned, while the number of very low oxygen sites near coasts have multiplied tenfold. Most sea creatures cannot survive in these zones and current trends would lead to mass extinction in the long run, risking dire consequences for the hundreds of millions of people who depend on the sea.

Climate change caused by fossil fuel burning is the cause of the large-scale deoxygenation, as warmer waters hold less oxygen. The coastal dead zones result from fertiliser and sewage running off the land and into the seas.

The analysis, published in the journal Science, is the first comprehensive analysis of the areas and states: “Major extinction events in Earth’s history have been associated with warm climates and oxygen-deficient oceans.” Denise Breitburg, at the Smithsonian Environmental Research Center in the US and who led the analysis, said: “Under the current trajectory that is where we would be headed. But the consequences to humans of staying on that trajectory are so dire that it is hard to imagine we would go quite that far down that path.”

“This is a problem we can solve,” Breitburg said. “Halting climate change requires a global effort, but even local actions can help with nutrient-driven oxygen decline.” She pointed to recoveries in Chesapeake Bay in the US and the Thames river in the UK, where better farm and sewage practices led to dead zones disappearing.

However, Prof Robert Diaz at the Virginia Institute of Marine Science, who reviewed the new study, said: “Right now, the increasing expansion of coastal dead zones and decline in open ocean oxygen are not priority problems for governments around the world. Unfortunately, it will take severe and persistent mortality of fisheries for the seriousness of low oxygen to be realised.”

The oceans feed more than 500 million people, especially in poorer nations, and provide jobs for 350 million people. But at least 500 dead zones have now been reported near coasts, up from fewer than 50 in 1950. Lack of monitoring in many regions means the true number may be much higher.

The open ocean has natural low oxygen areas, usually off the west coast of continents due to the way the rotation of the Earth affects ocean currents. But these dead zones have expanded dramatically, increasing by millions of square kilometres since 1950, roughly equivalent to the area of the European Union.

Furthermore, the level of oxygen in all ocean waters is falling, with 2% – 77bn tonnes – being lost since 1950. This can reduce growth, impair reproduction and increase disease, the scientists warn. One irony is that warmer waters not only hold less oxygen but also mean marine organisms have to breathe faster, using up oxygen more quickly.

There are also dangerous feedback mechanisms. Microbes that proliferate at very low oxygen levels produce lots of nitrous oxide, a greenhouse gas that is 300 times more potent than carbon dioxide.

In coastal regions, fertiliser, manure and sewage pollution cause algal blooms and when the algae decompose oxygen is sucked out of the water. However, in some places, the algae can lead to more food for fish and increase catches around the dead zones. This may not be sustainable though, said Breitburg: “There is a lot of concern that we are really changing the way these systems function and that the overall resilience of these systems may be reduced.”

The new analysis was produced by an international working group created in 2016 by Unesco’s Intergovernmental Oceanographic Commission. The commission’s Kirsten Isensee said: “Ocean deoxygenation is taking place all over the world as a result of the human footprint, therefore we also need to address it globally.”

Lucia von Reusner, campaign director of the campaign group, Mighty Earth, which recently exposed a link between the dead zone in the Gulf of Mexico and large scale meat production, said: “These dead zones will continue to expand unless the major meat companies that dominate our global agricultural system start cleaning up their supply chains to keep pollution out of our waters.”

Diaz said the speed of ocean suffocation already seen was breathtaking: “No other variable of such ecological importance to coastal ecosystems has changed so drastically in such a short period of time from human activities as dissolved oxygen.”

He said the need for urgent action is best summarised by the motto of the American Lung Association: “If you can’t breathe, nothing else matters.”"
16,Ocean dead zones have quadrupled in size since 1950,"Ocean dead zones, which contain no oxygen, have become four times larger since 1950, while the number of areas with very low oxygen close to coasts has increased tenfold, according to the first comprehensive analysis of these areas. Most marine species cannot exist in such conditions, and the continuation of such trends would result in mass extinction, endangering the livelihood of millions of people. Large-scale deoxygenation is the result of climate change caused by burning fossil fuels; as waters warm, they contain less oxygen.
",http://science.sciencemag.org/content/359/6371/eaam7240,2018-01-04 00:00:00,"Beneath the waves, oxygen disappears As plastic waste pollutes the oceans and fish stocks decline, unseen below the surface another problem grows: deoxygenation. Breitburg et al. review the evidence for the downward trajectory of oxygen levels in increasing areas of the open ocean and coastal waters. Rising nutrient loads coupled with climate change—each resulting from human activities—are changing ocean biogeochemistry and increasing oxygen consumption. This results in destabilization of sediments and fundamental shifts in the availability of key nutrients. In the short term, some compensatory effects may result in improvements in local fisheries, such as in cases where stocks are squeezed between the surface and elevated oxygen minimum zones. In the longer term, these conditions are unsustainable and may result in ecosystem collapses, which ultimately will cause societal and economic harm. Science, this issue p. eaam7240

Structured Abstract BACKGROUND Oxygen concentrations in both the open ocean and coastal waters have been declining since at least the middle of the 20th century. This oxygen loss, or deoxygenation, is one of the most important changes occurring in an ocean increasingly modified by human activities that have raised temperatures, CO 2 levels, and nutrient inputs and have altered the abundances and distributions of marine species. Oxygen is fundamental to biological and biogeochemical processes in the ocean. Its decline can cause major changes in ocean productivity, biodiversity, and biogeochemical cycles. Analyses of direct measurements at sites around the world indicate that oxygen-minimum zones in the open ocean have expanded by several million square kilometers and that hundreds of coastal sites now have oxygen concentrations low enough to limit the distribution and abundance of animal populations and alter the cycling of important nutrients. ADVANCES In the open ocean, global warming, which is primarily caused by increased greenhouse gas emissions, is considered the primary cause of ongoing deoxygenation. Numerical models project further oxygen declines during the 21st century, even with ambitious emission reductions. Rising global temperatures decrease oxygen solubility in water, increase the rate of oxygen consumption via respiration, and are predicted to reduce the introduction of oxygen from the atmosphere and surface waters into the ocean interior by increasing stratification and weakening ocean overturning circulation. In estuaries and other coastal systems strongly influenced by their watershed, oxygen declines have been caused by increased loadings of nutrients (nitrogen and phosphorus) and organic matter, primarily from agriculture; sewage; and the combustion of fossil fuels. In many regions, further increases in nitrogen discharges to coastal waters are projected as human populations and agricultural production rise. Climate change exacerbates oxygen decline in coastal systems through similar mechanisms as those in the open ocean, as well as by increasing nutrient delivery from watersheds that will experience increased precipitation. Expansion of low-oxygen zones can increase production of N 2 O, a potent greenhouse gas; reduce eukaryote biodiversity; alter the structure of food webs; and negatively affect food security and livelihoods. Both acidification and increasing temperature are mechanistically linked with the process of deoxygenation and combine with low-oxygen conditions to affect biogeochemical, physiological, and ecological processes. However, an important paradox to consider in predicting large-scale effects of future deoxygenation is that high levels of productivity in nutrient-enriched coastal systems and upwelling areas associated with oxygen-minimum zones also support some of the world’s most prolific fisheries. OUTLOOK Major advances have been made toward understanding patterns, drivers, and consequences of ocean deoxygenation, but there is a need to improve predictions at large spatial and temporal scales important to ecosystem services provided by the ocean. Improved numerical models of oceanographic processes that control oxygen depletion and the large-scale influence of altered biogeochemical cycles are needed to better predict the magnitude and spatial patterns of deoxygenation in the open ocean, as well as feedbacks to climate. Developing and verifying the next generation of these models will require increased in situ observations and improved mechanistic understanding on a variety of scales. Models useful for managing nutrient loads can simulate oxygen loss in coastal waters with some skill, but their ability to project future oxygen loss is often hampered by insufficient data and climate model projections on drivers at appropriate temporal and spatial scales. Predicting deoxygenation-induced changes in ecosystem services and human welfare requires scaling effects that are measured on individual organisms to populations, food webs, and fisheries stocks; considering combined effects of deoxygenation and other ocean stressors; and placing an increased research emphasis on developing nations. Reducing the impacts of other stressors may provide some protection to species negatively affected by low-oxygen conditions. Ultimately, though, limiting deoxygenation and its negative effects will necessitate a substantial global decrease in greenhouse gas emissions, as well as reductions in nutrient discharges to coastal waters. Low and declining oxygen levels in the open ocean and coastal waters affect processes ranging from biogeochemistry to food security. The global map indicates coastal sites where anthropogenic nutrients have exacerbated or caused O 2 declines to <2 mg liter−1 (<63 μmol liter−1) (red dots), as well as ocean oxygen-minimum zones at 300 m of depth (blue shaded regions). [Map created from data provided by R. Diaz, updated by members of the GO 2 NE network, and downloaded from the World Ocean Atlas 2009].

Abstract Oxygen is fundamental to life. Not only is it essential for the survival of individual animals, but it regulates global cycles of major nutrients and carbon. The oxygen content of the open ocean and coastal waters has been declining for at least the past half-century, largely because of human activities that have increased global temperatures and nutrients discharged to coastal waters. These changes have accelerated consumption of oxygen by microbial respiration, reduced solubility of oxygen in water, and reduced the rate of oxygen resupply from the atmosphere to the ocean interior, with a wide range of biological and ecological consequences. Further research is needed to understand and predict long-term, global- and regional-scale oxygen changes and their effects on marine and estuarine fisheries and ecosystems.

Oxygen levels have been decreasing in the open ocean and coastal waters since at least the middle of the 20th century (1–3). This ocean deoxygenation ranks among the most important changes occurring in marine ecosystems (1, 4–6) (Figs. 1 and 2). The oxygen content of the ocean constrains productivity, biodiversity, and biogeochemical cycles. Major extinction events in Earth’s history have been associated with warm climates and oxygen-deficient oceans (7), and under current trajectories, anthropogenic activities could drive the ocean toward widespread oxygen deficiency within the next thousand years (8). In this Review, we refer to “coastal waters” as systems that are strongly influenced by their watershed, and the “open ocean” as waters in which such influences are secondary.

Fig. 1 Oxygen has declined in both the open ocean and coastal waters during the past half-century. (A) Coastal waters where oxygen concentrations ≤61 μmol kg−1 (63 μmol liter−1 or 2 mg liter−1) have been reported (red) (8, 12). [Map created from data in (8) and updated by R. Diaz and authors] (B) Change in oxygen content of the global ocean in mol O 2 m−2 decade−1 (9). Most of the coastal systems shown here reported their first incidence of low oxygen levels after 1960. In some cases, low oxygen may have occurred earlier but was not detected or reported. In other systems (such as the Baltic Sea) that reported low levels of oxygen before 1960, low-oxygen areas have become more extensive and severe (59). Dashed-dotted, dashed, and solid lines delineate boundaries with oxygen concentrations <80, 40, and 20 μmol kg−1 ­ , respectively, at any depth within the water column (9). [Reproduced from (9)]

Fig. 2 Dissolved oxygen concentrations in the open ocean and the Baltic Sea. (A) Oxygen levels at a depth of 300 m in the open ocean. Major eastern boundary and Arabian Sea upwelling zones, where oxygen concentrations are lowest, are shown in magenta, but low oxygen levels can be detected in areas other than these major OMZs. At this depth, large areas of global ocean water have O 2 concentrations <100 μmol liter−1 (outlined and indicated in red). ETNP, eastern tropical North Pacific; ETSP, eastern tropical South Pacific; ETSA, eastern tropical South Atlantic; AS, Arabian Sea. [Max Planck Institute for Marine Microbiology, based on data from the World Ocean Atlas 2009] (B) Oxygen levels at the bottom of the Baltic Sea during 2012 (59). In recent years, low-oxygen areas have expanded to 60,000 km2 as a result of limited exchange, high anthropogenic nutrient loads, and warming waters (59) (red, O 2 concentration ≤63 μmol liter−1 [2 mg liter−1]; black, anoxia). [Reproduced from (59)]

The open ocean lost an estimated 2%, or 4.8 ± 2.1 petamoles (77 billion metric tons), of its oxygen over the past 50 years (9). Open-ocean oxygen-minimum zones (OMZs) have expanded by an area about the size of the European Union (4.5 million km2, based on water with <70 μmol kg−1 oxygen at 200 m of depth) (10), and the volume of water completely devoid of oxygen (anoxic) has more than quadrupled over the same period (9). Upwelling of oxygen-depleted water has intensified in severity and duration along some coasts, with serious biological consequences (11).

Since 1950, more than 500 sites in coastal waters have reported oxygen concentrations ≤2 mg liter−1 (=63 μmol liter−1 or ≅61 µmol kg-1), a threshold often used to delineate hypoxia (3, 12) (Fig. 1A). Fewer than 10% of these systems were known to have hypoxia before 1950. Many more water bodies may be affected, especially in developing nations where available monitoring data can be sparse and inadequately accessed even for waters receiving high levels of untreated human and agricultural waste. Oxygen continues to decline in some coastal systems despite substantial reductions in nutrient loads, which have improved other water quality metrics (such as levels of chlorophyll a) that are sensitive to nutrient enrichment (13).

Oxygen is naturally low or absent where biological oxygen consumption through respiration exceeds the rate of oxygen supplied by physical transport, air-sea fluxes, and photosynthesis for sufficient periods of time. A large variety of such systems exist, including the OMZs of the open ocean, the cores of some mode-water eddies, coastal upwelling zones, deep basins of semi-enclosed seas, deep fjords, and shallow productive waters with restricted circulation (14, 15). Whether natural or anthropogenically driven, however, low oxygen levels and anoxia leave a strong imprint on biogeochemical and ecological processes. Electron acceptors, such as Fe(III) and sulfate, that replace oxygen as conditions become anoxic yield less energy than aerobic respiration and constrain ecosystem energetics (16). Biodiversity, eukaryotic biomass, and energy-intensive ecological interactions such as predation are reduced (17–19), and energy is increasingly transferred to microbes (3, 16). As oxygen depletion becomes more severe, persistent, and widespread, a greater fraction of the ocean is losing its ability to support high-biomass, diverse animal assemblages and provide important ecosystem services.

But the paradox is that these areas, sometimes called dead zones, are far from dead. Instead they contribute to some of the world’s most productive fisheries harvested in the adjacent, oxygenated waters (20–22) and host thriving microbial assemblages that utilize a diversity of biogeochemical pathways (16). Eukaryote organisms that use low-oxygen habitats have evolved physiological and behavioral adaptations that enable them to extract, transport, and store sufficient oxygen, maintain aerobic metabolism, and reduce energy demand (23–26). Fishes, for example, adjust ventilation rate, cardiac activity, hemoglobin content, and O 2 binding and remodel gill morphology to increase lamellar surface area (27). For some small taxa, including nematodes and polychaetes, high surface area–to–volume ratios enhance diffusion and contribute to hypoxia tolerance (26). Metabolic depression (23, 25, 28) and high H 2 S tolerance (24) are also key adaptations by organisms to hypoxic and anoxic environments.

Causes of oxygen decline Global warming as a cause of oxygen loss in the open ocean The discovery of widespread oxygen loss in the open ocean during the past 50 years depended on repeated hydrographic observations that revealed oxygen declines at locations ranging from the northeast Pacific (29) and northern Atlantic (30) to tropical oceans (2). Greenhouse gas–driven global warming is the likely ultimate cause of this ongoing deoxygenation in many parts of the open ocean (31). For the upper ocean over the period 1958–2015, oxygen and heat content are highly correlated with sharp increases in both deoxygenation and ocean heat content, beginning in the mid-1980s (32). Ocean warming reduces the solubility of oxygen. Decreasing solubility is estimated to account for ~15% of current total global oxygen loss and >50% of the oxygen loss in the upper 1000 m of the ocean (9, 33). Warming also raises metabolic rates, thus accelerating the rate of oxygen consumption. Therefore, decomposition of sinking particles occurs faster, and remineralization of these particles is shifted toward shallower depths (34), resulting in a spatial redistribution but not necessarily a change in the magnitude of oxygen loss. Intensified stratification may account for the remaining 85% of global ocean oxygen loss by reducing ventilation—the transport of oxygen into the ocean interior—and by affecting the supply of nutrients controlling production of organic matter and its subsequent sinking out of the surface ocean. Warming exerts a direct influence on thermal stratification and indirectly enhances salinity-driven stratification through its effects on ice melt and precipitation. Increased stratification alters the mainly wind-driven circulation in the upper few hundred meters of the ocean and slows the deep overturning circulation (9). Reduced ventilation, which may also be influenced by decadal to multidecadal oscillations in atmospheric forcing patterns (35), has strong subsurface manifestations at relatively shallow ocean depths (100 to 300 m) in the low- to mid-latitude oceans and less pronounced signatures down to a few thousand meters at high latitudes. Oxygen declines closer to shore have also been found in some systems, including the California Current and lower Saint Lawrence Estuary, where the relative strength of various currents have changed and remineralization has increased (36, 37). There is general agreement between numerical models and observations about the total amount of oxygen loss in the surface ocean (38). There is also consensus that direct solubility effects do not explain the majority of oceanic oxygen decline (31). However, numerical models consistently simulate a decline in the total global ocean oxygen inventory equal to only about half that of the most recent observation-based estimate and also predict different spatial patterns of oxygen decline or, in some cases, increase (9, 31, 39). These discrepancies are most marked in the tropical thermocline (40). This is problematic for predictions of future deoxygenation, as these regions host large open-ocean OMZs, where a further decline in oxygen levels could have large impacts on ecosystems and biogeochemistry (Fig. 2A). It is also unclear how much ocean oxygen decline can be attributed to alterations in ventilation versus respiration. Mechanisms other than greenhouse gas–driven global warming may be at play in the observed ocean oxygen decline that are not well represented in current ocean models. For example, internal oscillations in the climate system, such as the Pacific Decadal Oscillation, affect ventilation processes and, eventually, oxygen distributions (35). Models predict that warming will strengthen winds that favor upwelling and the resulting transport of deeper waters onto upper slope and shelf environments in some coastal areas (41, 42), especially at high latitudes within upwelling systems that form along the eastern boundary of ocean basins (43). The predicted magnitude and direction of change is not uniform, however, either within individual large upwelling systems or among different systems. Upwelling in the southern Humboldt, southern Benguela, and northern Canary Eastern Boundary upwelling systems is predicted to increase in both duration and intensity by the end of the 21st century (43). Where the oxygen content of subsurface source waters declines, upwelling introduces water to the shelf that is both lower in oxygen and higher in CO 2 . Along the central Oregon coast of the United States in 2006, for example, anoxic waters upwelled to depths of <50 m within 2 km of shore, persisted for 4 months, and resulted in large-scale mortality of benthic macro-invertebrates (11). There are no prior records of such severe oxygen depletion over the continental shelf or within the OMZ in this area (11). Nutrient enrichment of coastal waters Sewage discharges have been known to deplete oxygen concentrations in estuaries since at least the late 1800s (44), and by the mid 1900s the link to agricultural fertilizer runoff was discussed (45). Nevertheless, the number and severity of hypoxic sites has continued to increase (Fig. 2B). The human population has nearly tripled since 1950 (46). Agricultural production has greatly increased to feed this growing population and meet demands for increased consumption of animal protein, resulting in a 10-fold increase in global fertilizer use over the same period (47). Nitrogen discharges from rivers to coastal waters increased by 43% in just 30 years from 1970 to 2000 (48), with more than three times as much nitrogen derived from agriculture as from sewage (49). Eutrophication occurs when nutrients (primarily N and P) and biomass from human waste and agriculture, as well as N deposition from fossil fuel combustion, stimulate the growth of algae and increase algal biomass. The enhanced primary and secondary production in surface waters increases the delivery rate of degradable organic matter to bottom waters where microbial decomposition by aerobic respiration consumes oxygen. Once oxygen levels are low, behavioral and biogeochemical feedbacks can hinder a return to higher-oxygen conditions (50). For example, burrowing invertebrates that introduce oxygen to sediments die or fail to recruit, and sediment phosphorus is released, fueling additional biological production in the water column and eventual increased oxygen consumption. Coastal systems vary substantially in their susceptibility to developing low oxygen concentrations. Low rates of vertical exchange within the water column reduce rates of oxygen resupply (51), and long water-retention times favor the accumulation of phytoplankton biomass (14) and its eventual subsurface degradation. Chesapeake Bay develops hypoxia and anoxia that persist for several months during late spring through early autumn and cover up to 30% of the system area. In contrast, the nearby Delaware Bay, which has weaker stratification and a shorter retention time, does not develop hypoxia, in spite of similar nutrient loads (52). Manila Bay is adjacent to a megacity and also receives similar loads on an annual basis, but it becomes hypoxic principally during the wet southwest monsoon period, when rainfall increases nutrient loads and stratification (53). Low oxygen in coastal waters and semi-enclosed seas can persist for minutes to thousands of years and may extend over spatial scales ranging from less than one to many thousands of square kilometers. Both local and remote drivers lead to temporal and spatial variations in hypoxia. Local weather can influence oxygen depletion in very shallow water through wind mixing and the effect of cloud cover on photosynthesis (54). At larger spatial scales, variations in wind direction and speed (55), precipitation and nutrient loads (56), sea surface temperature (57), and nutrient content of water masses transported into bottom layers of stratified coastal systems contribute to interannual and longer-period variations in hypoxic volume, duration, and rate of deoxygenation (14). Climate change in coastal waters Warming is predicted to exacerbate oxygen depletion in many nutrient-enriched coastal systems through mechanisms similar to those of the open ocean: increased intensity and duration of stratification, decreased oxygen solubility, and accelerated respiration (4, 58, 59). The current rate of oxygen decline in coastal areas exceeds that of the open ocean (60), however, likely reflecting the combined effects of increased warming of shallow water and higher concentrations of nutrients. Higher air temperatures can result in earlier onset and longer durations of hypoxia in eutrophic systems through effects on the seasonal timing of stratification and the rate of oxygen decline (58). An ensemble modeling study of the Baltic Sea projects declining oxygen under all but the most aggressive nutrient-reduction plans, owing to increased precipitation and consequent nutrient loads, decreased flux of oxygen from the atmosphere, and increased internal nutrient cycling. Even aggressive nutrient reduction is projected to yield far less benefit under climate change than under current conditions (61). Because of regional variations in the effects of global warming on precipitation and winds, the rate and direction of change in oxygen content is expected to vary among individual coastal water bodies (4, 58). Where precipitation increases, both stratification and nutrient discharges are expected to increase, with the reverse occurring in regions where precipitation decreases. Changes in seasonal patterns of precipitation and rates of evaporation can also be important. Coastal wetlands that remove nutrients before they reach open water are predicted to be lost as sea levels rise, decreasing capacity to remove excess nitrogen, but the rate of wetland inundation and the ability of wetlands to migrate landward will vary.

Effects of ocean deoxygenation Oxygen influences biological and biogeochemical processes at their most fundamental level (Fig. 3). As research is conducted in more habitats and using new tools and approaches, the range of effects of deoxygenation that have been identified, and the understanding of the mechanisms behind those effects, has increased substantially. Although 2 mg liter−1 (61 μmol kg−1) is a useful threshold for defining hypoxia when the goal is to quantify the number of systems or the spatial extent of oxygen-depleted waters, a more appropriate approach when considering biological and ecological effects is to simply define hypoxia as oxygen levels sufficiently low to affect key or sensitive processes. Organisms have widely varying oxygen tolerances, even in shallow coastal systems (19). In addition, because temperature affects not only oxygen supply (through its effect on solubility and diffusion) but also the respiratory demand by organisms, oxygen limitation for organisms is better expressed as a critical oxygen partial pressure below which specific organisms exhibit reduced metabolic functions than in terms of oxygen concentration (62, 63). Fig. 3 Life and death at low oxygen levels. (A) Animals using low-oxygen habitats exhibit a range of physiological, morphological, and behavioral adaptations. For example, teribellid worms (Neoamphitrite sp., Annelida) with large branchaea and high hemoglobin levels can survive in the extremely low oxygen levels found at 400 m depth in the Costa Rica Canyon. (B) Fish kills in aquaculture pens in Bolinao, Philippines, had major economic and health consequences for the local population. (C) The ctenophore Mnemiopsis leidyi is more tolerant of low oxygen than trophically equivalent fishes in its native habitat in the Chesapeake Bay and can use hypoxic areas from which fish are excluded. (D) A low-oxygen event caused extensive mortality of corals and associated organisms in Bocas del Toro, Panama. These events may be a more important source of mortality in coral reefs than previously assumed. PHOTOS: (CLOCKWISE FROM TOP LEFT) GREG ROUSE/SCRIPPS INSTITUTION OF OCEANOGRAPHY; PHILIPPINE DAILY INQUIRER/OPINION/MA. CERES P. DOYO; PETRA URBANEK/WIKIMEDIA COMMONS/HTTPS://CREATIVECOMMONS.ORG/LICENSES/BY-SA/4.0/; ARACDIO CASTILLO/SMITHSONIAN INSTITUTION Biological responses Ocean deoxygenation influences life processes from genes to emergent properties of ecosystems (Fig. 4). All obligate aerobic organisms have limits to the severity or duration of oxygen depletion for which they can compensate. Low oxygen levels can reduce survival and growth and alter behavior of individual organisms (3, 4, 26, 64). Reproduction can be impaired by reduced energy allocation to gamete production, as well as interference with gametogenesis, neuroendocrine function, and hormone production, and can ultimately affect populations and fisheries (65–67). Exposure to hypoxia can trigger epigenetic changes expressed in future generations, even if these generations are not exposed to hypoxia (68). Brief, repeated exposure to low oxygen can alter immune responses, increase disease, and reduce growth (69, 70). Fig. 4 Oxygen exerts a strong control over biological and biogeochemical processes in the open ocean and coastal waters. Whether oxygen patterns change over space, as with increasing depth, or over time, as the effects of nutrients and warming become more pronounced, animal diversity, biomass, and productivity decline with decreasing levels of oxygen. At the edge of low-oxygen zones, where nutrients are high and predators and their prey are concentrated into an oxygenated habitat, productivity can be very high, but even brief exposures to low oxygen levels can have strong negative effects. (Top) Well-oxygenated coral reef with abundant fish and invertebrate assemblages. (Middle) Low-oxygen event in Mobile Bay, United States, in which crabs and fish crowd into extreme shallows where oxygen levels are highest. (Bottom) Anoxic mud devoid of macrofauna. PHOTOS: (TOP) UXBONA/WIKIMEDIA COMMONS/HTTP://CREATIVECOMMONS.ORG/LICENSES/BY/3.0; (BOTTOM) B. FERTIG/COURTESY OF THE INTEGRATION AND APPLICATION NETWORK, UNIVERSITY OF MARYLAND CENTER FOR ENVIRONMENTAL SCIENCE In both oceanic and coastal systems, vertical and horizontal distributions of organisms follow oxygen gradients and discontinuities, and migratory behavior is constrained in response to both oxygen availability and the ways that oxygen alters the distributions of predators and prey (64, 71). Because oxygen tolerances and behavioral responses to low oxygen levels vary among species, taxa, trophic groups, and with mobility (19), encounter rates, feeding opportunities, and the structure of marine food webs change. Movement to avoid low oxygen can result in lost feeding opportunities on low-oxygen–tolerant prey and can increase energy expended in swimming (19, 70). Hypoxia effects on vision, a function that is highly oxygen intensive, may contribute to these constraints, in part through changing light requirements (72). The presence and expansion of low–water column oxygen reduces diel migration depths, compressing vertical habitat and shoaling distributions of fishery species and their prey (73–75). For pelagic species, habitat compression can increase vulnerability to predation as animals are restricted to shallower, better-lit waters and can increase vulnerability to fishing by predictably aggregating individuals at shallower or lateral edges of low-oxygen zones (71, 76–78). For demersal species, hypoxia-induced habitat compression can lead to crowding and increased competition for prey (73), potentially resulting in decreased body condition of important fishery species such as Baltic cod (79). In contrast, migration into and out of hypoxic waters can allow some animals to utilize oxygen-depleted habitats for predator avoidance or to feed on hypoxia-tolerant prey, and then to return to more highly oxygenated depths or locations (23, 80). Habitat compression may also enhance trophic efficiency in upwelling regions, contributing to their extraordinary fish productivity (20, 21). Some hypoxia-tolerant fish and invertebrate species expand their ranges as OMZs expand (28, 81), and their predators and competitors are excluded. Multiple stressors Deoxygenation is mechanistically linked to other ocean stressors, including warming (82) and acidification (83), and thus it is often their combined effects that shape marine ecosystems (84, 85). Because hypoxia limits energy acquisition, it is especially likely to exacerbate effects of co-occurring stressors that increase energy demands (65). The thermal tolerance of ectotherms is limited by their capacity to meet the oxygen demands of aerobic metabolism (62). Increased temperature elevates oxygen demand while simultaneously reducing oxygen supply, thus expanding the area of the oceans and coastal waters where oxygen is insufficient. Through this mechanism, ocean warming is predicted to result in shifts in the distribution of fishes and invertebrates poleward by tens to hundreds of kilometers per decade, shifts into deeper waters, and local extinctions (63, 86). Models project that warming combined with even modest O 2 declines (<10 μmol kg−1) can cause declines in important fishery species that are sensitive to low oxygen levels (87). Physiological oxygen limitation in warming waters is also predicted to reduce maximum sizes of many fish species, including some that support important fisheries (88). Increased respiration that causes deoxygenation also amplifies the problem of ocean acidification because the by-product of aerobic respiration is CO 2 . Temporal and spatial variations in oxygen in subpycnocline and shallow eutrophic waters are accompanied by correlated fluctuations in CO 2 . In highly productive estuarine, coastal, and upwelling regions, oxygen concentrations and pH can exhibit extreme fluctuations episodically and on diel, tidal, lunar, and seasonal cycles (83, 89). Elevated CO 2 can sometimes decrease the oxygen affinity of respiratory proteins (90), reduce tolerance to low oxygen by increasing the metabolic cost of maintaining acid-base balance (91), and reduce responses to low oxygen that would otherwise increase survival (92). Neither the occurrence nor the magnitude of cases in which acidification exacerbates the effects of low oxygen are currently predictable (83). Other covarying factors, such as nutrients and fisheries dynamics, can mask or compensate for effects of deoxygenation, complicating management decisions. Fisheries management is designed to adjust effort and catch as population abundance changes (93). Thus, direct and indirect effects of deoxygenation on a harvested population may not be easily traceable in monitoring or catch data because management actions adjust for the loss in abundance. In addition, high nutrient loads can stimulate production in a habitat that remains well oxygenated, at least partially offsetting lost production within a hypoxic habitat (52). Total landings of finfish, cephalopods, and large mobile decapods are positively correlated with nitrogen loads (22), in spite of hypoxia in bottom waters (52). The conflation of habitat loss and nutrient enrichment is prominent in upwelling zones, as well as eutrophic coastal waters. Increased upwelling of nutrient-rich, oxygen-depleted waters from the 1820s to the 20th century has increased primary and fish productivity off the coast of Peru, for example (94). However, there are limits to the extent of hypoxia that can form before total system-wide fishery landings decline. In addition, individual species dependent on a degraded habitat may decline, whereas other species able to use more highly oxygenated habitats within the same system thrive (52). Biogeochemistry Oxygen availability affects remineralization processes and associated sources and sinks of important nutrient elements, such as nitrogen, phosphorus, and iron. Even when occurring in relatively small, low-oxygen regions, the effects of oxygen-dependent nutrient-cycling processes are communicated to the wider ocean by circulation. Hence, local changes within OMZs can influence nutrient budgets, biological productivity, and carbon fixation on regional to global scales, and changes in oxygen-depleted bottom waters of coastal systems can affect entire water bodies. In addition to nitrogen, phosphorus, and iron, which are discussed in more detail below, a wide range of other elements are affected by oxygen conditions. Hydrogen sulfide, which is toxic to most aerobic organisms, is produced in anoxic sediments and can be released to the overlying water column, especially during upwelling events (16). Methane, a potent greenhouse gas, is also produced in anoxic sediments, but methanotrophic activity limits its release to the atmosphere (95). Hypoxia increases conversion of As(V) to the more toxic As(III) (96). Cadmium, copper, and zinc form sulfide precipitates in the presence of anoxic or extremely oxygen-deficient waters and sulfides (97). This process may affect the global distribution of trace metals, some of which serve as micronutrients for plankton growth, but the importance of such controls is yet to be fully evaluated. Where oxygen levels are extremely low or absent, anaerobic remineralization of organic matter by denitrification and anaerobic ammonium oxidation (anammox) leads to a net loss of bioavailable nitrogen through the formation of dinitrogen gas (N 2 ). Recent investigations have reported functionally anoxic conditions within open-ocean OMZs (98) and have shown that traces of oxygen at nanomolar levels can inhibit anaerobic processes, such as denitrification (99). Total loss of bioavailable nitrogen from the open ocean is currently estimated to be 65 to 80 Tg year−1 from the water column and 130 to 270 Tg year−1 from sediments (100). Analysis and modeling of global benthic data also indicate that denitrification in sediments underlying high-nutrient and low-oxygen areas (such as OMZs) removes around three times as much nitrogen per unit of carbon deposited as sediments underlying highly oxygenated water and accounts for ~10% (i.e., 15 Tg year−1) of global benthic denitrification (101). Similarly enhanced benthic denitrification has been observed at very low bottom-water oxygen concentrations in eutrophic coastal systems (102, 103) and in the oxycline of the water column, comparable to OMZs (104). Certainly, there is genetic potential for water column denitrification to occur once anoxic conditions are reached. A by-product of both nitrification and denitrification is nitrous oxide, N 2 O, a potent greenhouse gas (105). The amount of N 2 O produced is strongly dependent on prevailing oxygen conditions. Production of N 2 O is enhanced at the oxic-suboxic boundaries of low-oxygen waters, but N 2 O is further reduced to N 2 in anoxic conditions (95), so small differences in oxygen concentration determine whether there is net production or consumption of this gas. Low-oxygen zones (including shelf and coastal areas) contribute a large fraction of the total oceanic N 2 O emission to the atmosphere, and expansion of these systems may substantially enhance oceanic N 2 O emissions (95). Record air-sea N 2 O fluxes have recently been observed above the OMZ in the eastern tropical South Pacific (106). Although our understanding of the relationships among oxygen, remineralization of bioavailable N, and production of N 2 O has greatly increased, the consequences of a shift in these relationships in a warming world with increased O 2 -depleted waters are less well understood. Continued deoxygenation of OMZ waters is expected to increase the volume of water where denitrification and anammox occur and may lead to increased marine nitrogen loss (99). This could alter the ocean’s nitrogen inventory and, eventually, biological production on millennial time scales if nitrogen losses are not compensated for by increases in nitrogen fixation (107). However, the feedbacks that link nitrogen loss and nitrogen fixation remain enigmatic (101). The direction and magnitude of change in the N 2 O budget and air-sea N 2 O flux are also unclear because increased stratification could reduce the amount of N 2 O that reaches the surface ocean and escapes to the atmosphere (108). The supply of phosphorus and iron released from the sediments is generally enhanced under anoxic conditions (109, 110). These nutrients have the potential to further stimulate biological production if they reach well-lit surface waters, such as above the OMZs associated with coastal upwelling regions and the surface layer of coastal waters. Elevated dissolved inorganic phosphorus and chlorophyll are found in surface waters when anoxia occurs in fjords and estuaries (111), and, in some systems, deep waters supply as much phosphorus to productive surface layers as do watershed discharges (112). Increased productivity will tend to increase oxygen consumption, may increase the sediment area in contact with low-oxygen waters, and may eventually lead to further release of phosphorus and iron from the sediment. There is evidence for this positive feedback in enclosed seas such as the Baltic Sea, where enhanced nitrogen fixation in response to deoxygenation has led to the recent proliferation of undesirable cyanobacterial blooms that can be toxic and have adverse effects on ecosystems and society (102). Enhanced phosphate and iron levels may generally favor nitrogen fixation by diazotrophs, especially in the presence of nitrogen loss when ordinary plankton are driven toward nitrogen limitation.

Predicting oxygen decline Sound management of marine ecosystems is based on reliable predictions under a range of future scenarios and an understanding of associated uncertainties. Numerical models that can project effects of climate change and eutrophication on oxygen availability in the open ocean and in coastal systems can offer these predictions. Current state-of-the-art global models generally agree that the total amount of oxygen loss will be a few percent by the end of the century (31), a decline that could have substantial biogeochemical and ecological effects. However, there is little agreement among models about the spatial distribution of future low-oxygen zones having <100 μmol O 2 kg−1 (113) or the spatial patterns of O 2 changes that have occurred over the past several decades (40). This uncertainty currently limits our ability to reliably predict the regional impact of climate warming on open-ocean OMZs and, hence, on oxygen-sensitive biogeochemical processes, including the nitrogen budget. More realistic and detailed inclusion of mechanisms other than CO 2 -driven global warming—such as atmospheric nutrient deposition and decadal- to multidecadal-scale climate variability (especially fluctuations in wind patterns)—may improve agreement among models and, therefore, their ability to predict the spatial distribution of past and future low-oxygen areas. Predicting oxygen levels in individual coastal water bodies requires modeling the variability in these systems, which is tightly governed by interactions with the land, atmosphere, sediment, and offshore waters at small space and time scales. This can be achieved by current estuary-specific and regional three-dimensional coupled hydrodynamic–water quality models (67); these and other state-of-the-art modeling approaches deserve broader implementation. However, model performance can be hampered by the use of forcing data, such as river discharges and atmospheric conditions, that lack sufficiently resolved spatial and temporal detail. Projections of future deoxygenation also require reliable information on changes in key parameters and interactions under a range of climate change and nutrient management scenarios and benefit from the use of approaches that explicitly model connections along the river–estuary–adjacent ocean or sea continuum. Projections of local changes in timing and magnitude of precipitation and warming are especially important. Future characteristics of human populations, such as rates of population growth, the effect of climate change on the geography of population centers, and the effects of education and income on demands for improved sanitation and animal protein are also needed because of their influence on nutrient discharges at both local and global scales. Improving predictions critical for management in both the open ocean and coastal systems will require increased observations from field measurements and experiments to constrain and refine models. Ideally, such data should include representations of future environmental conditions. An improved mechanistic understanding of feedbacks that limit or exacerbate oxygen depletion and alter oxygen-sensitive biogeochemical cycles is especially important. In the open ocean, information is needed on transport mechanisms—such as small-scale mixing processes (114), stirring, and transport by mesoscale structures (115)—that influence oxygen distributions. Advanced observation networks can provide data to underpin the development of an improved mechanistic understanding and the refinement of current models. Drifters and autonomous platforms ranging from Argo floats to tethered arrays provide real-time data and have the potential to increase knowledge of oxygen dynamics at the small spatial and temporal scales that are ultimately needed for both regional and global models. High-resolution measurements have revealed the small-scale patchiness of oxygen-sensitive processes in space and time (99, 106) and have provided new insight into the biogeochemistry of OMZs (98). Optical oxygen sensors mounted on Argo floats or gliders can now use atmospheric oxygen to perform ongoing, in situ calibrations throughout the float (116) or glider lifetime. The accuracy of autonomous measurements of in situ oxygen concentrations ≤1 μmol kg−1 has been improved by the development of STOX (switchable trace amount oxygen) sensors (117), and novel trace-oxygen optical sensors can now provide precise oxygen quantification in OMZs and detect oxygen concentrations as low as ~5 nmol kg−1 (118). The new platforms and sensors facilitate the implementation of regional and global oxygen observatories targeted toward the much-improved monitoring and, eventually, modeling and management of deoxygenation. For coastal waters, it is also important to develop sensors that are affordable for use in low-income developing countries (LIDCs) and that can be used to generate reliable data from citizen science.

Predicting effects at large scales of space, time, and ecological organization Improved management and conservation of open-ocean and coastal systems requires predictions of the effects of deoxygenation at spatial, temporal, and ecological scales most relevant to the ecosystem services provided by these waters. Although research has clearly shown that low-oxygen zones reduce habitat for species dependent on aerobic respiration and that exposure to suboptimal oxygen levels leads to a host of negative effects on individuals, identifying effects of expanding deoxygenation at the scale of populations or fisheries stocks has been difficult, particularly for mobile species (52, 119). A similar problem applies to scaling up oxygen-sensitive biogeochemical processes to predict feedbacks on global ocean nutrient inventories and Earth’s climate. Scaling to predict effects on food webs and fisheries is confounded by compensatory mechanisms; examples include increased production of planktonic prey under high nutrient loads and increased encounter rates between predators and their prey when they are squeezed into smaller oxygenated habitat space (52, 119, 120). In addition, populations maintained below their habitat-dependent carrying capacity by fisheries or other factors may not be as strongly affected by the loss of habitat as species nearer their carrying capacity. In these cases, habitats suitable for feeding and other life functions may remain sufficient, even when their size is reduced by low oxygen. The most promising approaches to scaling employ a suite of methods ranging from detailed mechanistic studies to large-scale field efforts, as well as new and increasingly sophisticated analyses and modeling tools that address spatial processes (120), temporal fluctuations (121, 122), and the role of co-occurring stressors. Consideration of the effects of early hypoxia exposure on later life stages after organisms migrate to more highly oxygenated habitats can indicate the large spatial scales over which even spatially limited hypoxia can have impacts (123). Paleoecological approaches are critical for gaining a long-term perspective beyond the time scale of biological and oceanographic observation (94, 124). Even sophisticated approaches will not always provide support for large-scale negative effects of deoxygenation, but eliminating deoxygenation as a major cause of population declines is also important to effective management. Increased research is most needed in locations where deoxygenation is likely to affect local economies and food security. Place-based, artisanal fisheries with little capacity to relocate as local habitat degrades are more likely to suffer from deoxygenation than industrialized fisheries with highly mobile fishing fleets. Aquaculture, in particular, can be a critical intersection between deoxygenation and societal effects because aquaculture itself can cause deoxygenation (125), and animals restrained in nets and cages are unable to escape harmful oxygen conditions. But critically, much of the world’s marine aquaculture is done in LIDCs. Fish kills in aquaculture pens (125) can compromise livelihoods and can directly harm human health when low incomes and food insecurity lead to consumption of fish killed by low-oxygen conditions (126). Coral reefs contribute to food security and local economies through their value to tourism and storm protection, as well as food production. Recent research indicates that low oxygen may be an increasingly important factor in the mortality of corals and associated fauna in some regions and that low-oxygen problems on coral reefs are likely underreported (127).

Reducing deoxygenation and its negative effects Local, national, and global efforts are required to limit further oxygen declines, restore oxygen to previously well-oxygenated environments, and enhance the resilience of ecosystems affected by deoxygenation. At their most basic level, the actions needed to address deoxygenation—reducing nutrient loads to coastal waters and reducing greenhouse gas emissions globally—have substantial benefits to society above and beyond improving oxygen conditions. Improved sanitation can benefit human health directly while also reducing coastal nutrient loads. Eliminating excess and inefficiently applied fertilizer can reduce costs to farmers (128) and emissions of N 2 O (129) and may decrease nitrogen loads to waterways. Eliminating emissions from combustion of fossil fuels can reduce greenhouse gas production and may result in decreased atmospheric deposition of nitrogen that stimulates primary production in coastal waters (130). Reducing or eliminating greenhouse gas emissions can, more generally, lower the threats from global warming and ocean acidification and, simultaneously, reduce ocean deoxygenation. Improved management of fisheries and marine habitats that are sensitive to the development and effects of low oxygen helps to protect economies, livelihoods, and food security (Fig. 5). Fig. 5 Strategies for deoxygenation management and policy-making. (Left) Multiple management actions can help to mitigate deoxygenation. Key among these are reductions in (i) anthropogenic nutrient inputs from land, which will reduce algal blooms and subsequent oxygen drawdown; (ii) greenhouse gas emissions, which will slow warming; and (iii) waste production from aquaculture, which will contribute to oxygen consumption. (Right) Adaptive measures can reduce stress and may increase resilience of marine ecosystems that face deoxygenation. Examples include creating protected areas that can serve as refugia in hypoxic areas or during hypoxic events; incorporating oxygen effects on population distribution and dynamics into catch limits and closures, as has been done for rockfish; and adopting gear regulations that reduce stress on vulnerable fisheries or ecosystems. (Bottom) Both types of actions benefit from enhanced oxygen and biological monitoring, including access to real-time data that can elicit quick management responses, as well as more synthetic analyses that might reveal spatial and temporal trends. PHOTO: (TOP LEFT) DAVID DIXON/WIKIMEDIA COMMONS/HTTPS://CREATIVECOMMONS.ORG/LICENSES/BY-SA/2.0/; (BOTTOM RIGHT) SMITHSONIAN ENVIRONMENTAL RESEARCH CENTER; (BOTTOM LEFT) NOAA Failure to reduce nutrient loads, at all or sufficiently, is the primary reason that oxygen levels have not improved in most coastal systems. But some of the reasons for slow progress are inherent in the problem itself. High sedimentary oxygen demand can continue for decades as accumulated organic matter degrades (57), phosphorus may continue to be released from sediments once oxygen thresholds have been crossed (102), and nitrogen leached from soils and dissolved in groundwater continues to enter waterways for decades (131). Increasing temperatures can require greater reductions in nutrients to meet the same oxygen goals (57, 61). Because of changing conditions and the nonlinearity of ecological processes, ecosystems may not return to their predisturbed state even if conditions that caused the initial deoxygenation are eased (132). To maintain the current conditions, per capita reductions in nutrient discharges and greenhouse gas emissions will need to increase as the global population continues to grow. Nevertheless, considerable improvements have been observed in some coastal systems through implementation of a wide range of strategies to reduce the input of nutrients and biomass (133). Some of the most notable improvements have occurred in systems such as the Thames and Delaware River estuaries, where steps to keep raw sewage out of the rivers and, eventually, to treat wastewater substantially decreased biological oxygen demand (133). In the Maryland portion of the Chesapeake Bay, where both point- and nonpoint-source nutrient reduction strategies have been implemented, oxygen concentrations <0.1 mg liter−1 (<3 μmol kg−1) have rarely been measured since 2014—a marked contrast to the first 30 years of frequent monitoring (1984–2013) (134). In one Chesapeake tributary, the Potomac River, nitrogen reductions due to better air quality have played the major role in water quality improvements (135). Additionally, better understanding of deoxygenation may enable a range of adaptive, protective actions for fisheries and the habitats that sustain them (Fig. 5). An integrated framework that combines modeling, observations, and experiments in a multiple-stressor environment and involves the full range of stakeholders (e.g., scientists, local governments, intergovernmental bodies, industrial sectors, and the public) will facilitate the development and implementation of the most ecologically and economically effective plans to reverse deoxygenation (Fig. 6). Networks of research scientists, such as the Intergovernmental Oceanographic Commission (IOC)–UNESCO Global Ocean Oxygen Network (www.unesco.org/new/en/natural-sciences/ioc-oceans/sections-and-programmes/ocean-sciences/global-ocean-oxygen-network/), as well as groups with more limited geographic and disciplinary scope, can help to keep the process updated and to build capacity in parts of the world where improved technology and training are needed. The key to effective management is raised awareness of the phenomenon of deoxygenation, as well as its causes, consequences, and remediation measures. Fig. 6 Monitoring in coastal waters and the open ocean enables documentation of deoxygenation and, in some cases, improved oxygen conditions. In shallow water, handheld, continuous, and shipboard sensors are used worldwide. In the open ocean and nearshore waters, global arrays of sensors (such as the Argo floats), shipboard measurements, and deep platforms and profilers provide data to validate global models. Archiving data in well-documented databases accessible by all stakeholders facilitates scientific and management advances and public engagement. Experiments and field studies at scales ranging from genes to ecosystems provide information to predict the effects of low oxygen levels on ecological processes and services and are also used to develop fisheries and ecosystem models. Model projections and analyses of deoxygenation and its effects inform management and policy at both local and multinational scales and provide the basis for strategies to combat deoxygenation. IMAGES: (TOP ROW, LEFT) © CSIRO AUSTRALIA; (TOP ROW, MIDDLE) OCEAN OBSERVATORIES INITIATIVE/NSF; (SECOND ROW, MIDDLE) NOAA; (THIRD ROW, LEFT) AUGUST LINNMAN/WIKIMEDIA COMMONS/HTTPS://CREATIVECOMMONS.ORG/LICENSES/BY-SA/2.0/; (BOTTOM ROW) TUUKKA TROBERG/HELCOM

http://www.sciencemag.org/about/science-licenses-journal-article-reuse This is an article distributed under the terms of the Science Journals Default License."
17,US public divided on online abuse: Pew,"The US public is divided over whether certain behaviours can be categorised as online harassment, according to a new survey by Pew. Although most respondents agreed that direct personal threats constituted harassment, opinion differed as to whether unkind messages, or the public sharing of a private conversation, met the requirements. The survey posed a series of fictional scenarios, asking those surveyed which actions depicted constituted harassment. The majority felt that social media platforms should intervene in the case of threats, though respondents were divided over the platforms’ responsibilities in other areas.
",http://www.pewinternet.org/2018/01/04/crossing-the-line-what-counts-as-online-harassment/,2018-01-04 00:00:00,"Pew Research Center surveys have found that online harassment is a common phenomenon in the digital lives of many Americans, and that a majority of Americans feel harassment online is a major problem. Even so, there is considerable debate over what online harassment actually means in practice.

In an effort to examine more deeply where people “draw the line” when it comes to online harassment, the Center conducted a survey in which respondents were presented with fictional scenarios depicting different types of escalating online interactions. The survey then asked them to indicate which specific elements of the story they considered to be harassment.

Their answers indicate that Americans broadly agree that certain behaviors are beyond the pale. For instance, in various contexts most agree that online harassment occurs when people make direct personal threats against others. At the same time, the public is much more divided over whether or not other behaviors – such as sending unkind messages or publicly sharing a private conversation – constitute online harassment.

In two vignettes, respondents were asked if and when the social media platforms where the incidents were occurring should have stepped in and addressed the unfolding events. Again, majorities agree that the platforms should step in to address behaviors such as threatening messages. But public views are more split when it comes to the responsibilities of the platforms at other points in these incidents.

Scenario 1: A private disagreement between friends that becomes public and escalates in severity

People’s perceptions of online harassment incidents can often depend on who is involved in the conflict, as well as whether that conflict plays out publicly or in private. The first scenario in the survey presented respondents with an example of a private disagreement between a fictional character named “David” and his friend over a sensitive political issue. The conversation begins in a private messaging thread but then becomes public and escalates in severity:

“David and his friend are messaging privately about a sensitive political issue on which they disagree. David says something that offends his friend, who forwards the conversation to some people they know. One of those people shares the conversation publicly on a social media account, and David receives unkind messages from strangers. The original conversation is then reposted on an account with thousands of followers, and David receives messages that are vulgar. Eventually someone posts David’s phone number and home address online, and David starts to receive threatening messages.”

The vast majority of Americans (89%) agree that David does experience online harassment at some point in this conflict. Just 4% feel that he does not experience online harassment at least somewhere during the episode, and 7% say they are not sure if he was harassed or not. Although there are some modest demographic differences on this question, sizable majorities of Americans across a wide range of groups agree that this scenario as a whole does in fact involve online harassment.

When asked to identify which specific elements of the scenario they consider online harassment, only a small share of Americans (5%) think the initial disagreement when David offends his friend qualifies. The public is more evenly divided on the next two elements of the scenario: 48% think it constitutes online harassment when David’s friend forwards their private conversation to other people, while 52% do not deem it harassment. Similarly, 54% say it counts as harassment when someone then shares the conversation publicly on social media, while 46% think it does not.

There is relatively broad consensus on the remaining elements of the scenario. Substantial shares of Americans think David experiences online harassment when he begins to receive unkind messages from strangers (72%), when those messages become vulgar (82%), when his personal information is posted online (85%), and when he starts to receive threatening messages (85%).

Views of this scenario differ little based on the gender of the main character

The gender of the scenario’s lead character has little impact on Americans’ perceptions of whether online harassment did or did not occur in this situation. A separate group of respondents was given an identical scenario to consider but with a woman as the lead character instead of a man. Some 91% of Americans feel that the scenario involving a female protagonist qualifies as online harassment, compared with 89% in the scenario involving a man. And their responses to the specific elements of the story are also nearly identical in each version.

Scenario 2: Harassment involving sexism

The second scenario in the survey used a story involving a character named Julie to explore how Americans view online harassment issues involving sexism and sexual harassment:

“Julie posts on her social media account, defending one side of a controversial political issue. A few people reply to her, with some supporting and some opposing her. As more people see her post, Julie receives unkind messages. Eventually her post is shared by a popular blogger with thousands of followers, and Julie receives vulgar messages that insult her looks and sexual behavior. She also notices people posting pictures of her that have been edited to include sexual images. Eventually, she receives threatening messages.”

As was true in the preceding scenario, the vast majority of Americans (89%) agree that Julie does indeed experience online harassment at some point in this scenario. Another 6% feel that Julie was not harassed at any point in the encounter, while 5% say they are unsure if this scenario involves harassment or not.

And as was the case in the preceding scenario, the public has differing views on which aspects of this story represent online harassment. A very small share of Americans (3%) think Julie’s initial disagreement with her friends counts as online harassment. Some 43% consider it harassment when she begins to receive unkind messages, while around one-in-five (17%) consider it harassment when her post is shared by the popular blogger with thousands of followers. Meanwhile, substantial majorities of Americans think Julie is being harassed when she receives vulgar messages about her looks and sexual behavior (85%), when her picture is edited to include sexual images (84%), and when she receives threatening messages (85%).

Along with asking respondents to identify which specific elements of this scenario count as online harassment, this scenario included a second set of questions about when – if it all – people think the social media service where this incident was occurring should have stepped in to address the behaviors in question. These findings indicate that the public has a somewhat different standard for behaviors that constitute online harassment, as opposed to behaviors that necessitate a response from online platforms.

For example, some 43% of Americans consider it to be online harassment when Julie receives unkind messages from the people reading her post – but just 20% think that the platform should have stepped in to address that behavior when it occurred. The public’s attitudes diverge in similar ways on some of the more severe behaviors in the scenario. Most prominently, 85% of Americans think that Julie experiences online harassment when she begins to receive vulgar messages about her looks and sexual behavior. But substantially fewer (although still a majority at 66%) think that the social media platform has an obligation to step in and address that behavior.

Women are more likely than men to view certain behaviors in this scenario as harassing

The vast majority of both men and women feel that Julie does in fact experience online harassment in this scenario. But at the same time, men and women respond somewhat differently to some of the specific elements of the scenario. Most notably, women are roughly three times as likely as men to consider it online harassment when Julie’s post is shared on social media by the blogger (24% vs. 9%), and they are also substantially more likely to consider it harassment when Julie first begins to receive unkind messages (50% vs. 35%). And although roughly eight-in-ten men consider it harassment when Julie receives vulgar messages, when she sees people editing her picture to include sexual imagery, and when she receives threatening messages, in each case that point of view is shared by roughly nine-in-ten women.

Scenario 3: Harassment involving racism

The final scenario in the survey used a story involving a character named John to explore how Americans view online harassment issues in the context of racially motivated content. This scenario is nearly identical to the preceding one involving “Julie” and sexual harassment but with racial rather than sexual overtones:

“John posts on his social media account, defending one side of a controversial political issue. A few people reply to him, with some supporting and some opposing him. As more people see his post, John receives unkind messages. Eventually his post is shared by a popular blogger with thousands of followers, and John receives vulgar messages that make racial insults and use a common racial slur. He also notices people posting pictures of him that have been edited to include racially insensitive images. Eventually, he receives threatening messages.”

In many ways, Americans’ views on this scenario mirror those in the previous scenario involving sexual harassment. Fully 85% of adults believe John experiences online harassment in this scenario, while 6% feel he does not face harassment, and 10% are unsure if this scenario involves online harassment or not.

They also respond in largely similar ways when asked which specific elements of the story constitute harassment. Very few Americans think that John’s initial social media argument constitutes online harassment, but sizable majorities agree that John experiences harassment when he receives vulgar messages with racially insulting language (82%), when his picture is edited to include racially insensitive images (80%), and when he receives personal threats (82%).

And as with the case of the scenario involving sexual content, Americans have a somewhat different threshold for behavior that constitutes online harassment as opposed to behavior that deserves a response by the social media platform hosting that behavior. For instance, 80% of Americans think it constitutes online harassment when people begin posting pictures of John that include racially insensitive imagery, but 57% think that the social media service should have stepped in to address that behavior.

Slightly larger share of the public thinks social media platforms should step in for behaviors involving sexual harassment than for behaviors involving racial harassment

The scenarios involving “John” and “Julie” are generally identical in content, with the former involving explicitly racial content and the latter involving sexual content. Overall, similar shares of Americans view these scenarios as involving online harassment at some point. But slightly larger shares of the public – although a majority of Americans in each instance – think the social media platform should have stepped in at various times during the scenario involving Julie, as opposed to the scenario involving John:"
18,Kremlin may investigate failings in Russia's space programme,"The Kremlin is considering an official inquiry into problems in Russia's space programme. A failed launch from the country's new facility in the Far East in November led to the loss of a Russian weather satellite and nearly 20 micro-satellites from other countries, while communication links with a Russian-built communications satellite launched on behalf of Angola have also been lost. Deputy Prime Minister Dmitry Rogozin, who has responsibility for the space industry, has been publicly critical of the country's space corporation, Roscosmos, and a Kremlin spokesman said the situation warrants a thorough analysis.
",https://tsarizm.com/news/2018/01/02/kremlin-looks-probe-russian-space-industry-failures/,2018-01-03 18:06:51.693000,"

Image by Kremlin.ru

A spat of recent failures in Russia’s space industry has caused the Kremlin to consider an official probe of problems in the sector. Kremlin spokesman, Dmitry Peskov, said this week that authorities warrant a thorough analysis of the situation in the space industry, reported Associated Press.

4GB – A Georgian Electronic Music Festival In Soviet Style: Watch

“A Russian weather satellite and nearly 20 micro-satellites from other nations were lost following a failed launch from Russia’s new cosmodrome in the Far East on Nov. 28. And in another blow to the Russian space industry, communications with a Russian-built communications satellite for Angola, the African nation’s first space vehicle, were lost following its launch on Tuesday,” wrote AP.

Russian Deputy Prime Minister Rogozin, who’s portfolio includes space operations, was critical of Russian space corporation, Roscosmos, declaring Roscosmos was “trying to prove that failures occur not because of mistakes in management but just due to some ‘circumstances.'”

SpaceX Says It Will Take 65% Of Global Commercial Launch Market

Tsarizm has previously reported that Russia has lost a large percentage of the commercial launch market in recent years due to the rise of SpaceX and other commercial operators in the West, and failures such as the Kremlin has seen recently which has damaged Russia’s once strong reputation in space.

The new Russian launch pad at Vostochny has been marred with delays and accusations of corruption. Russian President Vladimir Putin himself publicly called for those in charge of the project to be punished. The work horse for Moscow’s space efforts previously has been the Russia-leased Baikonur launch pad in Kazakhstan, where heavier launches still take place until Vostochny can be finished in 2021.

Russian Space Program Close To Collapse

The loss of market share in the commercial launch space has put a critical source of revenue and foreign currency in crisis."
19,Frail Patients Losing Access To Dental House Calls,"Recent policy changes in California has endangered the dental care of a rising number of patients who are unable to receive dental care as they are too frail to leave their nursing homes. The state has slashed payments to dental care providers and hygienists and created a cumbersome preauthorization process which is hindering the delivery of dental care as part of Denti-Cal. Denti-Cal, the publicly funded program for the poor, reduced the rate for a simple cleaning procedure from $130 to $55 which has seen hygienists unable to continue treating their patients. The Department of Health Care Services maintains it made the changes to reduce unnecessary treatment, but the hygienists maintain that the new regulations victimize the most vulnerable patients. Whilst dental hygienists are allowed to practice without direct supervision in 40 states, rules governing patient choice, reimbursement and preauthorization rates vary. Since reducing payments for maintenance cleaning, many hygienists have stopped seeing their patients, and a lawsuit was filed, arguing that the decision was made without obtaining federal approval. At the same time as the reductions, requirements were also imposed for x-rays but this is an especially difficult process when working with disabled and elderly patients, should the requests even be granted. State data indicates a disparity between hygienists’ observations and state approval, which suggests that between July 2016 to July 2017, 10,000 of almost 13,000 requests were granted for deep cleaning, paying $2.5 million in the process. These figures are disputed by hygienists, one of whom claims that she lost 70 percent of her Denti-Cal patients because of the changes. This is the first time in 20 years, since dental hygienists were permitted to act independently that preauthorization has been required.
",https://khn.org/news/frail-patients-losing-access-to-dental-house-calls/,2018-01-03 17:37:23.147000,"RANCHO CUCAMONGA, Calif. — Devon Rising shakes his head and tries to cover his face with his hands. It’s time to get his few remaining teeth cleaned, and he fusses for a bit.

Gita Aminloo, his dental hygienist, tries to calm him by singing “Itsy Bitsy Spider,” the classic children’s song.

Rising, 42, is mentally disabled and blind. He has cerebral palsy and suffers from seizures. It’s hard for him to get to a dentist’s office, so Aminloo brought her dental picks, brushes and other tools to him at the residential care facility he shares with several other people who have developmental disabilities.

Rising is among a vulnerable class of patients who are poor and so frail they can’t leave the nursing home or, in his case, the board-and-care home to visit dentists. Instead, they rely on specially trained dental hygienists like Aminloo, who come to them.

But this may be the last time Aminloo cleans Rising’s teeth. And it’s not because of his resistance.

Hygienists say some of their patients are no longer getting the critical dental care they need because of recent policy changes: The state dramatically slashed payment to providers and created a preauthorization process they call cumbersome.

In 2016, Denti-Cal, the publicly funded dental program for the poor, cut the rate for a common cleaning procedure for these fragile patients from $130 to $55. Hygienists say they can’t afford to continue treating many of them for that kind of money. They also claim that half of their requests to perform the cleanings are rejected — an assertion not supported by state data.

The Department of Health Care Services, which runs Denti-Cal, said it made the changes to bring the program’s reimbursement policy in line with other states and to reduce “unnecessary dental treatment.”

But Aminloo insists the new state regulations victimize the most vulnerable people, who she said are losing their access to routine dental care.

“If these patients don’t get preventive oral care, their overall health is going to suffer,” she warned.

Dental hygienists are generally allowed to practice without the direct supervision of a dentist in 40 states, including Nevada, Texas, Colorado, Michigan and Florida. But the type of patients they can see varies by state. So do reimbursement and preauthorization rules.

Washington state’s Medicaid program pays providers $46 for a similar cleaning procedure, said Anita Rodriguez, a member of the Washington State Dental Hygienists’ Association. Hygienists there don’t have to obtain preauthorization to perform cleanings, but they are required to explain why the cleaning was necessary when they bill Medicaid.

“Our state makes access for our independent hygienists relatively uncomplicated though, like other Medicaid providers, we make pennies on the dollar for our care,” she said.

Since California reduced payments for “maintenance” cleanings for these patients — usually performed every three months to treat gum disease — many hygienists have stopped seeing them. Eight hygienists, including Aminloo, filed a lawsuit in Los Angeles County Superior Court in 2016, arguing that the health care services department cut the reimbursement rate without first obtaining necessary federal approval.

At one point, it appeared as if the department had agreed to settle and cancel its rate change but then backed out, court documents show. The department said it will not comment on pending litigation.

At the time of the rate reduction, the state also started requiring dental hygienists to obtain prior authorization to treat gum disease in patients who live in special care facilities. Hygienists must submit X-rays along with their authorization requests. But they say it’s almost impossible to take decent X-rays of elderly or disabled patients who have a hard time controlling their head movements, or who refuse to open their mouths widely.

When hygienists do manage to get X-rays, their requests are often denied anyway, hygienists from across the state told California Healthline.

In a letter to the state legislature last year, the California Dental Hygienists’ Association wrote that more than half of their authorization requests had been denied since the change. “Denti-Cal’s sweeping new rules are destroying the lives of fragile patients and the women who own small businesses providing care at the bedside,” the letter said.

But state statistics show a much lower denial rate.

From the time the change took effect in July 2016 through June 2017, the health care services department approved 10,000 of nearly 13,000 deep cleanings requested by these dental hygienists to treat gum infections, according to the data. It also approved 31,300 of the nearly 33,000 requests for routine cleanings that follow a deep cleaning. The state said it paid more than $2.5 million to dental hygienists for these procedures.

Darla Dale, a hygienist in Eureka and a vice president of the hygienists association, said the department’s denial numbers don’t reflect what her organization is seeing.

“There’s no way that’s true,” Dale said. “We’re in contact with these hygienists. … Many have stopped working because we can’t spend our lives trying to get authorization.”

Darci Trill, a hygienist working in Alameda and Contra Costa counties, is among those who stopped seeing patients in nursing homes after denial letters piled up. “I lost about 70 percent of my Denti-Cal clients,” she said.

State health officials pointed to the American Academy of Periodontology, which considers the new authorization guidelines standard, including X-rays to diagnose gum disease.

An April 2016 report by the Little Hoover Commission, an independent state watchdog agency, said the state health services department found it “unusual” that nearly 88,000 out of 100,000 Denti-Cal-eligible patients in nursing homes had received deep cleanings during the 2013-14 fiscal year. This figure and other factors raised “questions about their necessity ­— and hence the new policy requiring X-ray documentation,” the report said.

In frail patients, advanced gum disease can cause not only tooth loss, but pneumonia and other respiratory issues, Trill said.

Maureen Titus, a hygienist in the San Luis Obispo area, said her clients rely entirely on caregivers for their dental hygiene, and that brushing and flossing is neither easy nor effective. “Most have bleeding gums, inflamed gums and tartar buildup,” she said.

Among patients who are attached to feeding tubes, tartar builds up quickly because they don’t chew their food, Aminloo said. “After two or three months, you can’t even see their teeth.”

The independent practice of dental hygienists in California dates to 1997, when the state legislature allowed them, with additional training and certification, to work without the direct supervision of dentists. Some started their own mobile businesses. This is the first time in the intervening 20 years that they’ve had to obtain preauthorization to perform dental cleanings, Trill said.

The California Dental Association, which represents dentists, said dentists have long been required to get prior approval for cleanings for patients in special care facilities.

“We supported the department’s decision to equalize requirements for periodontal services, regardless of whether a dentist or hygienist provides the service,” said Alicia Malaby, the association’s spokeswoman.

Dr. Leon Assael, the director of community-based education and practice at the University of California-San Francisco’s School of Dentistry, said preauthorization requirements in other states, including Minnesota and Kentucky, where he used to work, have also delayed or limited care for homebound patients.

The requirements have driven providers out of the system, he said, leaving patients behind.

“If this were toes being lost, this would be a scandal,” Assael said, “but with teeth, it’s been accepted.”"
20,"One change to dental care that could save lives, money","Poor dental health affects tens of millions of Americans with 63 million live in places described as dental shortage areas; suffering from decaying teeth, toothaches and chronic dental pains. Not only does this lead to general suffering, from an economic stance work productivity is damaged across all age ranges. Moreover, high medical costs mean only 1 in 3 dentists actually accept Medicaid patients. However, states are now attempting to overcome the issues that stop both adults and children receiving proper dental care at little cost to the taxpayer. Political advocates Grover Norquist and Don Berwick see a simple solution, one that received over a 75 percent backing from Democrats and Republicans alike. They want to allow dentists to hire a variety of professionals from dental therapists to nurse practitioners who can supply preventive care and perform routine procedures that do not require dentists. This would utilize the free market at little cost, allowing dental therapists to operate without unnecessary government barriers while being able to treat more patients, offering the potential opportunity for SME’s expansion. Tragic cases like Deamonte Driver, who lost his life after receiving poor dental treatment would be prevented if dental therapists were able to operate. Subsequently, dental therapists are far cheaper than dentists, they provide high-quality care, are well educated and readily available, having existed in more than 50 countries for over a century. Should states turn to dental therapists, it would provide a simple solution to help patients, businesses and combat rising health care costs.
",http://www.chicagotribune.com/news/opinion/commentary/ct-perspec-dental-therapists-dentists-teeth-health-care-0103-20180102-story.html,2018-01-03 17:34:08.853000,"In 2007, the tragic case of 12-year-old Deamonte Driver gained national attention when the Maryland boy died from an untreated tooth infection because his family couldn’t find a dentist who would treat him. Instead of an $80 procedure that could have prevented Deamonte’s death, his saga turned into a series of hospital visits that came too late, ending with the needless loss of his life, but also costing taxpayers tens of thousands of dollars through Medicaid. And yet, little has changed across the U.S. since then when it comes to dental care access. There has been a serious market failure, harming lives and raising costs. Fortunately, a market solution now exists, if only states will adopt it: dental therapists."
21,States Look To Create Own Individual Mandates In Wake Of GOP Tax Bill,"Following on from the Republican tax bill, several Democratic-led states are looking to implement state-level individual mandates to overcome bare counties and the prospect of failing risk pools. California, Connecticut, New York, Maryland and Washington State, are all considering the move when their state legislatures come into session in early 2018. It is likely that the states will attempt to implement a model similar to that of RomneyCare, introduced in Massachusetts in 2006, and the ACA’s individual mandate. These states do not require federal approval for the move as the mandate penalty is a tax, and as a result, have the ability to implement their own version of the Obamacare mandate. It is unlikely that the move will extend beyond these states as there is far more likely to be partisan pushback as state legislatures often skew to the right. California, in particular, is looking at the possibility of a state individual mandate to overcome the uncertainty at federal level surrounding Obamacare. Maryland too would likely introduce an individual mandate, Massachusetts could fall back on its original proposition, with Washington State proving the most complicated.  
",https://insidehealthpolicy.com/daily-news/states-look-create-own-individual-mandates-wake-gop-tax-bill,2018-01-03 17:32:41.667000,"Updated Story Several Democratic-led states are looking to implement state-level individual mandates for insurance coverage in an effort to reduce the prominence of bare counties and failing risk pools due to the end of the Affordable Care Act’s individual mandate in 2019 and other instabilities surround the law. California, New York, Maryland, Connecticut, and Washington state are all considering pursuing state individual mandates for insurance coverage when their state legislatures come into session in early 2018 ,..."
22,Scripps Health reorganization to include layoffs,"The San Diego-based non-profit health care system Scripps Health is set to pursue layoffs in a restructuring process in 2018. Scripps missed its annual budget by $20 million and intends to target lower costs and a greater dependence on caring for patients outside of its hospitals. The company’s CEO, Chris Van Gorder, revealed in a memo that the cuts are necessary with insurers and patients increasingly targeting lower prices as health costs rise. The announcement by Scripps follows in a cost-cutting trend amongst large health care organizations; both Advocate Health of Illinois and Tenet Health announced similar measures following disappointing results. Although Scripps maintains a $2.6 billion balance in cash and investments, with an increased profitability of $350 million, its operating margin has slimmed from 9 percent in 2012 to 2.3 percent in 2017. The firm believes that it would not be responsible management to turn the savings towards a recurring problem especially as it requires favorable interest rates from lenders on its $2.6 billion building project. Alongside the personnel cuts intended to save $30 million in the current budget year, there will be a consolidation of management positions.
",http://www.sandiegouniontribune.com/news/health/sd-no-scripps-regroups-20171219-story.html,2018-01-03 17:32:04.877000,"Though it has billions in the bank, Scripps Health will pursue layoffs in 2018 as part of a reorganization strategy that emphasizes lower costs and greater reliance on caring for patients outside of its five hospitals.

In a recent memo to all of the health system’s 15,000 employees and 3,000 affiliated doctors, Chris Van Gorder, Scripps’ chief executive officer, says that cuts are necessary to remain competitive in a health care world where health insurance companies increasingly consider low prices as a main factor in contracting and patients are more often shopping around for services as deductibles increase.

Scripps missed its annual budget by $20 million last year for the first time in 15 years, Van Gorder said in an interview this week. It was a wake-up call, he said, that added urgency to the need to both cut costs and also re-think how the private not-for-profit health network does business.

“We’ve got to shift our organizational structures around to be able to deal with the new world of health care delivery, find ways of lowering our costs significantly,” Van Gorder said. “If we don’t, we will not be able to compete.”

Advertisement

Scripps is far from the only large health care system to announce cost-cutting measures in recent months.

Advocate Health, Illinois’ largest health care provider with 12 hospitals and 35,000 employees, announced in May that it would try to cut more than $200 million in costs due to flat revenue projections. Tenet Health, the nation’s third-largest medical chain with 77 hospitals, announced in October that it will eliminate a middle layer of management after posting a $56 million loss after a 1.4 percent single-quarter revenue decline, according to industry news source Modern Healthcare.

Things haven’t gotten quite so bad yet at Scripps.

A look at Scripps’ bond industry financial filings make it clear that this is not a company teetering on the edge of insolvency. Far from it. Scripps, the audited financial statements for the 2017 budget year show, has banked about $2.6 billion in unrestricted cash and other investments.

However, Scripps has recently seen its operating margin, the percentage of revenue left over after all the bills are paid, shrink significantly from about 9 percent in 2012 to 2.3 percent in 2017. Financial statements show that Scripps’ bottom line was significantly bolstered this year by investment income. A roaring stock market helped significantly increase the value of its holdings, pushing total profitability up to $350 million for the year, a number that is 25 percent greater than last year.

How can an organization be making money, have a significant financial cushion and yet still be planning for layoffs in the coming year?

Van Gorder said the basic fact is that operation of the organization must continue to bring in more revenue than it spends and declining operating margins must be addressed even if revenue from the stock market is currently masking those declines.

“As strong as we are on the bottom, bottom line, the trends at the top end are changing, and we have to adjust to them,” Van Gorder said.

Advertisement

Chris Van Gorder, president and chief executive of Scripps Health, speaks to the editorial board of The San Diego Union-Tribune in 2012. (John Gastaldo )

Still, with more than two billion in the bank, couldn’t Scripps afford to burn some savings and stave off layoffs?

Van Gorder said that it’s not responsible management to fix a recurring problem with savings. And, having a hefty balance sheet, he added, is necessary to get favorable interest rates from lenders as Scripps moves to execute a recently announced $2.6 billion building plan that will replace Scripps Mercy Hospital in Hillcrest, add a new patient tower at Scripps Memorial Hospital, La Jolla and upgrade facilities in Encinitas, Chula Vista and Torrey Pines.

These upgrades and replacements, the executive added, are made more urgent than they would otherwise be due to a state law that requires all hospitals to meet certain seismic requirements by 2030 or cease operation.

Advertisement

In addition to borrowing and revenue from philanthropy, Scripps plans to tap its savings to fund its building plan. That plan, Van Gorder said, will continue but will be undertaken with knowledge that insurance companies want to avoid paying the higher prices charged by hospitals whenever possible.

That means pulling back on the previous tendency to include plenty of space inside hospital complexes to patients whose medical needs don’t require them to be admitted for a overnight stay. Scripps has already started on this path with the purchase of Imaging Healthcare Specialists in 2015. The company operates stand-alone imaging centers that offer cheaper scans than are available in the outpatient centers attached to the region’s major hospitals.

“We’re seeing a huge shift in the ambulatory side,” Van Gorder said. “We’re now, for example, doing total joint replacement in ambulatory surgery centers. A year ago, that didn’t take place. The experts are telling me and others that you’re going to see a huge jump in that as technology continues to improve … that’s going to pull a whole lot of utilization away from hospitals and hospitals that haven’t prepared for that shift are going to be in deep trouble in the not-to-distant future. That’s why we’re trying to make this shift proactively.”

Scripps’ bond filings do show that it has seen a significant shift in its business over the last five years.

Advertisement

From 2011 to 2016, the most recent year for which aggregated patient data is available, Scripps reported that inpatient discharges, the total number of days that patients spent in its hospitals, and surgeries performed in hospitals all decreased slightly. Meanwhile, the number of surgeries performed at its outpatient surgery centers increased 69 percent. Visits to Scripps Clinic and Scripps Coastal Medical Group increased 14 percent and 19 percent respectively and emergency visits were up 21 percent.

Scripps’ first step in its reorganization plan to better address the shift from inpatient to outpatient is to collapse its ranks of hospital management. Instead of having a chief executive officer to manage operations at each of its five hospitals, the plan is to have two executives handle those duties, one for hospitals in La Jolla and Encinitas and another to oversee operations in San Diego and Chula Vista. A third executive will be in charge of all ancillary services, including operation of the medical offices, outpatient surgery centers and other facilities that Scripps owns or leases throughout the region.

This consolidation will not require any layoffs. Gary Fybel, currently the chief executive officer at Scripps Memorial Hospital La Jolla, and Robin Brown, chief executive officer at Scripps Green Hospital, will retire. The northern chief executive officer position has been awarded to Carl Etter, current chief executive officer of Scripps Memorial Hospital Encinitas. The south position will be handled by Tom Gammiere, who currently runs Scripps Mercy Hospital in Hillcrest. Lisa Thakur, currently corporate vice president of operating rooms, pharmacy and supply chain, will fill the ancillary services post, and her previous job will not be filled.

Van Gorder said further personnel cuts are coming and are intended to save $30 million during the current budget year ending Sept. 30, 2018. Cuts should save $40 million in subsequent years, he added. Most will be focused on administrative job classifications.

Advertisement

“There will be layoffs,” Van Gorder said. “I don’t like it, but it has to be done for me to protect the organization and our ability to take care of our community into the future.”

So far, he said, there are no specifics to share on which particular jobs are most at risk.

He added that, as the current reorganization effort takes shape, hiring is also anticipated. More workers will be needed to provide quicker service to patients.

“What I want to do at the point of service is support our nursing staff with more paid professional staff,” Van Gorder said.

Advertisement

Gerard Anderson, a professor of health policy and management at Johns Hopkins School of Medicine in Baltimore, said the current trend of cost-cutting at large health care systems does indeed appear to be driven by decreases in reimbursement by Medicare and private health insurance companies.

“In almost every single hospital they’re losing some amount of money in operating margin but making it up in spades when you add in their investment income,” Anderson said.

The researcher said he was a bit disturbed to see organizations flush with stock market earnings discuss cuts, especially if those cuts are to employees who directly care for patients.

“You’re seeing layoffs but I don’t necessarily understand why you need to lay someone off when your margin is something like 11 percent after non-operating income is added into the mix,” Anderson said.

Advertisement

But what about the need to shift business strategy and reduce administrative expenses as reimbursement falls? Anderson said he can see that point as long as it’s true.

“If they really are doing this in the non-patient-care area, I think that makes sense,” he said. “In general, we’ve seen more across-the-baord cuts than targeted specifically to areas of management and administration. The salaries in management have grown faster than they have for clinicians.”

Advertisement

Health Playlist On Now Video: Why aren't Americans getting flu shots? 0:37 On Now Video: Leaders urge public to help extinguish hepatitis outbreak On Now San Diego starts cleansing sidewalks, streets to combat hepatitis A On Now Video: Scripps to shutter its hospice service On Now Video: Scripps La Jolla hospitals nab top local spot in annual hospital rankings On Now Video: Does a parent's Alzheimer's doom their children? On Now Video: Vaccine can prevent human papillomavirus, which can cause cancer 0:31 On Now 23 local doctors have already faced state discipline in 2017 0:48 On Now EpiPen recall expands On Now Kids can add years to your life

paul.sisson@sduniontribune.com

(619) 293-1850

Advertisement

Twitter: @paulsisson"
23,Millennials aspire to own homes despite pessimism over chances,"A majority of millennials anticipate they will own a home in the future despite being pessimistic about their overall prospects. A recent survey commissioned by the Bank of England found that 52% of under-35s expect to buy a home at some point in the future, 24% expect never to buy and 25% are unsure. Purchase costs, such as deposits, estate agency fees and stamp duty, were cited as the biggest barriers to home ownership. An earlier Ipsos MORI poll found 71% of respondents thought millennials faced worse prospects than their parents of owning a home. 
",http://www.resolutionfoundation.org/media/blog/time-for-some-housing-honesty/,2018-01-03 17:04:12.677000,"The return to work after Christmas is never easy. Unless you’re an estate agent: they love January. Following the pre-Christmas lull, families rush back into wanting to buy and sell their houses (helped in part by the traditional post-festivity spike in family breakdown). But for an increasing number of us, house hunting is becoming little more than an exercise in window shopping (or ‘property porn’ if you’d rather).

The share of the population owning a home has been falling since 2003, with particularly profound consequences for younger families. As the chart below shows, today’s 30 year olds (that is, the oldest members of the millennial generation born between 1981 and 2000) are only half as likely to own their house as their parents were at the same age.

Like so much of the Christmas TV schedule, this is a story that’s been on repeat for some time. Britons get that their country is no longer a nation of home owners. As research carried out by Ipsos MORI back in the summer for the Intergenerational Commission showed, 71 per cent of people (across all generations) think millennials face worse prospects than their parents in this regard. Just 7 per cent think young adults are better off. Indeed, of all of the questions asked in the survey, it was the one on which respondents were most pessimistic.

Yet, despite being pessimistic about the overall picture for millennials, new data shows that a significant share of the generation think they personally will manage to beat the gloom. The next chart takes data from the Bank of England’s latest NMG survey to show that more than half (52 per cent) of non-owning households headed by someone aged under-35 (roughly speaking, the millennial generation) expect to buy at some point in the future. And that proportion holds up even among lower income millennials.

If such expectations were borne out, around 75 per cent of millennial households would eventually own a home. That would put the generation on a par with the home ownership rates recorded among baby boomers. It would also be roughly 10 percentage points higher than the ‘optimistic’ scenario we set out in September (our ‘pessimistic’ scenario put the figure under 50 per cent). Short of a significant turnaround in housing trends, the implication is that many members of the younger generation will find their aspirations go unmet.

And, while the one-in-four (24 per cent) non-owning millennials who think they’ll never buy a home might have a more realistic outlook of the future, they’re just as likely to be unhappy with their lot. The next chart sets out the factors which this group identify as being among the three most important reasons for not owning. What stands out is that just one-in-ten of them cite positive-sounding reasons: 10 per cent say they like their current home and just 8 per cent prefer the flexibility of renting. The upshot is that as few as 1 per cent of millennials appear to be happy with the idea of never owning a home.

It’s this finding that goes a long way to explaining why politicians are so keen to be seen to be offering hope on home ownership.

And, with ‘purchase costs’ (such as the deposit, stamp duty and estate agents fees) being cited by millennials as the main barrier to owning, it’s easy to understand the temptation to focus on subsidising buyers. Measures such as the removal of stamp duty for first time buyers of property worth up to £300,000 – which Philip Hammond announced in the Autumn Budget – give the impression of extending home ownership to a wider group. But they largely miss the mark. The OBR’s assessment of the stamp duty policy was that it would benefit just 3,500 first time buyers who would not otherwise have been able to buy a home, costing roughly £160,000 per additional owner.

Supply-based approaches represent a preferable and more sustainable option, but they take time to take effect. That’s not to say government should give up, and the Autumn Budget plans for returning housing capital spending back to the levels of the 2000s (outside of the fiscal stimulus peak of 2008-10) is a very welcome one.

But it’s hard to escape the conclusion that, even if we get to grips with the longer-term problem, home ownership will remain off-limits for significant numbers of millennials. Some might expect to benefit from the bank of mum and dad in the near-term and from inheritances as they age. But such support may come too late to cover expensive family-rearing years for many households, and will never arrive for many – mainly lower income – others.

That reality raises a number of challenges for today’s young people. Over the longer-term, home ownership plays an important role in building wealth (via semi-enforced saving), providing leverage and hedging against costs and location in retirement. In its absence, alternatives are needed.

More immediately, the generally higher housing costs associated with renting leave young people with less disposable income and less opportunity to save than earlier cohorts faced. The chart below sets out the share of income allocated to rent among younger respondents to the NMG survey. It shows that 30 per cent of renters in this group spend more than one-third of their pre-tax income – a threshold that is often taken as a sign of housing unaffordability. And that figure jumps to a massive 71 per cent among the poorest fifth of millennials.

We’ll turn to the question of how the country might rise to these challenges in a forthcoming policy options paper for the Intergenerational Commission. But our politicians – unlike our estate agents – need to be more honest about the housing aspiration gap. It’s good to offer hope, but a healthy dose of realism would sharpen the focus on the broader living standards challenge posed by our housing crisis."
24,Bitcoin technology set for big changes this year,"The technology underpinning bitcoin is set for major changes in 2018, with several projects scheduled. Among them is the wider adoption of the segwit upgrade, originally activated in August, thanks to the new bech32 address format, scheduled for a March release. Also expected is the launch of bitcoin's Lightning Network, offering secure, instant confirmations and near-free transfers, while privacy solutions ZeroLink and TumbleBit are set to make the network more secure. Activation of segwit will also make it easier to improve cryptographic signatures by facilitating the use of Schnorr signatures, which would reduce transaction costs and increase bitcoin's maximum capacity.
",https://bitcoinmagazine.com/articles/keep-eye-out-these-bitcoins-tech-trends-2018/,2018-01-03 16:21:22.037000,"From a tech perspective, Bitcoin seems to be just getting started: 2018 promises to be the year that a number of highly anticipated projects are either launched or adopted.

In many ways, 2017 was Bitcoin’s best year yet. Most obviously, increased adoption made the pioneering cryptocurrency’s exchange rate skyrocket from under $1000 to well over 10 times that value.

But from a tech perspective, things seem to be just getting started: 2018 promises to be the year that a number of highly anticipated projects are either launched or adopted.

Here’s a brief overview of some of the most promising upcoming technological developments to keep an eye on in the new year.

Cheaper Transactions with Segregated Witness and a New Address Format

Segregated Witness (SegWit) was one of Bitcoin’s biggest — if not the biggest — protocol upgrade to date. Activated in August 2017, it fixed the long-standing malleability bug, in turn better enabling second-layer protocols. Additionally, SegWit replaced Bitcoin’s block size limit with a block weight limit, allowing for increased transactions throughout the network, thereby lowering fees per transaction.

However, adoption of the upgrade has been off to a relatively slow start. While some wallets and services are utilizing the added block space offered by SegWit, many others are not yet doing so. This means that, while Bitcoin is technically capable of supporting between two and four megabytes worth of transactions per ten minutes, it barely exceeds 1.1 megabytes.

This is set to change in 2018.

For one, the Bitcoin Core wallet interface will allow users to accept and send SegWit transactions. Bitcoin Core 0.16, scheduled for May 2018 (though this may be moved forward), will most likely realize this through a new address format known as “bech32,” which also has some technical advantages that limit risks and mistakes (for example, those caused by typos).

“To spend coins from the P2SH format currently used for SegWit, users need to reveal a redeem script in the transaction,” Bitcoin Core and Blockstream developer Dr. Pieter Wuille, who also co-designed the bech32 address format, told Bitcoin Magazine.

“With native SegWit outputs this is no longer necessary, which means transactions take up less data. Recipients of SegWit transactions will be able to spend these coins at a lower cost.”

Perhaps even more importantly, several major Bitcoin services — like Coinbase — plan to upgrade to SegWit in 2018 as well. Since such services account for a large chunk of all transactions on the Bitcoin network, this could significantly decrease network congestion, thereby decreasing average transaction fees and confirmation times, even for those who do not use these services.

The Lightning Network Rolling Out on Bitcoin’s Mainnet

While further SegWit adoption should provide immediate relief of fee pressure and confirmation times, truly meaningful long-term scalability will likely be achieved with second-layer solutions built on top of Bitcoin’s blockchain.

One of the most highly anticipated solutions in this regard — especially for lower value transactions — is the lightning network. This overlay network, first proposed by Joseph Poon and Tadge Dryja in 2015, promises to enable near-free transactions and instant confirmations, all while leveraging Bitcoin’s security.

The solution has been under active development for about two years now, with major efforts by ACINQ, Blockstream and Lightning Labs. Progress on the scaling layer has been significant all throughout 2017, with early software releases of different but compatible software implementations, useable wallets interfaces and test transactions happening both on Bitcoin’s testnet and even on Bitcoin’s mainnet on a regular basis now.

“I'd say we have solved the main technical problems and have a relatively good idea on how to improve on the current system,” Christian Decker, lightning developer at Blockstream, told Bitcoin Magazine. “One last hurdle that's worth mentioning is the network topology: We'd like to steer the network formation to be as decentralized as possible.”

Given the current state of development, adoption of the lightning network should only increase throughout 2018 — not just among developers, but increasingly among end users as well.

“Integration and testing will be the next major step forward,” Lightning Labs CEO Elizabeth Stark agreed, noting: “Some exchanges and wallets are already working on it.”

Increased Privacy Through TumbleBit and ZeroLink

While it is sometimes misrepresented as such, Bitcoin is not really private right now. All transactions are included in the public blockchain for anyone to see, and transaction data analysis can reveal a lot about who owns what, who transacts with whom and more. While there are solutions available to increase privacy right now — like straightforward bitcoin mixers — these usually have significant drawbacks: They often require trusted parties or have privacy leaks.

This situation could be improved significantly in 2018. Two of the most promising projects in this domain — TumbleBit and ZeroLink — are both getting close to mainnet deployment.

TumbleBit was first proposed in 2016 by a group of researchers led by Ethan Heilman. It is essentially a coin-mixing protocol that uses a tumbler to create payment channels from all participants to all participants in a single mixing session. Everyone effectively receives different bitcoins than what they started with, breaking the trail of ownership for all. And importantly, TumbleBit utilizes clever cryptographic tricks to ensure that the tumbler can’t establish a link between users either.

An initial implementation of the TumbleBit protocol was coded by NBitcoin developer Nicolas Dorier in early 2017. His work was picked up by Ádám Ficsór as well as other developers, and blockchain platform Stratis announced it would implement the technology in its upcoming Breeze wallet, which also supports Bitcoin, by March 2018. Recently, in mid- December of 2017, Stratis released TumbleBit integration in this wallet in beta.

The other promising solution, ZeroLink, is an older concept: it was first proposed (not under the same name) by Bitcoin Core contributor and Blockstream CTO Gregory Maxwell, back in 2013. Not unlike TumbleBit, ZeroLink utilizes a central server to connect all users but without being able to link their transactions. As opposed to TumbleBit, however, it creates a single (CoinJoin) transaction between all participants, which makes the solution significantly cheaper.

This idea seemed to have been forgotten for some years until Ficsór (indeed, the same Ficsór that worked on TumbleBit) rediscovered it earlier this year. He switched his efforts from TumbleBit to a new ZeroLink project and has since finished an initial ZeroLink implementation.

Ficsór recently ran some tests with his ZeroLink implementation, and while results showed that his implementation needs improvement, Ficsór considers it likely that it will be properly usable within months.

“I could throw it out in the open right now and let people mix,” he told Bitcoin Magazine. ""There is no risk of money loss at any point during the mix, and many mixing rounds were executing correctly. It is just some users would encounter some bugs I am not comfortable with fixing on the fly.”

More Sidechains, More Adoption

Sidechains are alternative blockchains but with coins pegged one-to-one to specific bitcoins. This allows users to effectively “move” bitcoins to chains that operate under entirely different rules and means that Bitcoin and all its sidechains only use the “original” 21 million coins embedded in the Bitcoin protocol. A sidechain could then, for example, allow for faster confirmations, or more privacy, or extended smart contract capabilities, or just about anything else that altcoins are used for today.

The concept was first proposed by Blockstream CEO Dr. Adam Back and others back in 2014; it formed the basis around which Blockstream was first founded. Blockstream itself also launched the Liquid sidechain, which allows for instant transactions between — in particular — Bitcoin exchanges. Liquid is currently still in beta but could see its 1.0 release in 2018.

Another highly anticipated sidechain that has been in development for some time is RSK. RSK is set to enable support of Turing-complete smart contracts, hence bringing the flexibility of Ethereum to Bitcoin. RSK is currently in closed beta, with RSK Labs cofounder Sergio Demian Lerner suggesting a public release could follow soon.

Further, Bloq scientist Paul Sztorc recently finished a rough implementation of his drivechain project. Where both Liquid and RSK for now apply a “federated” model, where the sidechain is secured by a group of semi-trusted “gatekeepers,” drivechains would be secured by bitcoin miners.

If drivechains are deployed in 2018, the first iteration of such a sidechain could well be “Bitcoin Extended:” essentially a “big block"" version of Bitcoin to allow for more transaction throughput. That said, reception of the proposal on the Bitcoin development mailing list and within Bitcoin’s development community has been mixed so far. Since drivechains do need a soft-fork protocol upgrade, the contention does make the future of drivechains a bit more uncertain.

“Miners could activate drivechains tomorrow, but they often outsource their understanding of ‘what software is good’,” Sztorc told Bitcoin Magazine. “So they'll either have to decide for themselves that it is good, or it would have to make it into a Bitcoin release.”

A Schnorr Signatures Proposal

Schnorr signatures, named after its inventor Claus-Peter Schnorr, are considered by many cryptographers to be the best type cryptographic signatures in the field. They offer a strong level of correctness, do not suffer from malleability, are relatively fast to verify and enable useful features, thanks to their mathematical properties. Now, with the activation of Segregated Witness, it could be relatively easy to implement Schnorr signatures on the Bitcoin protocol.

Perhaps the biggest advantage of the Schnorr signature algorithm is that multiple signatures can be aggregated into a single signature. In the context of Bitcoin, this means that one signature can prove ownership of multiple Bitcoin addresses (really, “inputs”). Since many transactions send coins from multiple inputs, having to include only one signature per transaction should significantly benefit Bitcoin’s scalability. Analysis based on historical transactions suggest it would save an average of 25 percent per transaction, which would increase Bitcoin’s maximum transaction capacity by about 33 percent.

Further on, Schnorr signatures could enable even more. For example, with Schnorr, it should also be possible to aggregate different signatures from a multi-signature transaction, which require multiple signatures to spend the same input. This could, in turn, make CoinJoin a cheaper alternative to regular transactions for participants, thereby incentivizing a more private-use Bitcoin. Eventually the mathematical properties of Schnorr signatures could even enable more advanced applications, such as smart contracts utilizing “Scriptless Scripts.”

Speaking to Bitcoin Magazine, Wuille confirmed that there will probably be a concrete Bitcoin Improvement Proposal for Schnorr signatures in 2018.

“We might, as a first step, propose an upgrade to support Schnorr signatures without aggregation,” he said. “This would be a bit more straightforward to implement and already offers benefits. Then a proposal to add aggregation would follow later.”

Whether Schnorr signatures will already be adopted and used on Bitcoin’s mainnet is harder to predict. It will require a soft fork protocol upgrade, and much depends on the peer review and testing process."
25,Louisiana named best place to mine bitcoin in the US,"A new study has named Louisiana as the cheapest state in the US in which to mine bitcoin. Electrical supply company Crescent Electric based its calculation on the cost of electricity in each state, the power requirements of the equipment needed, and the average length of time taken to mine a token. This produced a figure of $3,224 per bitcoin for Louisiana, with the most expensive places being Hawaii, at $9,483, and Alaska at $7,059. All of these figures are notably less than the current trading price of bitcoin.
",https://cointelegraph.com/news/mining-bitcoin-in-the-us-best-do-it-in-louisiana-study-says,2018-01-03 16:20:27.840000,"Electrical supply company Crescent Electric (CESCO) study reveals that the state of Louisiana is the cheapest state in the US to mine Bitcoin.

Digital currency mining requires a lot of electric power and the power rates differ in every state.

Based on CESCO’s latest study of the cost of cryptocurrency mining across the US, it is currently cheapest to mine Bitcoin in Louisiana -- electricity costs at 9.87 cents per watt puts the average cost of mining one Bitcoin at $3,224.

This is significantly cheaper than the current price of Bitcoin, which is currently trading at around $12,000 per coin, as of press time.

Where else in the US is it cheap to mine?

In their study, CESCO also estimated the cost of Bitcoin mining based on the wattage consumption of the three most popular mining rigs, namely, the AntMiner S9, the AntMiner S7, and the Avalon 6, as well as the average days each rig takes to mine a token. These figures were then multiplied by the average electricity rate in each state.

Aside from Louisiana, the other top five states with the lowest cost to mine Bitcoin are Idaho ($3,289 per token), Washington ($3,309), Tennessee ($3,443) and Arkansas ($3,505).

The study also names the most expensive states for digital currency mining. The list of costliest states is led by Hawaii, which takes an average mining cost of $9,483 per coin.

Rounding up the top five states with the highest Bitcoin mining rates are Alaska ($7,059), Connecticut ($6,951), Massachusetts ($6,674) and New Hampshire ($6,425).

The growing interest in cryptocurrency has been accompanied by growing concern over the energy required to mine crypto, namely Bitcoin. Such claims have been recently countered, as a report came out claiming that put cryptocurrency mining in the larger context of energy consumption."
26,Revenues from robo-advice set to reach $25bn by 2022,"Revenues generated by robo-advice platforms could hit $25bn by 2022, according to a study from Juniper research. The figures would represent more than a ten-fold increase on 2017 robo-advice revenues, which accounted for roughly $1.7bn. The study also estimates assets under management on robo-advice platforms could rise to $4.1tn by 2022, up from $330bn in 2017. 
",https://www.verdict.co.uk/private-banker-international/news/robo-advice-revenues-hit-25bn-2025/,2018-01-03 16:13:21.140000,"Revenues earned by the robo-advice industry could shoot up to $25bn by 2022, more than ten times what revenues were worth in 2017, new research shows.

The findings were revealed in a study by Juniper Research.

The research predicts revenues generated by the robo-advice industry to reach as high as $25bn by 2022, up from an estimated $1.7bn in 2017 thanks to automated wealth management services.

Robo-advisers will make investments that appeal to a wider segment of high net worth individuals (HNWIs) and to lower income individuals for as little as 0.6% of assets under management (AUM), Juniper said. This is due to new disruptive fintechs such as Moneybox and Nutmeg.

Juniper also said robo-advisers are making the investment process far more convenient by changing their delivery methods. They are specifically targeting smartphone apps, offering millennials more compelling reasons to invest.

According to Juniper this would drive AUM held by robo-advisers to about $4.1trn by 2022, up from $330bn in 2017.

Nick Maynard, who authored the research, said: “The technologies powering robo-advisers will mature to such an extent that they move from their current human supervised role to being utilised in a fully automated way. This will be aided by track records of performance automated robo-adviser systems are establishing.”

The implementation of robo-advice is not restricted to new participants, with even traditional players inching towards the service.

BlackRock, currently the world’s largest asset manager, and Aberdeen Asset Management have partnered with robo-advisory startups.

Juniper said: “The appeal of these [robo-advice] technologies is clear to established players, as automated systems even in a limited role will enable significant cost reductions and therefore increase overall quality of service and profitability [for traditional players].”"
27,Transparency and data are key to commercial property: tech leader,"The commercial property market will become more reliant on data and more transparent, according to the global head of product at Swedish company Datscha, Magnus Svantegård, who was one of the founding employees of the commercial property company over 20 years ago. He said the current handling of properties as investments was based too much on gut feeling and small networks of people. Datscha has 45 employees in Sweden, Finland and the UK, but retains a ""start-up mentality"", with a focus on property technology, according to Svantegård.
",http://www.metaprop.org/innovation-conversations-all/2017/12/27/magnus-svantegrd-global-gpo-at-stronghold?platform=hootsuite,2018-01-03 16:05:40.137000,"First of all, my role is the Global Head of Product at Datscha (I was one of the founding employees.) working with everything from strategy and business development down to what bugs [we should focus on] in the upcoming sprint. Datscha has been around for 20-plus years, but we still keep a very outspoken startup mentality and an effective organization, enabling us to be in three markets: Sweden, Finland and the UK with only 45 employees. In addition to my role at Datscha, I’m also a Partner in Stronghold Invest (the sole owner of Datscha), where we own, among others, the largest property consulting firm in the Nordics (Newsec with 2000 employees and 31 million sqm under management) and the most successful private equity real estate firm in Northern Europe with real estate assets under management of approximately €3.5 billion. Furthermore, Stronghold is an active #PropTech investor."
28,"Agents offered AI-backed app to identify, predict property trends ","An app that uses artificial intelligence (AI) to identify property-market trends and predict future changes is being offered to agents by Houseprice.ai. The Horizon app uses pricing data from the past 20 years to teach itself local property trends and quickly provide valuation reports and forecasts. Houseprice.ai CEO Eldred Buck said the service's primary benefits for agents was that ""it has no subjective biases and takes in over 50 individual factors to arrive at the capital valuation and rental values"". The app is available to agents for a £100 ($136) monthly subscription.",http://www.propertyindustryeye.com/the-robot-valuer-agents-offered-articifical-intelligence-app-offering-instant-property-valuations/,2018-01-03 15:56:31.957000,"Email to a friend Post navigation

Agents are being offered a new property valuation tool that claims to be different to others by using artificial intelligence to pick up trends and make predictions for the future.

Houseprice.ai has been testing an app called Horizon since August for agents, property developers and mortgage lenders that launched just before Christmas.

The software is programmed to use pricing data going back 20 years and to learn local trends using official house price, bank lending and rental figures as well as other factors such as crime, energy performance and planning data to provide valuation reports and forecasts within minutes.

In comparison, the Land Registry works out average values based on sale prices, while Zoopla’s online valuation tool uses factors such as property size and what neighbouring local properties have sold for. Rightmove provides market trend reports that show how much neighbouring property has sold for.

For subscriptions starting at an average of £100 per month depending on usage, subscribers can either work with the Horizon app and use the reports it generates or they can incorporate all the data into their own valuation and customised reports.

EYE queried whether these were all things an agent could already provide for free, but Houseprice.ai chief executive Eldred Buck said: “The main difference is that it is faster, has no subjective biases and takes in over 50 individual factors to arrive at the capital valuation and rental values.”

Two of the directors of the company, Buck and co-founder Giovanni Miano, come from a banking and financial technology background, while chief creative officer Vivienne Brooks has had a career in graphic design and marketing. The company has also appointed general practice chartered surveyor Philip Challinor as non-executive chairman."
29,Zurich acquires Hong Kong-based auto tech firm,"Zurich Insurance Group has acquired Bright Box, a Hong Kong-based car technology provider. The move is billed as part of a push from the insurer to improve its connected car technology solutions. Bright Box operates a connected platform which links drivers to their vehicles while also connecting vehicles with dealerships and original equipment manufacturers. Terms of the deal were not disclosed. Bright Box will continue to operate as a separate entity following the acquisition. 
",https://smartcitiesworld.net/news/zurich-insurance-snaps-up-car-tech-vendor-2436,2018-01-03 15:54:49.617000,
30,Fluenta's new gauges measure gas at extreme temperatures,"Fluenta has launched two ultrasonic flow transducers that enable measurement of gas flow in extremely high and low temperatures. The high-temperature transducer measures gas up to 250C, while the cryogenic transducer functions down to minus 200C, targeting the liquefied natural gas (LNG) sector. The sensors can work in environments of up to 100% methane or 100% carbon dioxide, gas mixes that historically have presented challenges to standard ultrasonic flow meters.",https://www.oilfieldtechnology.com/product-news/28122017/fluenta-launches-new-ultrasonic-flow-transducers/,2018-01-03 15:20:05.370000,"Fluenta has announced the launch of its new range of ultrasonic flow transducers. The two new transducers enable ultrasonic measurement of gas flow in highly challenging environments. Accurate measurement of gas flow at all temperatures.

A new high-temperature transducer to measure gas flow processes at up to +250°C, and a cryogenic transducer functional down to -200°C with significant applications in the liquified natural gas (LNG) sector.

The ability to measure gas flow in processes from 6 to 72 in. diameter and up to 100% methane or 100% carbon dioxide, depending on pipe diameter.

The high-temperature range transducer can accurately measure gas flow at up to +250°C, allowing Fluenta’s ultrasonic technology to be deployed in a wider range of flare applications, as well as in the chemical processing industry. The cryogenic transducer is designed to work in processes as cold as -200°C, typically found in the liquefied natural gas (LNG) industry and other gas liquification and chemical processes.

New software and signal processing allows these transducers to function in processes containing up to 100% methane or 100% carbon dioxide, gas mixes which historically have presented challenges to standard ultrasonic flow meters.

Fluenta’s non-intrusive transducers do not interrupt gas flow and can be used across a wide range of pipe diameters from 6 in. to 72 in. The new range of transducers and software are compatible with Fluenta’s FGM160, and can be fitted to existing installations.

With government regulation increasingly strict for monitoring flare gas emissions, companies are under pressure to accurately measure and record gas flow.

“These new transducers greatly increase the capacity of Fluenta to meet the needs of our existing customers, and to move into new markets such as chemical processing and liquified natural gas.”, comments Sigurd Aase, CEO of Fluenta.

“We are investing significantly in ultrasonic gas flow monitoring, and this product launch provides customers with an unrivalled combination flexibility, reliability and accuracy in gas flow monitoring.”"
31,Tesla Model 3 sets electric vehicle Cannonball Run record,"One of the first Tesla Model 3 customer cars has set a new record for the Cannonball Run in an electric vehicle. The car made the trip from Redondo Beach, California to New York City, a total journey mileage of 2,860 miles, in 50 hours and 16 minutes. The total charging cost for the duration of the Cannonball Run came in at $100.95.
",http://www.thedrive.com/new-cars/17312/tesla-model-3-sets-new-ev-cannonball-run-record-of-50-hours-16-minutes,2018-01-03 15:14:30.627000,"Approximately one hundred years ago, Erwin ""Cannonball"" Baker began driving cross-country, as quickly as possible, in anything he could get his hands on. His point: to demonstrate the reliability, range, and ease of refueling internal combustion cars.

On Thursday, December 28th, 2017, Alex Roy joined Daniel Zorrilla, a Tesla Model 3 owner, to test the range and reliability of that vehicle—which happens to be one of the first delivered Model 3 customer cars. The pair departed the Portofino Inn in Redondo Beach, California; their final destination was the Red Ball garage in New York City. The two completed the cross-country drive in 50 hours and 16 minutes, setting a new electric Cannonball Run record.

Total time: 50 hours, 16 minutes, 32 seconds

Total mileage: 2860 miles

Charging cost: $100.95"
32,McDonalds UK introduces highest pay rise in decade after strikes,"McDonald's UK will introduce its largest pay rise in a decade following the first-ever strikes in the country’s branches in September last year. The increases, which will be implemented from 22 January, mean that 16-17 year-olds will start working for the fast food company on a minimum wage of £5.75 ($7.77), up from £5.10. Those aged over 25 will receive an initial wage of £8 per hour, up from £7.60. The strikes, which took place in two branches in Cambridge and London, were called in protest at poor working conditions, low pay and the use of zero-hour contracts.
",http://www.mirror.co.uk/news/uk-news/mcdonalds-staff-cry-victory-finally-11790744,2018-01-03 15:11:06.063000,"Get email updates with the day's biggest stories Invalid Email Something went wrong, please try again later. Sign up Thank you for subscribing We have more newsletters Show me See our privacy notice

McDonald's UK has pledged to give its employees their biggest pay rise in ten years, Mirror Online can reveal.

The move comes into force on January 22 this year and is banded by position, region, and age. Only company-owned McDonald's restaurants (about a quarter of branches in Britain) are affected.

A staff member at a McDonald's branch in London shared with Mirror Online a company notice put up by management on Tuesday night.

Pay will increase up in all company-owned McDonald's restaurants

(Image: Daily Mirror)

The employee, who falls into the 21-24 category and has asked to remain anonymous, said in a private Facebook post: ""WE WON THIS. Biggest pay rise for 10 years! If 0.001% going on strike can win this imagine what more can do!""

They told Mirror Online: ""[We've been told] pay will be raised, with some crew over 25 even getting £10 an hour!

""Everyone's pay has gone up. It's not loads, but it's a win! My pay was around £7.45 and now it will be £7.95. It's the biggest raise in ten years.""

McDonald's recommends starting rates to managers. For perspective, under 18s currently get around £5.10 per hour, while those over 25 usually start on £7.60.

McDonald's has confirmed to us that the wages on the pamphlet are correct.

Now, 16-17 year-olds will join on a minimum wage of £5.75, while crew over the age of 25 will initially receive £8 per hour.

The decision comes after last year's strikes – a British first for McDonald's – that saw staff from two branches stage a 24-hour protest.

McDonald's workers took their protest to Parliament

(Image: AFP)

Video Loading Video Unavailable Click to play Tap to play The video will auto-play soon 8 Cancel Play now

Workers at a branch in Cambridge, and another in Crayford, south east London, made history on Monday September 4 after repeated claims of poor working conditions, zero-hour contracts, and low pay. Some staff talked about ""extreme stress"" and even ""bullying"".

Cambridge restaurant crew member Steve Day, who took part in the strikes, told Mirror Online: ""Obviously we welcome this. It's brilliant and a step in the right direction. And it's good McDonald's are finally listening to us.

""But there's much more to be done. It's still not really enough money to live on. Wages have been stagnant for so long, and this is McDonald's just buckling under a bit of pressure.

""When the CEO gets £8,000 per hour [according to Steve] we think we should maybe get a little more. The burgers and fries don't cook themselves – we keep him in a job.""

The 25-year-old, whose wage will rise from £7.65 to £8 per hour, suggested more could be achieved were a greater proportion of the workforce organised.

""It shows what an impact a small number of us can have. A tiny number did this – tiny, but not insignificant. I think we can do more.""

Steve, who's originally from Yorkshire and has worked for the company for nearly six months, also told us that he works around 35-40 hours a week on a zero-hour contract, and would like to be given better job security.

More can be done

While today is a small victory, the 30 strikers initially wanted to see their [crew member] wages rise to £10 per hour from around £7.50.

McDonald's management had earlier in the year promised to give permanent positions to workers on zero-hour contracts. It's not clear whether this has been implemented.

The fast food workers who took action were at the time represented by The Baker’s, Food and Allied Workers Union (BFAWU). A representative for the union called the strike a ""historic step"", that it would give employees its full support, and had previously seen a ballot of 95.7 per cent in favour of striking.

At the time, Lewis Baker, who then worked at the Crayford McDonald's, one of the restaurants at which workers took action, wrote a blog post explaining the strike.

He said: ""We have been left with no choice but to strike. It’s our only real option. We need to raise awareness over our working conditions and the way we are treated in McDonald’s.

""I – like many others – have had [my] grievances ignored by the company, time and time again.""

Labour leader Jeremy Corbyn said: ""Congratulations to McDonald's workers and @BFAWU1 for winning pay rises but the fight for £10 an hour is not over.

""We achieve more together than we can alone, which is why we should all join a trade union.""

McDonald’s employs around 85,000 people in the UK.

A spokesman told Mirror Online: ""Reward and recognition for our people and their contribution is a key priority, and to ensure we can attract and retain the best people, we regularly review pay and benefits.

""While our franchisees set their own pay rates, we have recommended an increase across all age bands for our hourly employees to be implemented from 22 January."""
33,Crystal Group clothing makers favour human labour over robots,"The CEO of Hong Kong-based clothing manufacturer Crystal Group has said cheap human labour is preferable to using robots. Andrew Lo told the Financial Times the company plans to increase its workforce in Bangladesh and Vietnam by 10% in the coming years. The two countries, along with Cambodia and Sri Lanka, account for two-thirds of the output from Crystal, which makes clothes for brands including H&M and Gap. Low wages mean there is currently no cost advantage to increased automation, according to Lo, particularly given the technical challenges of using robots to work on soft materials.
",https://qz.com/1169397/crystal-group-is-investing-in-low-wage-labor-not-robots-after-its-ipo/,2018-01-03 15:09:28.700000,"Hong Kong’s Crystal Group makes clothes for many of the world’s clothing giants, including H&M, Gap, Fast Retailing (owner of Uniqlo), and L Brands (owner of Victoria’s Secret). It’s the world’s largest apparel maker by production volume, according to research firm Euromonitor, and attracted attention in October for having the biggest IPO on the Hong Kong stock exchange since 2015.

It’s the sort of company you might expect to be pouring R&D money into automation, as labor costs rise in China and the world prepares for a future of robots taking over more repetitive, manual tasks, such as stitching clothes. But that’s not the case, says Andrew Lo, CEO of the Crystal Group.

In an interview with the Financial Times (paywall), Lo says high-tech sewing robots are “interesting” and could change how some companies make clothes, but in the near-term they still can’t beat cheap human labor on cost. Crystal Group plans to increase its human staff in Bangladesh and Vietnam—garment hubs with some of the lowest wages in Asia—by 10% annually in the years ahead. Currently about two-thirds of its sales are made from clothes produced in Bangladesh, Vietnam, Cambodia, and Sri Lanka, which have all become more attractive to garment manufacturers as producing in China, still the global leader in clothing production, gets more expensive.

Experts are cautiously watching how automation might affect the garment industry, which is a lifeline to millions of less-skilled workers in Asia and elsewhere, even as many of the jobs can be exploitative and dangerous. The International Labor Organization warned in 2016 that robots could replace the majority of textile, clothing, and footwear workers in Indonesia, Vietnam, and Cambodia in the coming decades. These workers could move into better jobs, but only if governments and employers start training them for those more skilled roles sooner than later.

For now, at least, Crystal Group will not be replacing humans with machines. One reason is that while robots in use in other industries can work easily with stiff materials, such as sheets of metal or plastic, they can’t yet work with soft, flexible fabrics that stretch and distort during sewing. “The handling of soft materials is really hard for robots,” Lo said.

A few companies believe they’ve solved the issue. Sewbo treats fabrics so that they become stiff and rigid for stitching, but return to normal when rinsed in hot water. SoftWear Automation, meanwhile, created a robotic table that uses machine vision to adjust on the spot to stretching and distortion in the fabric as it sews. SoftWear says one of these sewbots can make as many t-shirts per hour as about 17 humans working in a conventional set-up.

SoftWear Automation SoftWear Automation’s ultra-fast sewbot.

But Palaniswamy Rajan, SoftWear’s chief executive, admitted to the Financial Times that, when it comes to price, his sewbots can’t beat workers in Bangladesh. The original aim of SoftWear’s technology was to make clothes more cheaply in the US, where human wages are much higher. Rajan says robots could still be the best option for producing locally in the US when you factor in shipping costs, import duties, and the advantage of short lead times.

For now, though, most brands will likely keep their production in Asia, where it will be done by human hands."
34,Land sales in China's first-tier cities surge by 46% year on year,"Land sales in China's first-tier cities such as Beijing, Shanghai and Guangzhou surged by 46% year on year to 30 million sq metres in 2017, according to the China Index Academy. The academy also revealed that land sales in 300 Chinese cities rose by 8% to 950 million sq metres and sales of land for residential purposes rose 24% to 354 million sq metres. The government has been increasing land supply in order to put a brake on rising house prices. New purchase restrictions and requirements for higher deposits have also had an effect in slowing price rises.",http://www.xinhuanet.com/english/2018-01/03/c_136869279.htm,2018-01-03 15:01:07.050000,"Source: Xinhua| 2018-01-03 15:31:55|Editor: Xiang Bo

Video Player Close

BEIJING, Jan. 3 (Xinhua) -- Land sales increased in Chinese cities last year as the government moved to cool the market with higher supply, according to the China Index Academy, a property research organization.

Land sales in 300 Chinese cities totaled 950.36 million square meters in 2017, up 8 percent from 2016, while sales of land for residential projects reached 354.33 million square meters, an increase of 24 percent year on year.

Land sales in major cities like Beijing, Shanghai and Guangzhou were particularly robust, as local governments increased land supply to cool down runaway house prices fueled by huge demand and limited supply.

In China's first-tier cities, land sales jumped 46 percent year on year to 29.79 million square meters last year, according to China Index Academy.

Boosted by surging sales, revenue from land transactions rose 36 percent to 4.01 trillion yuan (about 620 billion U.S. dollars) in a total of 300 Chinese cities.

China's property market, once deemed a major risk for the broader economy, cooled in 2017 amid tough curbs such as purchase restrictions and increased downpayment requirements as the government sought to rein in speculation.

Due to these efforts, both investment and sales in China's property sector slowed. Real estate investment rose 7.5 percent year on year during January-November, down from 7.8 percent in the first 10 months.

Property sales in terms of floor area climbed 7.9 percent in the first 11 months, retreating from 8.2 percent in January-October.

With the market holding steady, Chinese authorities are aiming for a ""long-term mechanism"" for real estate regulation, and a housing system that ensures supply through multiple sources and encourages both housing purchases and rentals.

A report from the National Academy of Economic Strategy predicted that the country's property market would remain stable in 2018 if there were no major policy shocks."
35,China turns to big data analysis for social insurance coverage,"China's Ministry of Human Resources and Social Security will use big data analytics to ensure 100 million people access the country's social endowment insurance system, and aims to achieve universal coverage by 2020. The ministry's Social Insurance Management Centre plans to reach out to 10% of the population who are not part of the system, including immigrant workers or those in new forms of industry such as e-commerce, using internet platforms and mobile terminals. In 2014, the government launched a four-year campaign to register all eligible people for social insurance into a national database.
",http://www.chinadaily.com.cn/cndy/2018-01/03/content_35428542.htm,2018-01-03 14:56:14.807000,"China aims to build a database that includes all the eligible people for social insurance to implement targeted beneficiary measures, according to the Ministry of Human Resources and Social Security.

Officials will use the latest technology, including big data, to reach those without social insurance and to ensure universal coverage by 2020, according to the ministry's Social Insurance Management Center.

China's social insurance system is the largest in the world. About 900 million people are included in the endowment insurance system, and more than 1.3 billion are covered by medical insurance, according to a statement from the center provided exclusively to China Daily."
36,AliPay continues to grow for transit payments,"Chinese payment system Alipay is now accepted on on public transport across 30 cities, after Xi'an's subway became the latest addition this month. The scheme also includes an initiative to plant trees in regions suffering desertification, with Alipay users accumulating 'green' points every time they pay for subway travel using the wallet. Zhengzhou, Beijing and Shanghai were the first cities to allow mobile subway system payments last year.
",https://www.mobilepaymentstoday.com/news/alipay-adds-another-chinese-city-for-transit-payments/,2018-01-03 14:46:55.723000,"Xi'an is the latest Chinese city to accept Alipay on its subway system, according to local reports. The system started to accept Alipay as of Jan. 1.

As part of the initiative, riders can participate in a program meant to encourage more ""green"" ways of travel such as public transit.

Once an Alipay user accumulates a certain amount of green ""energy"" from using the mobile wallet to pay for subway fares, Alipay's partners, such as the Alxa SEE foundation, will plant a real tree in areas suffering from desertification upon the user's request.

Alipay is now accepted on public transport in more than 30 Chinese cities, including Hangzhou, Wuhan, Tianjin, Qingdao and Guangzhou. Zhengzhou became the first Chinese city to adopt mobile payments in its subway system in September, followed by Beijing and Shanghai."
37,Scientists turn to CRISPR to save cacao crops,"Scientists at the University of California, Berkeley are using the DNA manipulation technology CRISPR to ensure the survival of the cacao plant as the planet experiences warmer temperatures. The research is being carried out with confectionery giant Mars, which has pledged $1bn towards reducing its carbon footprint 60% by 2050. According to the National Oceanic and Atmospheric Association, by 2050 rising temperatures will have forced chocolate growers to altitudes 1000ft higher than today, while also increasing pest numbers and reducing water supplies. It's hoped that seedlings created at Berkeley will be equipped to thrive in these tougher conditions.
",http://uk.businessinsider.com/when-chocolate-extinct-2017-12?r=US&IR=T,2018-01-03 14:41:44.923000,"Cacao plants are under threat of devastation thanks to warmer temperatures and dryer weather conditions.

Scientists at the University of California are teaming up with Mars company to try to save the crop before it's too late.

They're exploring the possibility of using the gene-editing technology CRISPR to make crops that can survive the new challenges.

Beyond the glittery glass-and-sandstone walls of the University of California’s new biosciences building, rows of tiny green cacao seedlings in refrigerated greenhouses await judgment day.

Under the watchful eye of Myeong-Je Cho, the director of plant genomics at an institute that's working with food and candy company Mars, the plants will be transformed. If all goes well, these tiny seedlings will soon be capable of surviving — and thriving — in the dryer, warmer climate that is sending chills through the spines of farmers across the globe.

It's all thanks to a new technology called CRISPR, which allows for tiny, precise tweaks to DNA that were never possible before. These tweaks are already being used to make crops cheaper and more reliable. But their most important use may be in the developing world, where many of the plants that people rely on to avoid starvation are threatened by the impacts of climate change, including more pests and a lack of water.

Cacao plants occupy a precarious position on the globe. They can only grow within a narrow strip of rainforested land roughly 20 degrees north and south of the equator, where temperature, rain, and humidity all stay relatively constant throughout the year. Over half of the world's chocolate now comes from just two countries in West Africa — Côte d’Ivoire and Ghana.

But those areas won't be suitable for chocolate in the next few decades. By 2050, rising temperatures will push today's chocolate-growing regions more than 1,000 feet uphill into mountainous terrain — much of which is currently preserved for wildlife, according to the National Oceanic and Atmospheric Administration.

Mars, the $35 billion corporation best known for Snickers, is aware of these problems and others presented by climate change.

In September, the company pledged $1 billion as part of an effort called ""Sustainability in a Generation,"" which aims to reduce the carbon footprint of its business and supply chain by more than 60% by 2050.

""We're trying to go all in here,"" Barry Parkin, Mars' chief sustainability officer, told Business Insider. ""There are obviously commitments the world is leaning into but, frankly, we don't think we're getting there fast enough collectively.""

Its initiative with Cho at UC Berkeley is another arm of that efforts. If all goes as planned, they could develop cacao plants that don’t wilt or rot at their current elevations, doing away with the need to relocate farms or find another approach.

Jennifer Doudna, the UC Berkeley geneticist who invented CRISPR, is overseeing the collaboration with Mars. Although her tool has received more attention for its potential to eradicate human diseases and make so-called “designer babies,” Doudna thinks its most profound applications won’t be on humans but rather on the food they eat.

Courtney Verrill

An avid tomato gardener, Doudna thinks her tool can benefit everyone from large food companies like Mars to individual hobbyists like herself.

”Personally, I’d love a tomato plant with fruit that stayed on the vine longer,” Doudna told Business Insider.

The research lab she oversees at UC Berkeley is called the Innovative Genomics Institute. Many of the efforts by graduate students there focus on using CRISPR to benefit small-holder farmers in the developing world. One such project aims to protect cassava — a key crop that prevents millions of people from starving each year — from climate change by tweaking its DNA to produce less of a dangerous toxin that it makes in hotter temperatures.

Doudna founded a company called Caribou Biosciences to put CRISPR into practice, and has also licensed the technology to agricultural company DuPont Pioneer for use in crops like corn and mushrooms.

Regardless of which crop the public sees CRISPR successfully used in first, the technology will be a key tool in a growing arsenal of techniques we'll need if we plan to continue eating things like chocolate as the planet warms."
38,Publisher Purch profits from licensing its ad tech products,"Profitable Utah-based publisher Purch is building up a healthy business from selling licences for its ad tech products to its peers. CRO Mike Kisseberth said the company's managed-service product, which relies on server-side connections, is responsible for 20% of the company's $120m annual revenue. The now separate unit, Purch Publisher Services, plans to increase that to a quarter. Kisseberth said Purch may look to self-serve products in the future.
",https://digiday.com/media/purch-growing-ad-tech-licensing-business/,2018-01-03 14:41:05.233000,"With publishers realizing that they can no longer be wholly dependent on ads for their revenue, Purch is getting more serious about selling proprietary technology to other publishers.

Purch — a commerce-focused publisher that owns tech and product review sites such as Tom’s Guide, Top Ten Reviews and Live Science — is profitable. It makes about $120 million a year in revenue, with about 20 percent coming from ad tech products that it licenses to 25 publisher clients, said Purch CRO Mike Kisseberth. Over the next year, the company plans to grow its number of publisher clients to roughly 40, and have its tech licensing operation account for about 25 percent of its overall revenue, he said.

Purch began developing its own ad platform nearly four years ago. What’s changed is that the company has gotten more serious about licensing it to other publishers. In December, Purch broke out its tech licensing business into a separate unit, called Purch Publisher Services. About 60 of Purch’s 400 employees work on Purch’s tech platform at least part of the time, and 10 work on it full-time, Kisseberth said. These employees are made up of salespeople, engineers, data scientists and support specialists.

By licensing software, Purch is aiming to build a revenue stream in an area that most publishers have avoided. This is because most publishers don’t have the resources or patience to build their own ad tech, let alone build tech that can be licensed to other media companies.

One exception is The Washington Post, which calls itself a tech company and sells ad products to other publishers. But the Post is an outlier due to the fact that its owner, Amazon CEO Jeff Bezos, is a tech enthusiast who happens to be the richest person in the world.

What has driven the growth of Purch’s tech licensing business is that it was an early adopter of server-side bidding. Unlike on-page header bidding — where publishers simultaneously offer inventory to multiple exchanges before making calls to their ad servers — going server-to-server speeds up page-load times since the ad calls are hosted on publishers’ servers and not on users’ browsers. For over a year, Purch has sold all of its programmatic inventory through server-to-server connections.

The benefits of server-to-server might sound enticing, but as publisher tech teams are typically stretched thin, few publishers have shifted over to selling the bulk of their programmatic inventory this way. This is where Purch pitches itself as a vendor.

Purch’s server-side solution operates on a revenue share, but Kisseberth wouldn’t disclose monetary terms. It is a managed-service product where Purch takes control of the setup and maintenance, ad ops and relationships with the 30 supply-side platforms that are plugged into the product.



Purch last summer tested a self-service bidding product with some of its clients but found that it required more tech and service support than was worth it. As self-service gains steam within the ad tech industry, Purch is open to shifting its products to be self-service in the future, but that likely won’t happen in 2018, Kisseberth said.

Other tech products that Purch sells publishers include commerce management and CRM platforms. But these products are more geared toward B2B publishers. Purch’s programmatic bidding product is the main driver of its licensing business.

Purch doesn’t limit itself to selling its tech to non-competitors; its publisher clients include tech sites like VentureBeat, Mobile Nations and How-To-Geek. Purch figures that selling products to other tech sites, as long as they’re brand-safe, can bolster the reputation of Purch’s ad tech with buyers and in turn help Purch’s case when it comes to setting up private marketplace deals, where the ad rates tend to be much higher than on the open exchange.

Kisseberth emphasized that Purch isn’t looking to simply grow an audience extension. If a publisher already builds its own ad tech or runs its auctions server-to-server, then it’s likely not a fit as a client. Unlike the Washington Post, which licenses self-service products to a wide swath of publishers, Purch is focusing on selling its products to niche sites that want another publisher to control and scale their programmatic selling.

“This is not a huge land grab where we are signing thousands of publishers,” he said. “It is signing publishers who we think are good additions to our portfolio.”"
39,New York City to investigate renewable fuel for ferries,"New York City Council has commissioned a two-year study to determine the feasibility of using renewable fuels and technology to power the city’s ferries. Alternative fuels such as biodiesel and hybrid electric, battery electric and fuel-cell electric technologies will all be studied to assess which options are most compatible with the various types and classes of ferries the city uses.",http://www.biofuelsdigest.com/bdigest/2018/01/02/city-of-new-york-to-study-alternative-fuels-for-city-ferries/,2018-01-03 14:39:46.420000,"In New York state, the City of New York is set to spend the next two years analyzing the use of alternative fuels to power the city’s ferries following a successful City Council vote in mid-December. The study will look at biodiesel and hybrid electric, battery electric and fuel-cell electric technologies but also look at new ferries to evaluate their compatibility with these cleaner fuels. The study should be submitted to the city council by December 31, 2019."
40,New York City to investigate renewable fuel for ferries,"New York City Council has commissioned a two-year study to determine the feasibility of using renewable fuels and technology to power the city’s ferries. Alternative fuels such as biodiesel and hybrid electric, battery electric and fuel-cell electric technologies will all be studied to assess which options are most compatible with the various types and classes of ferries the city uses.",http://qns.com/story/2018/01/02/city-passes-queens-councilmans-bill-study-power-nyc-ferries-renewable-fuel/,2018-01-03 14:39:46.420000,"Sign up for our PoliticsNY newsletter for the latest coverage and to stay informed about the 2021 elections in your district and across NYC

A new bill passed in the City Council on Dec. 19, 2017 will require that a two-year study be completed to determine how feasible it would be to use renewable fuels and technology to power the city’s ferries.

Earlier this year, Mayor Bill de Blasio announced that he would expand ferry service to include routes in Astoria, Rockaway and South Brooklyn. Now, a new bill aims to make this expansion more environmentally friendly.

Introduced by Astoria Councilman Costa Constantinides, Int. No. 54-A will result in a study to analyze what alternative fuels can be used to power the motorized watercrafts.

Alternative fuels like biodiesel and hybrid electric, battery electric and fuel-cell electric technologies will all be considered in the study, which should be completed no later than Dec. 31, 2019.

The study would also require a review of the types and classes of ferries used, their compatibility with the alternative fuels and alternative fuel technologies, the availability of the fuels and technologies and other issues such as storage and regulatory requirements.

The most commonly used fuel, petroleum diesel fuel, generates greenhouse gases when it is burned, as well as harmful pollutants such as sulfur dioxide. The exhaust released by petroleum fuel can also cause respiratory illnesses such as asthma and lung disease.

According to the Staten Island Advance, the Staten Island Ferry began to use liquefied natural gas to power one of its ferries in 2013 as opposed to low-sulfur diesel, which was done to reduce carbon dioxide emissions and cut fuel costs.

“Over the past four years, our city has made environmental protection a priority – whether through ending our reliance on fossil fuels, cleaning our air quality, building sustainable transit habits, or encouraging use of renewable energy,” Constantinides said in a statement. “Int. 54 will help increase use of renewable fuel in one of our city’s most sustainable transit options – our ferries. As use of our citywide ferry system has grown exponentially, we must innovate the type of energy we use to fuel the boats.”

Another bill introduced by Constantinides also passed City Council on Dec. 19. It requires power plant operators in the city to stop burning dirty grades of fuel oil to power their plants sooner than originally proposed."
41,Amazon wins patent for on-demand 3D printing service,"Online retailer Amazon has secured the patent for a system which would allow it to 3D-print customer's orders in the same trucks used to deliver the items. In 2014, the company set up an online store for custom 3D printing but is limited to hardware and supplies. If the company continues with the technology it could lead to quicker delivery times and increased 3D printing on-the-go.
",https://www.geekwire.com/2018/amazon-gets-patent-3-d-printing-demand-pickup-delivery/,2018-01-03 14:17:39.183000,"Send Us a Tip

Have a scoop that you'd like GeekWire to cover? Let us know."
42,Glycolic acid electrolysis promises huge energy storage capacity,"Researchers at Kyushu University have created a device to store chemical energy through continuous electrolysis using glycolic acid, which they say has around 50 times the energy storage capacity of hydrogen. The team made an electrolytic cell using a new membrane-electrode assembly using an iridium oxide-based anode and a titanium dioxide-coated titanium cathode. The ideal operating temperature for the reaction is 60C, they said. ""The energy efficiency, as opposed to capacity, still lags behind other technologies"", said study co-author, Miho Yamauchi. ""However, this is a promising first step.""",https://phys.org/news/2018-01-exploring-electrolysis-energy-storage.html,2018-01-03 13:57:55.727000,"A Kyushu University research team realized continuous electrochemical synthesis of an alcoholic compound from a carboxylic acid using a polymer electrolyte alcohol electrosynthesis cell, which enables direct power charge into alcoholic compound. Credit: Masaaki Sadakiyo / International Institute for Carbon-Neutral Energy Research, Kyushu University

Interest in renewable energy continues to grow. Many renewables, though, can be frustratingly intermittent. When the sun stis obscured by clouds, or the wind stops blowing, the power fluctuates. The fluctuating supply can be partly smoothed out by energy storage during peak production times. However, storing electricity is not without its challenges.

Recently, a team at Kyushu University created a device to store energy in chemical form through continuous electrolysis. The researchers noted that glycolic acid (GC) has a much greater energy capacity than hydrogen, one of the more popular energy storage chemicals. GC can be produced by four-electron reduction of oxalic acid (OX), a widely available carboxylic acid. As described in their publication in Scientific Reports, the team devised an electrolytic cell based on a novel membrane-electrode assembly. Sandwiched between two electrodes are an iridium oxide-based anode and a titanium dioxide (TiO2)-coated titanium (Ti) cathode, linked by a polymer membrane.

""Flow-type systems are very important for energy storage with liquid-phase reaction,"" says lead author Masaaki Sadakiyo. ""Most electrolyzers producing alcohols operate a batch process, which is not suitable for this purpose. In our device, by using a solid polymer electrolyte in direct contact with the electrodes, we can run the reaction as a continuous flow without addition of impurities (e.g. electrolytes). The OX solution can effectively be thought of as a flowable electron pool.""

Another key consideration is the cathode design. The cathodic reaction is catalyzed by anatase TiO2. To ensure a solid connection between catalyst and cathode, the team ""grew"" TiO2 directly on Ti in the form of a mesh or felt. Electron microscope images show the TiO2 as a wispy fuzz, clinging to the outside of the Ti rods like a coating of fresh snow. In fact, its job is to catalyze the electro-reduction of OX to GC. Meanwhile, at the anode, water is oxidized to oxygen.

The team found that the reaction accelerated at higher temperatures. However, turning the heat up too high encouraged an unwanted by-process—the conversion of water to hydrogen. The ideal balance between these two effects was at 60°C. At this temperature, the device could be further optimized by slowing the flow of reactants, while increasing the amount of surface area available for the reaction.

Interestingly, even the texture of the fuzzy TiO2 catalyst made a major difference. When TiO2 was prepared as a ""felt,"" by growing it on thinner and more densely packed Ti rods, the reaction occurred faster than on the mesh—probably because of the greater surface area. The felt also discouraged hydrogen production, by blanketing the Ti surface more snugly than the mesh, preventing the exposure of bare Ti.

""In the right conditions, our cell converts nearly 100 percent of OX, which we find very encouraging,"" co-author Miho Yamauchi says. ""We calculate that the maximum volumetric energy capacity of the GC solution is around 50 times that of hydrogen gas. To be clear, the energy efficiency, as opposed to capacity, still lags behind other technologies. However, this is a promising first step to a new method for storing excess current.""

Explore further Self-healing catalyst films for hydrogen production

More information: Masaaki Sadakiyo et al, Electrochemical Production of Glycolic Acid from Oxalic Acid Using a Polymer Electrolyte Alcohol Electrosynthesis Cell Containing a Porous TiO2 Catalyst, Scientific Reports (2017). Journal information: Scientific Reports Masaaki Sadakiyo et al, Electrochemical Production of Glycolic Acid from Oxalic Acid Using a Polymer Electrolyte Alcohol Electrosynthesis Cell Containing a Porous TiO2 Catalyst,(2017). DOI: 10.1038/s41598-017-17036-3

Provided by Kyushu University, I2CNER"
43,Publishers invest in data protection officers for incoming GDPR,"Data protection officers (DPOs) are becoming crucial hires for publishers, ad tech companies and agencies affected by the impending General Data Protection Regulation (GDPR). DPOs reportedly command six-figure salaries due to the thin pool of GDPR expertise, and are virtually un-sackable. As publishing consultant Peter Lomax explains, the Information Commissioner’s Office has insisted that DPOs are not to be held accountable if their GDPR-related decisions hurt the business: ""You can’t take issue with how they approach [GDPR compliance]"". UK publishers News UK and Haymarket have filled DPO positions, though smaller publishers are likely to outsource in future.
",https://digiday.com/media/gdpr-made-chief-data-protection-officer-new-key-role-publishers/?utm_medium=email&utm_campaign=digidaydis&utm_source=uk&utm_content=180103,2018-01-03 13:54:36.497000,"To prepare for the coming General Data Protection Regulation, publishers have a new important role: data protection officers.

The Information Commissioner’s Office advised companies to hire DPOs last year, but unlike in Germany, where DPOs are now common at publishers, the U.K. has been far slower to embrace the role. News UK, which owns The Times and The Sun newspapers, was among the first to appoint a data protection officer. Haymarket has appointed one, and magazine publisher Future is planning to appoint one. Others have working groups comprised of staff across different parts of the business working on compliance. Ad tech companies and agencies are also bringing on data protection czars.

The DPO role comes with challenges. The ICO insists the DPO isn’t to be held accountable if decisions they make aren’t good for business.



“It’s the new important role at publishers, but it’s a strange role that’s virtually un-sackable,” said Paul Lomax, an independent publishing consultant and recently the chief technology officer at magazine group Dennis. “You can’t give them guidance on or take issue with how they approach it [GDPR compliance]. Basically you can’t fire them.”

Some publishers are simply choosing to expand current roles, such as privacy officers and heads of data. But with GDPR, the DPO cannot have any conflict of interest with other responsibilities across the business. So if it wasn’t a dedicated role before, it will have to be now, according to Lomax. In some cases, that may lead to some hasty job title changes, rather than investing in an unknown new hire.

Given GDPR is uncharted territory, real expertise and experience is thin on the ground. And with any role shortage, particularly in an area that requires senior, specialist skill sets, salaries are high. Industry sources have said DPO roles tend to be in the six-figure range.

In short, hiring a DPO is fraught with challenges. Whether or not a company must appoint one comes down to size and the extent to which they use data. Companies that have to renegotiate thousands of contracts with tech suppliers in order to ensure they’re compliant will likely already have a DPO or have plans to hire one. But for smaller businesses, outsourcing to external DPOs will be a more popular option.

Companies like Skimlinks, which provides affiliate link services to publishers, will likely outsource to a DPO in the spring, after the company has done the heavy lifting on its data mapping and other internal processes. The DPO will then just be tasked with auditing and validating all that’s happened, meaning the business can control and retain the expertise on how to comply with the law and then approaches an external DPO to rubber stamp its compliance as an unbiased partner. “That way it’s [GDPR knowledge] not all locked up in the mind of a DPO,” said Skimlinks co-founder Joe Stepniewski."
44,Croton nuts could be the future of biofuels,"The oilseed nuts of the croton tree, a prevalent shrub in Kenya, could hold the key to cheap development of biofuels in the country. Croton oil was found to generate 78% less CO2 emissions than diesel, which is in widespread use in rural areas of the country. At present, Eco Fuels Kenya buys the nuts from 5,000 farmers across the African nation, and is looking to garner support for the alternative fuel.",http://www.greenmatters.com/renewables/2017/12/29/Z24ro4k/kenyas-croton-nuts-could-be-the-future-of-biofuels-,2018-01-03 13:50:34.123000,"A prevalent shrub that grows almost everywhere across Kenya could be the key to a sustainable source of biofuel, set to replace diesel and potentially feed Africa’s growing demand for cheap, low-carbon energy.

Article continues below advertisement

Called the croton tree, this plant is widely used for firewood and shade, but its less used component—its oilseed nuts—are a powerful source for biofuels. And while the croton industry is still fledgling, this macadamia-sized nut could help Africa meet its sustainable development goals of clean energy, climate action and poverty reduction.

As of now, Kenya imports all of its oil. In rural communities, diesel use is widespread for everything from trucks to water pumps, but is barely affordable for the poverty-stricken farmers that rely on it. Meanwhile, in urban areas, car exhaust is causing dangerous levels of air pollution. Croton oil, on the other hand, generates 78 percent less carbon dioxide emissions than diesel.

Article continues below advertisement

While a biofuel replacement sounds good, this isn't the first time Kenya has promised such a massive overhaul of the fossil fuel system. In 2000, a plant native to Central America, jatropha, was introduced to the Kenyan landscape and billed as the saving biofuel crop. The government then took land away from farmers to grow thousands of acres of monoculture jatropha. In the end, yields of the plant were “dismal” and because 90% of the jatropha crops were established on former agricultural land, companies kept their land titles, leaving hundreds of farmers with no jobs and no land.

Article continues below advertisement

This time around, however, the government and companies pushing the croton tree are doing it differently, building sustainable business practices into the movement.

For one, the industry could also improve rural livelihoods; through the production of oil for energy and other products (such as animal feed and fertilizer), croton harvesting is an opportunity for many poor farmers to rise out of poverty. The trees don’t require an investment in water or fertilizer, and the harvest can last up to six months, which means it's a steady source of income. Additionally, sellers get paid upon delivery, unlike coffee famers in the region who have to wait months for a payout.

Article continues below advertisement

Additionally, the croton industry could help with food security: While many biofuel crops take edible ingredients out of the market, the croton nut is inedible, meaning it does not displace food for consumption. Additionally, because croton trees already grow all over the region, there's no need to create massive monocultures like the jatropha days, which potentially displace other food crops.

“Instead of going the way of monoculture, we have decided to collaborate with small-scale holders and minimise the risk for everyone involved,” said Myles Katz, the managing director of Eco Fuels Kenya, a startup pioneering the use of croton nuts for biofuel.



Article continues below advertisement

Eco Fuels Kenya currently buys nuts from 5,000 farmers across Kenya, and more are expressing interest. Michael Jacobson, professor of forest resources in the Penn State College of Agricultural Sciences, recently conducted a survey with Kenyan farmers to gauge interest in joining the movement. Most were ready to jump in.

“Many small farmers, although land constrained, have access to land to plant groves of croton trees if they become sold on the idea,” he said. “If they knew that there was going to be a dedicated market for croton, they would certainly add trees to their farm household lands.”"
45,Genomic Expression aims to simplify bladder cancer diagnosis,"Massachusetts-based biotech Genomic Expression has developed the OneRNA4Bladder project, a low-cost method of diagnosing bladder cancer which also halves the cost of care. The OneRNA test, which can be used to diagnose several types of cancer, offers a urine-based diagnostic and liquid biopsy, instead of a traditionally invasive and high-cost cystoscopy. Upon diagnosis, the platform also enables treatment selection, as well as monitoring response and recurrence. Treating bladder cancer in the US costs around $3.7bn per year.
",http://www.prweb.com/releases/2018/01/prweb15048428.htm,2018-01-03 13:20:06.080000,"OneRNA(TM) Making RNA Sequencing Actionable in Oncology ""This is an amazing project that delivers on our mission to save lives and increase the quality of life for the patients while reducing the cost of care“Gitte Pedersen, CEO of Genomic Expression

The OneRNA4Bladder project aims to save lives and halve the cost of care for bladder-cancer treatment. Bladder cancer is the most expensive cancer to treat due to expensive diagnostic procedures and ineffective drugs resulting in a recurrence rate of 70%.

The OneRNA4Bladder project, which validates a urine-based diagnostic and liquid-biopsy platform, has already been shown, in a smaller 500-patient study, to outperform cystoscopy in terms of sensitivity and specificity. For this reason, it may substitute the invasive, expensive cystoscopy procedure in the diagnostic workup for bladder cancer and for the identification of recurrence. To our knowledge, no other technology has successfully met this endpoint. Most other non-invasive tests add cost to the management of the disease instead of reducing cost simply because, instead of replacing cystoscopy, the test can only provide a supplement to it.

The new immune checkpoint inhibitors have recently been approved for bladder cancer. However, with a low overall response rate of 15%, this will soon become a very expensive option, driving up cost. Genomic Expression has developed a way to sequence RNA quickly and inexpensively and then link the statistically changes in tumor RNA to drugs that could provide more durable responses. This technology is called OneRNA™, and, applied as a liquid-biopsy platform, it can quickly and effectively diagnose cancer, select treatment, and measure response and recurrence.

Overall, this solution will guide patients toward more effective treatments, reduce the number of cystoscopies and biopsies, identify patients with a higher likelihood of responding to new, expensive immunotherapy treatments, and, finally, reduce the cost of bladder-cancer treatment.

Bladder cancer is the sixth most common cancer globally, with an estimated 357,000 new cases worldwide every year. Its high incidence coupled with its relapsing nature result in the highest lifetime treatment costs per patient of all cancers, accounting for almost $3.7 billion/year in direct costs. A significant part of this cost burden stems from routine use of highly invasive, expensive procedures such as cystoscopy in diagnosis and recurrence monitoring. Replacing routine cystoscopy with the OneRNA4Bladder system could cut the cost of diagnosing and managing bladder cancer in half. Perhaps even more striking, the costs associated with an untimely death due to bladder cancer (i.e., the ‘‘value’’ of life lost) approach $7 billion annually. OneRNA4Bladder is expected to significantly minimize the bladder-cancer burden on health care.

The market opportunity for OneRNA4Bladder is $4.5 Billion.

Genomic Expression can start commercializing various components of the technologies as tools for clinical development of drugs e.g. OneRNA™ is already commercially available as a tool to stratify patients into clinical trials. However once the OneRNA4Bladder clinical study is completed, the market for the solution as a diagnostic platform is estimated to $4.5 billion.

About Genomic Expression

Genomic Expression is finding the best drug for the patient and the best patient for the drug by sequencing RNA and linking changes due to cancer to drugs through its proprietary algorithms and databases. Analyzing RNA allows us to tell if a tumor will respond to the new immune therapies, which are the only kind of therapies that are potential cures. Right now, only one out of four cancer treatments prolongs life. We spend $100 billion on drugs every year, and eight million patients die. Genomic Expression was started as the diagnostic partner in the $32M Danish “Genome Denmark.” The company now has four in-house clinical programs established in selected cancers with clear unmet needs. The OneRNA™ test can be used on any type of cancer."
46,Life Sciences Partners announces $280m medical tech fund,"European investment firm Life Sciences Partners has raised €280m ($337m) for the LSP Health Economics Fund 2, which aims to support firms developing medical devices which also help reduce healthcare costs. The fund, which the company said was oversubscribed, will invest in around 15 private firms that have products relating to remote monitoring, drug compliance and big data analytics ""on the market or very close to market introduction"", according to LSP partner Rudy Dekeyser.
",https://www.digitalhealth.net/2018/01/vc-european-healthcare-tech/,2018-01-03 13:16:37.713000,"Venture capital firm Life Sciences Partners LSP has secured €280m in funding for new medical technologies, labelling it the largest fund in Europe dedicated exclusively to healthcare innovation.

The money will be invested in private companies developing forward-thinking medical devices, diagnostics and digital health products. Specifically, it will target technologies with the potential to improve the quality of patient care while simultaneously keeping healthcare costs under control.

According to the investment group, the LSP Health Economics Fund 2 was oversubscribed and exceeded both its target size and its original ‘hard’ cap.

Speaking to Digital Health News, Rudy Dekeyser, LSP partner, said: “In the digital health space we have a keen interest in proprietary and scalable products with both a clear potential to improve the prevention, diagnosis and treatment of major diseases and a straightforward impact on cost reduction for the healthcare system.”

Focused on Europe and the US, the fund will look to invest in around 15 private companies. The products of these companies will need to be “on the market or very close to market introduction”.

Dekeyser cited drug compliance, remote monitoring, big data analytics and clinical software as areas of particular interest.

He added that companies hoping for a share of the fund would have to “convince us that there is a clear path towards the integration of their innovative product in the complicated healthcare ecosystem, has to know who will pay for their product or services and should have access to the necessary partners for broad implementation of their product in the market.”

Investors in the fund include the European Investment Fund and a variety of health insurance companies and institutional investors.

Dr René Kuijten, managing partner of LSP, added: “With this new and sizable fund, we have now firmly established our health economics and medical technology strategy.”

Early in December, the UK government formed a partnership with the life sciences industry as part of a pledge to boost advancements in medical technology in Britain.

Earlier in December, Wayra UK and Merck Sharp & Dohme announced a £68,000 healthcare accelerator programme aimed at machine learning start-ups."
47,Scientists develop 8-week speed-breed crop harvest technique,"UK and Australian scientists have developed a speed-breeding technique that could enable crops to be harvested every eight weeks, according to an article in Nature Plants. LED lights were used to create day-long regimes in fully controlled growth environments, with success rates the team believes could lead to six generations of bread and durum wheat, as well as barley, peas, and chickpeas, each year. The research, by the John Innes Centre in the UK, the University of Queensland and University of Sydney, is aimed at finding ways to feed the growing human population.
",https://www.sciencedaily.com/releases/2018/01/180101144758.htm,2018-01-03 13:02:47.157000,"Pioneering new technology is set to accelerate the global quest for crop improvement in a development which echoes the Green Revolution of the post war period.

The speed breeding platform developed by teams at the John Innes Centre, University of Queensland and University of Sydney, uses a glasshouse or an artificial environment with enhanced lighting to create intense day-long regimes to speed up the search for better performing crops.

Using the technique, the team has achieved wheat generation from seed to seed in just 8 weeks. These results appear today in Nature Plants.

This means that it is now possible to grow as many as 6 generations of wheat every year -- a threefold increase on the shuttle-breeding techniques currently used by breeders and researchers.

Dr Brande Wulff of the John Innes Centre, Norwich, a lead author on the paper, explains why speed is of the essence:

""Globally, we face a huge challenge in breeding higher yielding and more resilient crops. Being able to cycle through more generations in less time will allow us to more rapidly create and test genetic combinations, looking for the best combinations for different environments.""

For many years the improvement rates of several staple crops have stalled, leading to a significant impediment in the quest to feed the growing global population and address the impacts of climate change.

advertisement

Speed breeding, says Dr Wulff, offers a potential new solution to a global challenge for the 21st century.

""People said you may be able to cycle plants fast, but they will look tiny and insignificant, and only set a few seed. In fact, the new technology creates plants that look better and are healthier than those using standard conditions. One colleague could not believe it when he first saw the results.""

The exciting breakthrough has the potential to rank, in terms of impact, alongside the shuttle-breeding techniques introduced after the second world war as part of the green revolution.

Dr Wulff goes on to say: ""I would like to think that in 10 years from now you could walk into a field and point to plants whose attributes and traits were developed using this technology.""

This technique uses fully controlled growth environments and can also be scaled up to work in a standard glass house. It uses LED lights optimised to aid photosynthesis in intensive regimes of up to 22 hours per day.

advertisement

LED lights significantly reduce the cost compared to sodium vapour lamps which have long been in widespread use but are ineffective because they generate much heat and emit poor quality light.

The international team also prove that the speed breeding technique can be used for a range of important crops. They have achieved up to 6 generations per year for bread wheat, durum wheat, barley, pea, and chickpea; and four generations for canola (a form of rapeseed). This is a significant increase compared with widely used commercial breeding techniques.

Speed breeding, when employed alongside conventional field-based techniques, can be an important tool to enable advances in understanding the genetics of crops.

""Speed breeding as a platform can be combined with lots of other technologies such as CRISPR gene editing to get to the end result faster,"" explains Dr Lee Hickey from the University of Queensland.

The study shows that traits such as plant pathogen interactions, plant shape and structure, and flowering time can be studied in detail and repeated using the technology.

The speed breeding technology has been welcomed by wheat breeders who have become early adopters.

Ruth Bryant, Wheat Pathologist at RAGT Seeds Ltd, Essex, UK, said: ""Breeders are always looking for ways to speed up the process of getting a variety to market so we are really interested in the concept of speed breeding. We are working closely with Dr Wulff's group at the John Innes Centre to develop this method in a commercial setting.""

Dr Allan Rattey, a wheat crop breeder with Australian company Dow AgroSciences, has used the technology to breed wheat with tolerance to pre-harvest sprouting (PHS) a major problem in Australia.

""Environmental control for effective PHS screening and the long time taken to cycle through several cycles of recurrent selection were major bottle necks. The speed breeding and targeted selection platform have driven major gains for both of these areas of concerns."""
48,WeChat claims it doesn't store users’ chat history,"WeChat has said a claim that the company was storing users' chat history was a ""misunderstanding"". In a blog post, WeChat said it ""will not use any content from user chats for big data analysis"". The comments followed media quotes from Li Shufu, chairman of Geely Holdings, who said Tencent chairman Ma Huateng ""must be watching all our WeChats every day"". Tencent scored 0 out of 100 for various privacy issues in a 2016 report by Amnesty International.
",http://www.silicon.co.uk/security/wechat-storing-users-data-surveillance-226613?inf_by=5a2e95d4681db89b168b4737,2018-01-03 13:00:05.133000,"The leading Chinese messaging app said it doesn’t store users’ chat history, after a top businessman said WeChat was ‘watching’ users

WeChat, China’s most popular messaging application, has denied “storing” users’ messages, following accusations by one of the country’s top businessmen that the Tencent Holdings-owned firm was spying on its users.

“WeChat does not store any users’ chat history. That is only stored in users’ mobiles, computers and other terminals,” WeChat said in a post on the platform.

The statement comes after Li Shufu, chairman of Geely Holdings, which owns the worldwide Volvo and Lotus car brands, was quoted by local media on Monday as saying Tencent chairman Ma Huateng “must be watching all our WeChats every day”.

Geely Holdings is one of China’s largest car manufacturers, and one of the few major companies without ties to the country’s government. It has owned Volvo since 2010, British taxi maker The London Electric Vehicle Company since 2013 and took a majority stake in British sports car maker Lotus Cars last year.

‘Misunderstanding’

In its carefully worded response, WeChat said Li’s remarks were the result of a “misunderstanding”.

“WeChat will not use any content from user chats for big data analysis,” the firm said in its post. “Because of WeChat’s technical model that does not store or analyse user chats, the rumour that ‘we are watching your WeChat everyday’ is pure misunderstanding.”

WeChat, like all social media firms operating in China, is legally required to censor public posts the country’s Communist Party designates as illegal, and its privacy policy says it may need to retain and disclose users’ information in response to government or law enforcement requests.

In a 2016 report, Amnesty International ranked Tencent zero out of 100 on various privacy criteria, noting it was the only company on the list that “has not stated publicly that it will not grant government requests to access encrypted messages by building a ‘backdoor'”.

Cyber laws

Tencent is the only Chinese company on Amnesty’s list, which also includes Japan’s Viber and Line and South Korea’s Kakao, as well as services such US-based companies such as Facebook, Apple, Telegram and Google.

Last September China’s internet regulator announced a new rule making chat group administrators and companies accountable for breaches of content laws.

The regulator also fined firms including Tencent, Baidu and Weibo for censorship lapses and demanded they improve content auditing measures.

Last June China brought into force the restrictive Cyber Security Law (CSL), which mandates certain companies to hold data within the country and to undergo on-site security reviews.

What do you know about the history of mobile messaging? Find out with our quiz!"
49,Sugarcane engineered to produce more oil for biojet fuel,"Researchers at University of Illinois at Urbana-Champaign are tinkering with sugarcane plants to create a more cost-effective feedstock for biofuels for aircraft than corn or soybeans. According to the team, lipidcane containing 20% oil is twice as profitable per acre than corn and five times more than soybeans, and could yield over 15 times more jet fuel per acre. The researchers also estimated that 23 million acres of lipidcane could produce 65% of the jet fuel used to supply US airlines, at a cost of $5.31 per gallon, cheaper than other biofuels.",http://www.biofuelsdigest.com/bdigest/2017/12/31/engineered-sugarcane-lipidcane-shows-promise-for-u-s-biojet-fuel/,2018-01-03 12:39:16.143000,"In Illinois, researchers from University of Illinois at Urbana-Champaign are engineering sugarcane plants, called lipidcane, to produce more oil as well as sugar. Growing lipidcane containing 20 percent oil would be five times more profitable per acre than soybeans, the main feedstock currently used to make biodiesel in the United States, and twice as profitable per acre as corn, according to their research. They estimate that compared to soybeans, lipidcane containing 5 percent oil could produce four times more jet fuel per acre of land. Lipidcane with 20 percent oil could produce more than 15 times more jet fuel per acre.

Researchers estimate that if 23 million acres in the southeastern United States was devoted to lipidcane with 20 percent oil, the crop could produce 65 percent of the U.S. jet fuel supply at a cost to airlines of US$5.31 per gallon, which is less than bio-jet fuel produced from algae or other oil crops."
50,Amazon tests voice-triggered audio search advertisements,"Amazon is testing audio advertisements for its voice-activated virtual assistant, Alexa. It is reported to have approached a number of companies to develop adverts that would be promoted on its Echo devices, with advertisers able to pay to optimise the placement of their products. The development would mean customers who ask for help with domestic problems could have branded products suggested to them or be played adverts for such products. The use of voice-activated virtual assistants in the US is predicted to grow significantly, making them attractive to advertisers as they often only offer one answer to a query.
",https://www.mediapost.com/publications/article/312381/amazon-testing-alexa-sponsored-voice-ads-report-s.html,2018-01-03 12:38:04.180000,"by Laurie Sullivan @lauriesullivan, January 2, 2018

Virtual assistants have marketers scrambling to figure out how to optimize content as companies like Amazon begin testing voice-triggered audio search advertisements.

Reports surfaced Tuesday that Amazon has been speaking to consumer products goods companies such as Clorox and Procter & Gamble to develop advertisements. The CPG companies would promote their products on Echo devices powered by the Alexa voice assistant.

Early discussions have centered on whether companies would pay for higher placement if a user searches for a product on the device, similar to how paid-search works on Google, according to CNBC, which cited ""people.""

This should not come as a surprise to marketers preparing to optimize content for voice searches. The ads are being described as what the industry refers to as sponsored content.

For example, if someone asks Alexa how to clean up a spill, Amazon's voice assistant would respond to the query by naming a specific brand that paid for the sponsorship or bid a higher price to serve the voice advertisement.

advertisement advertisement

Advertisers are focused on search placement on Alexa and on other hubs because voice assistants typically only provide one answer per consumer query.

Amazon has hinted at launching a voice-operated advertisement platform for sponsored products. And last week, reports surfaced that Amazon is testing several new advertising units and types.

Another offering would allow brands to target users based on past shopping behavior or perhaps shopping behavior at the Whole Foods market.

In May 2017, eMarketer estimated that the number of people in the U.S. using voice-enabled speakers would more than double to 36 million, with Amazon capturing nearly 71% of the market."
51,Third of US firms said they suffered hacker data breaches in 2017,"Hackers penetrated the data of 29% of US firms last year, with two-thirds of them suffering damage to their reputations as a result, according to a study by Zogby Analytics. It surveyed more than 400 senior US executives, 47% of whom said incidents were caused by external vendors or contractors. More than half said a lack of knowledge was the biggest obstacle to responding to a breach. A separate survey of IT executives by Balabit revealed 56% of 222 respondents believed employee personal data was most valuable to cyber criminals.
",https://www.esecurityplanet.com/network-security/almost-a-third-of-all-u.s.-businesses-were-breached-in-2017.html,2018-01-03 12:37:44.330000,"Twenty-nine percent of U.S. businesses suffered a data breach in the past year, according to a recent HSB survey of 403 senior executives in the U.S., conducted by Zogby Analytics.

Two thirds of respondents whose businesses were breached said their company’s reputation was negatively affected by the incident. Twenty-seven percent of respondents spent between $5,000 and $50,000 to respond to a breach, and 30 percent spent between $50,000 and $100,000.

Forty-seven percent of the breaches were caused by a third-party vendor or contractor, followed by employee negligence (21 percent) and lost or stolen mobile devices or storage media (20 percent). Just 11 percent were caused by hacking.

When asked to identify the biggest hurdle their organization faces in responding to a breach, 51 percent cited a lack of knowledge, while 41 percent said it comes down to a lack of resources.

“The results highlight how closely our economy and society are interconnected digitally,” HSB vice president Timothy Zellman said in a statement. “Almost all of our personal and business data can be accessible on the Internet through online business connections, websites and social media. And that exposes our private information to attacks from hackers and cyber thieves.”

Monitoring Privileged Users

A separate Balabit survey of 222 IT executives and IT security professionals found that 35 percent of respondents see themselves as the biggest internal security risk to networks within their organizations. While HR and finance staff may be easier targets for social engineering, IT staff have higher access rights than other users, including access to business-critical data.

When asked to identify the most important user data for spotting malicious activity, 47 percent of respondents listed the time and location of login, followed by private activities using corporate devices (41 percent), and biometrics identification characteristics such as keystroke analytics (31 percent).

Within the realm of privileged users, respondents said sysadmins present the biggest threat (42 percent), followed by C-level executives (16 percent).

When asked what data is most valuable to hackers, 56 percent of respondents cited personal employee data, followed by customer data (50 percent) and investor and financial information (46 percent).

“As attacks become more sophisticated, targeted attacks and APTs more commonly involve privileged users inside organizations — often via hacks involving stolen credentials,” Balabit security evangelist Csaba Krasznay said in a statement. “Today, IT security professionals’ tough job has become even tougher. It is not enough to keep the bad guys out; security teams must continuously monitor what their own users are doing with their access rights.”"
52,Record £400m invested in Cardiff commercial property last year,"Investment in Cardiff's commercial property sector reached record levels in 2017, at more than £400m ($542m), compared with £298m in 2016, according to Savills. Ross Griffin, director of investment at Savills Cardiff, said the figure was boosted by the £224.7m invested in the Central Square regeneration scheme. The data also revealed total investment in the office sector reached £350m, while prime yields for office investments dropped 75 basis points to 5.50%.
",http://www.walesonline.co.uk/business/commercial-property/cardiffs-thriving-commercial-property-market-14092710,2018-01-03 12:36:00.820000,"Sign up to FREE email alerts from Wales Online - The CardiffOnline Newsletter Invalid Email Something went wrong, please try again later. Subscribe Thank you for subscribing We have more newsletters Show me See our privacy notice

Investment into Cardiff's commercial property sector reached record levels in 2017 at more than £400m, according to international real estate advisory firm Savills.

In 2016 investment into the city totalled £298m.

The research by Savills shows that volumes were heavily skewed this year by the activity at the Central Square regeneration scheme, which amounted to a combined £224.7m.

In total, investment into the Cardiff office sector surpassed £350m, which represents a record high.

Prime yields for Cardiff office investments fall 75 basis points from the beginning of the year to 5.50% as strong investor interest has resulted in downward pressure.

The second largest sector in terms of investment levels in 2017 was the leisure sector with investment reaching £43m.

The sector was dominated by the £20.5m acquisition of Stadium Plaza by Naissance Capital Real Estate and the £22.1m purchase of the Clayton hotel by M&G Real Estate.

Ross Griffin, director of investment at Savills Cardiff, said: “Cardiff remains a popular investment destination, particularly for those looking to place their money into the regional office market.

The development at Central Square has significantly boosted the volumes for this year, providing an attractive opportunity.

""Looking ahead to 2018 we expect to see continued activity from institutions on both the buy and sell side as they look for long term income. Overseas capital will also be active, particularly for prime assets at attractive yields."""
53,Sale of Brooklyn's $905m Starrett City cleared by judge,"The sale of a huge apartment complex in New York has been cleared by a judge after it was challenged in court. Starrett City in Brooklyn is the largest federally subsidised housing development in the United States, with 5,581 apartments in a 145-acre site. It is being sold for $905m by the widow of its original developer, but the transaction has been opposed by a rival bidder, backed by a partner in the complex. The Supreme Court of the state of New York has now dismissed its objection, but the transaction still requires approval by state and federal officials.
",https://www.nytimes.com/2018/01/02/nyregion/starrett-city-sale-judge-rules.html,2018-01-03 12:29:51.420000,"Although the name of the complex was changed to Spring Creek Towers several years ago, it is still widely known as Starrett City. A massive development, it has its own power plant, schools, recreation center and ZIP code. The sale has garnered some notoriety not just because of its size but also because President Trump has a small stake in the complex.

Carol G. Deane, the managing partner of Starrett City Associates, who was behind the sale, had argued in court that she balanced the need to satisfy shareholders with a deal that could win government approval and preserve Starrett City as a home for low- and moderate-income New Yorkers.

More than 70 percent of the limited partners and beneficial owners approved the deal in September.

“We are pleased with the decision denying plaintiffs’ efforts to derail the sale of Spring Creek Towers,” Ms. Deane said in a statement Tuesday, “but not surprised because of the care we put into the process and choosing a buyer who is committed to maintaining the development as affordable and a quality place to live for the 15,000 residents who call it home.”

Joshua D.N. Hess, a lawyer for the dissidents, said Tuesday that they were reviewing the judge’s order and their options, which could include suing for damages.

Mr. Deane died in 2010. Ms. Deane was his third wife.

He tried to sell the complex to the highest bidder during a debt-fueled real estate boom in 2007, but the deal fell apart amid sharp criticism from city, state and federal officials, as well as tenants."
54,AI to be used in Canada to monitor social media for suicide risks,"The Public Health Agency of Canada is partnering with artificial intelligence (AI) firm Advanced Symbolics on a three-month pilot programme to monitor social media posts for indicators of suicidal behaviour. The scheme, set to cost up to $400,000 if extended to five years, seeks to better deploy Canada's mental health resources. In November, Facebook launched its AI suicide prevention tools globally, following Instagram's release of tools enabling users to report videos that demonstrated signs of suicidal thoughts.",https://www.engadget.com/2018/01/02/canada-track-suicide-risk-social-media-ai/,2018-01-03 12:28:05.697000,"The Canadian government is partnering with AI firm Advanced Symbolics to try to predict rises in regional suicide risk by monitoring social media posts. Advanced Symbolics will analyze posts from 160,000 social media accounts and will look for suicide trends. The company aims to be able to predict which areas of Canada might see an increase in suicidal behavior, which according to the contract document includes ""ideation (i.e., thoughts), behaviors (i.e., suicide attempts, self-harm, suicide) and communications (i.e., suicidal threats, plans)."" With that knowledge, the Canadian government could make sure more mental health resources are in the right places when needed.

Canada isn't the only one turning to technology in order to provide better support for those considering suicide or to prevent suicides from happening. In November, Facebook began a global rollout of its AI suicide prevention tools that reach out to users who post content that could be a sign of suicidal thought and allow other users to report content that they think might show signs of suicidal risk. Instagram, which is owned by Facebook, also released tools last year that allowed users to report live videos that showed signs of suicidal thought, which would prompt an offer of mental health resources to the person posting the content.

The project is scheduled to begin later this month and would initially end in June. During that time, Advanced Symbolics would monitor social media accounts for a period of three months as a pilot of the program. Afterwards, the Canadian government will determine if the program should be extended. It's authorized for up to five one-year extensions. The initial program period will cost the government just under $25,000 and if extended fully, would cost up to $400,000.

""To help prevent suicide, develop effective prevention programs and recognize ways to intervene earlier, we must first understand the various patterns and characteristics of suicide-related behaviours,"" a Public Health Agency of Canada spokesperson said to CBC in a statement. ""PHAC is exploring ways to pilot a new approach to assist in identifying patterns, based on online data, associated with users who discuss suicide-related behaviours.""

Canada residents suffering from suicidal thoughts can reach out to the Canadian Association for Suicide Prevention for help. US residents can call the National Suicide Prevention Lifeline at 1-800-273-8255."
55,Plans for first build-to-rent scheme in Belfast submitted,"A 19-storey apartment building in Belfast's Cathedral Quarter could become the city's first build-to-rent (BTR) scheme if submitted plans are given approval. The £15m ($20m) development by Lacuna and Watkin Jones is for 105 flats, and would meet an increasing appetite for renting in Northern Ireland. In Dublin, 65% of 25 to 39 year-olds are in rented accommodation, while several companies, including Ires Reit and Marlet Property Group, are planning BTR developments.
",https://www.independent.ie/business/commercial-property/first-buildtorent-homes-for-belfast-36454206.html,2018-01-03 12:24:52.913000,"Build-to-rent has been gaining traction in Dublin for some time now, with numerous developments either under way or in the process of being planned by major real estate companies. Stock photo: Deposit

Belfast is set to see its first build-to-rent (BTR) residential scheme following the submission of a planning application for a 19-storey apartment building in the city's Cathedral Quarter.

Should it get the go-ahead, the proposed £15m development by joint venture partners Lacuna and Watkin Jones would see the delivery of 105 one- and two-bed apartments on a site now occupied by a derelict building and surface car park.

Build-to-rent has been gaining traction in Dublin for some time now, with numerous developments either under way or in the process of being planned by major real estate companies such as Ires Reit, Kennedy Wilson, Hines Ireland, Marlet Property Group and the Cosgrave Property Group.

The increasing appetite for long-term rental over home ownership is being driven by a range of economic and societal factors. Quite apart from the shortage of housing supply and the relative dearth of mortgage finance, a recent report by real estate agents CBRE pointed to the changing nature of Ireland's demographics and living preferences.

CBRE noted that the proportion of renters here grew by 4.7pc in the five-year period between 2011 and 2016, to 497,111 households - or nearly 30pc of the population.

Looking at occupancy by age group, the report found younger people have a higher propensity to rent, with 65pc of the Dublin population aged 25-39 renting from a landlord. In the same age segment, 26pc of people own their home with the remainder renting from a local authority."
56,Smartphone apps secretly recording gamers' TV viewing habits,"More than 250 games sold in the Google Play Store and Apple Store have been found to contain software that uses smartphone microphones to track user's TV-watching habits and sell the data on to advertisers. A New York Times report said the software, developed by a company called Alphonso, often goes undetected by users who do not read their phone software use policies, where it is detailed. Alphonso CEO Ashish Chordia said its activity was compliant with Federal Trade Commission guidelines and users could opt out at any time.
",http://www.ibtimes.co.uk/hundreds-android-apps-found-covertly-using-your-phones-microphone-track-your-tv-habits-1653526,2018-01-03 12:23:09.927000,"Some smartphone games have been found using a specific software that uses your device's microphone to track users' TV watching habits and collect data for advertisers. According to a recent New York Times report, more than 250 games on the Google Play Store use software from a company called Alphonso that uses the smartphone's mic to listen for audio signals in TV ads and shows.

The data collected is then sold to advertisers for ad targeting and analysis.

NYT reports that the software is used in games, some of which are geared towards children. Although the software does not record human conversations, it can detect sounds even when a phone is stowed away in someone's pocket and the apps are running in the background.

Alphonso's chief executive Ashish Chordia told NYT that the company has also worked with film studios to analyse viewers' movie-watching habits in theaters as well.

""A lot of the folks will go and turn off their phone, but a small portion of people don't and put it in their pocket,"" Chordia said. ""In those cases, we are able to pick up in a small sample who is watching the show or the movie.""

While most apps seemed to be available in the Google Play Store, the Times noted that some were on Apple's App Store as well.

Although the software's activities are creepy, some of the apps do disclose its tracking of ""TV viewership details"" in their descriptions under the ""read more"" button and software use policies.

Both Apple and Google require apps to get explicit permission from users in order to access certain phone features such as the camera, microphone, location, photo gallery and more. However, most users don't usually read the disclosure and are often unaware they have agreed to let the app access their phone's microphone.

""The consumer is opting in knowingly and can opt out any time,"" Chordia said. He also noted that the company's disclosures comply with Federal Trade Commission guidelines and offer instructions for users to opt-out of the software on its website.

He added that the firm does not approve of its software being used in apps targeting children. However, it has been found integrated into a number of games such as ""Teeth Fixed"" and ""Zap Balloons"" by India-based KLAP Edutainment.

A simple search for ""Alphonso software"" and ""Alphonso automated"" on the Play Store yields numerous apps that integrate the software.

One game called ""Dream Run"" by Dumadu Games - which has been downloaded and installed by about 5000 to 10,000 users - discloses under a ""Read More"" button that it is integrated with Alphonso Automated Content Recognition (ACR) software.

""With your permission provided at the time of downloading the app, the ACR software receives short duration audio samples from the microphone on your device,"" the disclosure reads. ""Access to the microphone is allowed only with your consent, and the audio samples do not leave your device but are instead hashed into digital 'audio signatures.'

""The audio signatures are compared to commercial content that is playing on your television, including content from set-top-boxes, media players, gaming consoles, broadcast, or another video source (e.g., TV shows, streaming programs, advertisements, etc.).""

1 of 2

The revelation does seem to echo the years-long conspiracy theory that apps by major tech giants such as Facebook tap into users' smartphone mics to secretly listen in on conversations and offer up relevant ads. Facebook has long tried to dismiss the speculation.

""We have to be really careful as we have more devices capturing more information in living rooms and bedrooms and on the street and in other people's homes that the public is not blindsided and surprised by things,"" Dave Morgan, the founder and CEO of Simulmedia that works with marketers on targeted ads, told the Times. ""It's not what's legal. It is what's not creepy."""
57,Hong Kong property set for another year of steep price rises,"About 35,000 new flats are going on the market in Hong Kong this year, with flats costing less than HKD6m ($768,000) expected to be the most popular with first-time buyers. However, prices are forecast to remain stubbornly high this year, with predictions of further rises of 10% to 20%. Last year, flats near stations on the city's mass transit railway experienced the biggest increases, of between 15% and 20%. Apartment sales volumes are expected to be 5% higher than in 2017.
",http://www.scmp.com/property/hong-kong-china/article/2126393/supply-new-flats-hk-more-meet-demand-year-concerns-remain,2018-01-03 12:09:12.707000,"Most of the new supply to come up in Tseung Kwan O, Tai Po and Tuen Mun"
58,Blockchains could edge banks out of property financing: Deloitte,"Blockchain technology could help US real estate firms offer customers near-instant financing and mortgage deals, all but edging banks out of the process. Deloitte's Eric Piscini said blockchains have the potential to revolutionise land registry and make the mortgage-buying process almost seamless. Banks could retain a role by offering value-added products and services. However, Piscini said the technology was still in its infancy and getting local governments, the courts and homeowners to accept it would take time.",https://www.americanbanker.com/news/could-blockchains-replace-banks-in-real-estate-lending,2018-01-03 12:03:51.797000,"Real estate deals on a blockchain are becoming real.

The startup Propy recently sold an apartment in the Ukraine through its blockchain, and in the last week of December it began letting Californians buy and sell properties on its blockchain using bitcoin. They will be able to use U.S. dollars next year. It’s also offering other homes including a “Packer House”— a house located next to the Green Bay Packers’ stadium and training field that is draped in team paraphernalia and is available for $1 million.

Other startups, including ShelterZoom and RealBlocks, are offering other takes on the idea of buying and selling real estate on a distributed ledger. ShelterZoom has built an Ethereum-based platform that went live Dec. 14. RealBlocks lets people invest in housing on its blockchain with fiat or digital currency (and starting in February 2018, its own tokens). It has completed seven deals so far.

Distributed-ledger technology — a database that can live in many places at once, where transactions and smart contracts can be executed, theoretically without any need for middlemen — could simplify real estate investment, turning a complicated process into a series of clicks.

At some point in the near future, not only real estate transactions but mortgages themselves may be handled on a blockchain.

Banks will have to adapt.

“I don’t know if this is removing banks from the process — I think it will make them more efficient,” said Eric Piscini, principal, banking and technology consulting at Deloitte. “Maybe they’ll be leaner because they won’t need to have as many people as they used to, to manage those processes.”

What blockchains can do

Theoretically, almost every element of a real estate transaction could be handled on a blockchain.

“When you want to buy a piece of real estate, whether it’s commercial or retail, wherever the current process is very inefficient, which is most places, a blockchain platform can make it better, faster and cheaper,” Piscini said.

Propy, which is based in Menlo Park, Calif., calls itself the Amazon of real estate. Its site lets users search for properties and brokers the way Realtor.com and Trulia do.

It records deals on its blockchain registry, which it hopes will be adopted by many jurisdictions as an official ledger and as a way to issue title deeds online.

Herein is a big promise of blockchain: that it could replace today’s clunky title deed and registry processes, which involve going to a local town hall and getting a clerk to find the right documents.

Yet it will be a challenge to get thousands of local governments, as well as homeowners and real estate investors, to accept a number on a blockchain as the official deed to a property.

RealBlocks lets people invest in rental properties like Section 8 housing over a blockchain.

“Rather than having to set up LLCs and deal with the tax, legal and accounting complexities associated with purchasing real estate, we’re making the process seamless by doing it on the blockchain using tokens,” said Perrin Quarshie, RealBlocks' CEO.

The company can help users find a mortgage through its mortgage brokerage partner First National Financing. It has also partnered with SALT Lending so that after February, participants will be able to take out a loan or line of credit using the tokens they buy from RealBlocks as collateral.

A blockchain combined with smart devices could let real estate investors track the condition of their investments and know, for instance, that equipment is being repaired and replaced on a schedule.

“Almost in real time, you can know if that piece of real estate you invested in is in good condition or not,” Piscini said. “You don’t have to trust a third party for that; you can trust the blockchain.”

A blockchain can also let people who are nonresidents buy real estate in the U.S., which today is difficult.

And it could let more people participate.

“If someone who is managing a property can also be an investor in the property with that mechanism, then they would manage the property better,” Piscini said. “The renter or leaser might be more incentivized to do a good job maintaining the property if they’re also an investor.”

Legal, regulatory, public-sentiment hurdles

For real estate blockchains to work, several things need to happen: Governments, homeowners and investors would have to recognize and accept a blockchain registry. Small town halls would need to become blockchain-ready. Courts would have to accept smart contracts the way they accept paper-based contracts today.

“Blockchain is a very natural database technology to keep records like titles and to make them widely accessible,” said Dror Futter, partner at Rimon Law and a member of its blockchain practice. “The issue is, you need to have the real estate blockchain recognized as a title registry. You can’t have a situation where you have multiple registries.”

Consumers would have to be willing to accept a smart contract as their only way to engage with real estate participants.

“If something goes wrong, who’s picking up the phone?” Piscini said. “If there is a major event, an earthquake, how do you manage the smart contract? At the end of the day, are we willing to trust this? That’s going to be the biggest challenge.”

The tokens many blockchain startups plan to issue to represent real estate assets raise regulatory questions.

“Will those tokens be considered another risk or another type of equity or will they be considered just an investment in real estate?” Piscini said. “I think the jury is out on that.”

In Piscini’s view, the only way to get the entire real estate finance system to accept transactions on a blockchain would be for regulators to mandate its use.

Regulators might do this for three reasons: to make the real estate market more open; to exert control over the real estate market (for instance, to limit a North Korean investor’s U.S. purchases); and to obtain a macroeconomic, real-time view of the real estate market, so they can react immediately.

All of this will take time.

“Blockchain is the internet circa 1993,” Futter said. “The technology is still immature, it’s not user-friendly, there are still issues being identified, and hacks are occurring. It’s a little overhyped in terms of what it can deliver today. But it can do most of these things on a limited basis today.”

Mortgages on a blockchain

Eventually, it is likely that mortgages will be handled as self-executable smart contracts on a blockchain, rather than as paper documents.

“Could you do a mortgage completely by way of smart contract? Yes,” Futter said. “The technology is there today to form a mortgage between two parties. The question will be, from a legal perspective, will it be deemed an enforceable agreement? That’s more a question of evidence than anything else.”

States like California are starting to accept smart contracts as legal evidence, so long term this will not be an obstacle.

Futter believes smart contracts themselves won’t contain every term of a mortgage agreement. They might contain key terms like interest rate, loan amount and duration. But an underlying master agreement would cover all the terms and conditions typical of a mortgage.

A blockchain could also facilitate crowdsourced mortgages. Instead of taking out a $200,000 loan from one lender, a borrower could get $2,000 each from 100 investors.

What banks should do now

Blockchain technology will take over the recording and transaction activities banks do today, Piscini said. Therefore, they need to focus on value-added services.

“Now it’s not just lending money, it’s managing property and helping people do a lot of things outside of just getting money to buy real estate,” Piscini said. “So the banks have to reinvent themselves and find new services and solutions.”

Futter suggests that at a minimum, banks should have people following these developments. They could be experimenting with creating records, tracking documentation and verifying transactions on a blockchain.

“The financial crisis showed this recordkeeping aspect is not the biggest strength of a lot of banks,” Futter said. “The blockchain creates a reliable storage mechanism that’s accessible depending on whether you do a public chain or private chain. You can store all the documentation around the mortgage transaction, including the financing, on a blockchain. You could do the mortgage processing automatically going forward, payments could be made and foreclosure would occur automatically —those kinds of things are all doable on the blockchain.”

This is all true for auto lending as well, he noted. It is also true for other types of loans, debt collection and many other related services.

Editor at Large Penny Crosman welcomes feedback at penny.crosman@sourcemedia.com."
59,Purplebricks posts profit but business model questions remain,"UK hybrid estate agency Purplebricks revealed a £3.2m ($4.3m) operating profit in its mid-December interim results, and has revised up its revenue guidance by 5% to £84m. However, the results did not include data on how many properties had been sold in the six months to October, prompting some to query the company's £4.16 share price and question if its up-front agent fee business model was sustainable. The firm's Australian arm was on target to achieve its full-year revenue guidance of £12m, despite incurring losses of £5.1m, the results showed.
",http://www.afr.com/real-estate/residential/purplebricks-business-model-under-fire-20180101-h0c2y5,2018-01-03 11:51:04.513000,"This omission was noted by popular share investing website The Motley Fool with British analyst GA Chester writing that previously, various figures Purplebricks gave made it possible to at least estimate the number.

""My calculations of the average sale price suggested that either the company was cornering the market in trailer park homes sales or that a rather large proportion of instructions weren't being converted to completions,"" Mr Chester wrote.

""Obviously, if you're charging a fixed fee but fail to complete the sale in too many cases, you're not going to have a sustainable business in the longer term,"" he added.

Purplebricks' Australian website shows its agents have sold 2247 homes since September 2016 out of total listings of 3495. This suggests a clearance rate of 70-75 per cent, if recent listings are excluded.

Also a concern is that while Purplebricks has continued to ramp up its British advertising spending, UK revenue growth has halved in the past two years from 154 per cent in the first of 2016-17 to 77 per cent in the second half of 2017-18.

""For me, this trend appears ominous for the market's future top and bottom-line growth expectations,"" Mr Chester said.

Advertisement

Investors nervous

Despite these issues, Neil Woodford, Britain's most high-profile fund manager, remains a strong backer of Purplebricks, with his Woodford Investment Management Ltd retaining a 27 per cent stake having bought into the float.

However, investors are clearly nervous – the Purplebricks share price dropped 6 per cent in September when it was briefly, incorrectly reported that Woodford had reduced its stake to just 2.99 per cent.

Old Mutual is another backing Purplebricks – the insurance and banking group recently increased its stake in the company to 12.6 per cent from 11.1 per cent.

In its latest interim results released in mid-December, Purplebricks reported that total group revenue more than doubled to £46.8 million with its British business posting a healthy £3.2 million operating profit.

However, losses at its Australian business more than doubled to £5.1 million after it spent £5.7 million on marketing the brand locally.

Advertisement

Purplebricks upgraded its UK revenue guidance 5 per cent to £84 million as part of its interim results. The company said it was on course to achieve full-year revenue guidance of £12 million ($20.8 million) in Australia.

Chief executive and co-founder Michael Bruce said the company's Australian business was ""on track"" and performing ahead of expectations.

""Our progress in Australia has been exciting and encouraging. Our market share in Australia is greater than our market share was in the UK at the same time in its evolution,"" Mr Bruce said.

The Australian Financial Review has reported on a few notable successes by Purplebricks estate agents, including veteran property executive Bryce Mitchelson, the managing director of $500 million childcare trust Arena REIT selling a three-bedroom Edwardian home in Elsternwick for more than $1.7 million after just two weeks with Purplebricks and saving an estimated $38,000 in commission fees.

In another high-profile result, Purplebricks saved property developer David Fam more than $61,000 on the $3.1 million sale of his Sutherland Shire mansion.

However, the Financial Review has also highlighted what happens when Purplebricks does not achieve a result, leaving a customer with a big bill and a house that is still for sale.

This was the case for Sydney woman Kerryn Lehmann who ended up owing Purplebricks $12,000 when her four-bedroom riverfront home in Como in the Sutherland Shire failed to sell."
60,Probuild looks to car plants for tips on construction efficiency,"Australian building company Probuild has learned from the assembly line process of car manufacturing to significantly speed up the construction of a residential tower in Melbourne, enabling its completion ahead of schedule. Probuild focused on having more control over its supply chain to reduce delays and also on moving workers around the site more efficiently to reduce downtime. It also worked more closely with suppliers so that materials could be delivered pre-cut, which reduced assembly time. The firm relied on data analysis to create further efficiencies and intends to apply what it has learned to future projects.
",http://www.afr.com/real-estate/turning-construction-on-its-side-probuild-cuts-building-time-labour-costs-20171226-h0aeuy,2018-01-03 11:45:59.707000,"The tale illustrates the productivity lessons – and skills – Australian construction can take from the local car manufacturing industry.

Traditional ways

It wasn't always like that, however. Probuild, like many of its competitors in the industry, was suffering under the traditional ways of working and needed to change. It hired engineers with experience of the automotive production chain, who pointed out vast inefficiencies.

""We had huge issues with that supply chain,"" says Luke Stambolis, Probuild's managing director for Victoria. ""There wasn't enough time during the day to get materials up.""

Change required overturning established ways of operating. The first part involved working more closely with suppliers and understanding their processes. For a company used to working with a supplier on a lump-sum payment basis – which put all the financial risk on to the supplier – the head contractor sought to take on some of that risk itself.

Probuild's 62-storey Empire Melbourne residential tower under construction for client Mammoth Empire. Andy Henderson

Advertisement

""It became less of a dictatorship and more of a collaborative approach,"" Grant says.

On Empire Melbourne, the company had worked with the same plumber for 10 years, but had never set foot in his factory, asked about his processes or consulted with him on how to improve things.

""We didn't even know where they were getting their materials from,"" Grant says.

Took on board

But when they asked the plumber's opinion for a better way to deliver supplies – instead of having products delivered in bulk, cut to size on site and installed – the plumber came up with a stillage, a frame that fitted into the hoists used to lift materials. The plumber delivered materials precut, in the stillage, which he would take back empty and refill.

The completed 62-storey Empire Melbourne tower. Supplied

Advertisement

""It was something we took on board and spread through the entire site. All subcontractors ended up adopting that model for delivery,"" Grant says.

The process cut the volume of waste material needing to be removed from the site by 3000 cubic metres – saving $150,000 in shipping costs alone. For the contractor it resulted in a saving of between 2 per cent and 5 per cent on its bills to subcontractors.

Another bottleneck was moving people and materials around. The site had three lifts for people and materials and at peak times, such as the start and end of a shift, it could take an hour to move the 300 people clocking on or off at the same time. But by putting the three lifts on a timetable – with some express to certain floors while others were milk runs to every floor – and showing the waiting workers on screens which lifts were going where, they sped things up.

Measure performance

""We knew we could load our whole building at peak time in 22 minutes,"" he says.

""If we have control over materials through the warehouse and we're able to get workers to the floor as quickly as we can and know when materials are being dropped off, we know we can build as efficiently and productively as possible.""

The two-year-old process will keep evolving and the contractor is now trying to make better use of data to measure performance.

""I'd like to think by this time next year I'd be able to show in real time how a project's tracking, even how a particular level on a project is tracking,"" Grant says. ""That's where we could be improving, for sure."""
61,Diageo pulls ads from Snapchat over fears about under-18s viewing,"Drinks company Diageo has withdrawn all its advertising from Snapchat, following a ruling by the UK Advertising Standards Authority that it did not do enough to ensure a campaign for Captain Morgan's rum, using a pirate lens, did not reach users below the legal drinking age. The Captain Morgan lens has become the first Snapchat lens to be banned in the UK. Diageo and Snapchat are said to be discussing updates to interest-based targeting that allow brands to infer user age, which were only applied by the social media site after the campaign.
",https://digiday.com/marketing/diageo-pulls-global-ad-spend-snapchat/?utm_medium=email&utm_campaign=digidaydis&utm_source=uk&utm_content=180103,2018-01-03 11:36:59.307000,"Diageo is starting the new year with a resolution not to advertise on Snapchat — at least not until it can be sure of the social network’s ability to keep its ads away from users under the legal drinking age.

The alcohol advertiser has stopped all advertising on Snapchat while it tries to understand how its ads may have inadvertently reached the social network’s youngest users. Diageo has not taken “sufficient” care to prevent its ads from reaching kids and teenagers, the Advertising Standards Authority concluded after an investigation, the results of which were announced on Jan. 3. The ASA banned Diageo from running a sponsored lens for its Captain Morgan rum brand ever again.

While the lens debuted last summer without sparking any complaints from the public, the ASA decided it needed to be investigated due to concerns it appealed to people under the legal drinking age. The regulator wanted to set a “precedence” in this space, revealed an ASA spokesman, who added Captain Morgan is the first branded Snapchat lens to be banned in the U.K.

The rationale behind the decision was simple enough. By adding a beard and a pirate hat to a user’s face, the lens broke strict alcohol advertising rules on targeting kids, specifically on how ads must not use real or fictitious characters who are likely to encourage kids and teenagers to drink.

The ruling relies on the assumption that a significant portion of Snapchatters claim to be over 18 when they are not.



Diageo, however, stressed it bought the lens, which typically cost between $500,000 (£368,000) and $1 million (£736 million) per day, last year based on assurances Snapchat had given it. At the time Diageo ran the campaign, Snapchat had not launched the interest-based targeting it has since claimed allow brands to supplement age with behavioral data to infer the ages of potential audiences. Those updates are currently being discussed between the platform and the alcohol advertiser.

Captain Morgan took “all reasonable steps to ensure the content we put on Snapchat was not directed at under 18s,” said a spokeswoman for the brand. Diageo has now stopped all advertising on Snapchat globally, the spokeswoman added.

Any future investigations could stunt Snapchat’s plans to win over more alcohol advertisers. The social network has been trying to convince alcohol brands they have nothing to fear about marketing on the teen-friendly app but announcements like the ASA’s ruling threaten to undermine those efforts. Snapchat may have age-gating restrictions not to dissimilar to rivals Facebook and Instagram, and yet its status as a kid-friendly app potentially leave it open to greater scrutiny.

For example, Instagram does not ask for age. Instead, the social network pulls the age from a person’s Facebook profile if they have connected accounts. If not, Instagram prompts users for their age but does not use additional signals to determine whether the figure given is true, making it potentially less secure than Snapchat.

The issue with age verification on social media platforms is that if people want to circumnavigate it, they can and will, said Norm Johnston, chief digital officer at Mindshare Worldwide. “Brands that operate with age restrictions around advertising, whether that is alcohol, gambling or something else, always have the potential to run into this kind of trouble,” added Johnston."
62,Hong Kong property site GoHome.com.hk gets user interface remake,"Hong Kong online property platform GoHome.com.hk, part of REA Group, has overhauled its website to improve its customer experience. As well as featuring a new search result and property details page, the site now includes a property and serviced-apartment listings section, and automatically adapts to whatever device is being used to access it.
",https://www.centralcharts.com/en/11617-rea-holdings-plc/news/1235716-revolutionising-hongkongers-property-buying-journey-with-gohome-com-hk-s-new-website,2018-01-03 11:32:20.573000,"HONG KONG, Jan. 3, 2018 /PRNewswire/ -- For an even better property searching experience, GoHome.com.hk, part of REA Group, has recently launched a new website with refined functionalities and layout improving the user experience.

People can now can search for their dream property and find out the latest property insights about the area, property prices and information about the property through one simple click. The new website has a new search result page and property details page which offers:

A new property and serviced apartment section

Comprehensive secondary property listings

Responding to Hong Kong consumer demand, GoHome.com.hk's has introduced a new mobile responsive function which allow layouts to be automatically fitted for multiscreen devices such as desktops, tablets and mobile devices, meaning the search for an ideal home is now even easier when you're on the go.

Ms. Kerry Wong, Chief Executive Officer, Greater China Region, REA Group, said ""GoHome.com.hk is the place that people use to find their perfect home. We've focused on improving the experience so people can now effortlessly explore and search for their ideal properties using specific criteria anywhere and anytime they want to.""

""By providing comprehensive and timely property information, we're changing the way our customers and consumers better understand property insights and trends by giving them access to the latest information in addition to searching for the perfect property through the one portal,"" said Ms Wong.

Across its global network, REA Group's purpose is to change the way people experiences property through delivering the best property insights and information on their websites and creating the most engaging consumer experience to help people find their perfect place more quickly and easily.

For media queries, please contact:





REA Group (Hong Kong) Vis Communications Consultancy Limited Ms. Hermia Chan Mr. Felix Poon Tel.: +852 3965 4326 / + 852 9386 0166 Tel.: +852 2804 2388 / +852 9202 2885 Email: hermia.chan@rea-group.com Email: felix@vis-pr.com

About GoHome.com.hk

GoHome.com.hk is Hong Kong's leading online property platform. Since 1999, GoHome.com.hk has been focused on providing value-added search experiences for the property-related industry and market in Hong Kong, Greater China and ASEAN.

GoHome.com.hk was named ""Property Portal of the Year"" by Marketing Magazine in 2011, 2012 and 2013, ""Best Property Developer Partner – Most Comprehensive Property Website"" by Capital Magazine in 2013, 2014 and 2015, as well as ""Outstanding Online Property Information Platform"" at the Hong Kong Digital Brand Awards by Metro Broadcast Corporation Limited and CHKCI in 2017.

About REA Group Limited

REA Group Limited ACN 068 349 066 (ASX:REA) (""REA Group"") is a multinational digital advertising business specialising in property. REA Group operates Australia's leading residential and commercial property websites, realestate.com.au and realcommercial.com.au, Chinese property site myfun.com and a number of property portals in Asia via its ownership of iProperty Group. REA Group also has a significant shareholding in US based Move, Inc and PropTiger in India.

Within Hong Kong, REA Group Asia operates GoHome.com.hk, squarefoot.com.hk and SMART Expo. The brands aim to provide consumers with extensive local and overseas property news, listings and investment opportunities while offering property and home-related advertisers with a one-stop, multi-platform solution.

SOURCE GoHome.com.hk, part of REA Group"
63,L'Oreal to widen media-ownership plan in Spanish-speaking world,"Cosmetics firm L'Oreal is set to expand a media strategy piloted in Mexico to other Spanish-speaking nations, according to CMO Andres Amezquita. Using its media brand Fiufiu, L'Oreal plans to capture first-party cookies to promote its online advertising. Brands will have to take more ownership of their data and apply similar models to those employed by the adtech companies to target their audience effectively.
",https://adexchanger.com/advertiser/loreal-giving-media-ownership-spin/,2018-01-03 11:30:18.630000,"L'Oreal will expand a media ownership strategy it piloted in Mexico to other Spanish-speaking countries to generate first-party cookies from its customer base. The idea is similar to sponsored content, but instead of working with a brand-name media company on a story package, for the past year L’Oreal has developed fiufiu, a kind of pop-up media brand that turns out social media aggregated lists, influencer columns and work by the content generation startup Cultura Colectiva. Fiufiu has a website and a Facebook presence. During some months it generates unique visitor numbers on par with well-known beauty magazines, said L'Oreal Hispanic countries CMO Andres Amezquita. The company could generate page views and engagement if it paired social influencer campaigns with beauty magazine sponsored content deals, but with fiufiu L’Oreal gets first-party cookies and the opportunities to request email addresses that come with page ownership. “Part of our programmatic approach is to have a lot of data and understand our consumers,” Amezquita said. “The idea is to be able to have a one-on-one conversation at the scale of mass communication, but to do that in the current environment, we need to capture cookies.”

Recent policy changes at the government and operating system levels, such as GDPR in Europe and Apple’s Safari Intelligent Tracking Prevention, block access to cookie data for marketers.

Younger customers, who make up most of the social-driven L’Oreal site, also respond better to in-article recommendations than the hard sell of ads on the page, Amezquita said. and every page is another chance to gather an email address or send someone to a L’Oreal product page.

Consumer brands need first-party cookies to advance their online advertising. Since ad tech companies that do audience targeting or retargeting are losing access to first-party data, brands will need to bring their own data to continue running data-driven campaigns.

For instance, L’Oreal can use fiufiu to generate retargeting audiences and extrapolate its first-party data through lookalike models scaled for programmatic, Amezquita said.

“If you look at industry right now, it’s a moment where content, CRM and digital advertising are becoming one,” he said."
64,Engineers convert fossil fuels to power without emitting CO2,"Scientists at Ohio State University are creating new technologies that will convert fossil fuels into products, such as electricity, without emitting CO2. The engineers have developed a process through which shale gas is turned into methanol and gasoline, all while consuming carbon dioxide. Under the right conditions, the technology is capable of consuming all the CO2 in the process, and the researchers have found a way to make the process commercially viable.",https://techxplore.com/news/2018-01-fossil-fuel-technology-doesnt-pollute.html,2018-01-03 11:19:39.227000,"L.S. Fan, Distinguished University Professor in Chemical and Biomolecular Engineering at The Ohio State University, holds samples of materials developed in his laboratory that enable clean energy technologies. Credit: Jo McCulty, The Ohio State University.

Engineers at The Ohio State University are developing technologies that have the potential to economically convert fossil fuels and biomass into useful products including electricity without emitting carbon dioxide to the atmosphere.

In the first of two papers published in the journal Energy & Environmental Science, the engineers report that they've devised a process that transforms shale gas into products such as methanol and gasoline—all while consuming carbon dioxide. This process can also be applied to coal and biomass to produce useful products.

Under certain conditions, the technology consumes all the carbon dioxide it produces plus additional carbon dioxide from an outside source.

In the second paper, they report that they've found a way to greatly extend the lifetime of the particles that enable the chemical reaction to transform coal or other fuels to electricity and useful products over a length of time that is useful for commercial operation.

Finally, the same team has discovered and patented a way with the potential to lower the capital costs in producing a fuel gas called synthesis gas, or ""syngas,"" by about 50 percent over the traditional technology.

The technology, known as chemical looping, uses metal oxide particles in high-pressure reactors to ""burn"" fossil fuels and biomass without the presence of oxygen in the air. The metal oxide provides the oxygen for the reaction.

Chemical looping is capable of acting as a stopgap technology that can provide clean electricity until renewable energies such as solar and wind become both widely available and affordable, the engineers said.

""Renewables are the future,"" said Liang-Shih Fan, Distinguished University Professor in Chemical and Biomolecular Engineering, who leads the effort. ""We need a bridge that allows us to create clean energy until we get there—something affordable we can use for the next 30 years or more, while wind and solar power become the prevailing technologies.""

Five years ago, Fan and his research team demonstrated a technology called coal-direct chemical looping (CDCL) combustion, in which they were able to release energy from coal while capturing more than 99 percent of the resulting carbon dioxide, preventing its emission to the environment. The key advance of CDCL came in the form of iron oxide particles which supply the oxygen for chemical combustion in a moving bed reactor. After combustion, the particles take back the oxygen from air, and the cycle begins again.

The challenge then, as now, was how to keep the particles from wearing out, said Andrew Tong, research assistant professor of chemical and biomolecular engineering at Ohio State.

While five years ago the particles for CDCL lasted through 100 cycles for more than eight days of continuous operation, the engineers have since developed a new formulation that lasts for more than 3,000 cycles, or more than eight months of continuous use in laboratory tests. A similar formulation has also been tested at sub-pilot and pilot plants.

""The particle itself is a vessel, and it's carrying the oxygen back and forth in this process, and it eventually falls apart. Like a truck transporting goods on a highway, eventually it's going to undergo some wear and tear. And we're saying we devised a particle that can make the trip 3,000 times in the lab and still maintain its integrity,"" Tong said.

This is the longest lifetime ever reported for the oxygen carrier, he added. The next step is to test the carrier in an integrated coal-fired chemical looping process.

Another advancement involves the engineers' development of chemical looping for production of syngas, which in turn provides the building blocks for a host of other useful products including ammonia, plastics or even carbon fibers.

This is where the technology really gets interesting: It provides a potential industrial use for carbon dioxide as a raw material for producing useful, everyday products.

Today, when carbon dioxide is scrubbed from power plant exhaust, it is intended to be buried to keep it from entering the atmosphere as a greenhouse gas. In this new scenario, some of the scrubbed carbon dioxide wouldn't need to be buried; it could be converted into useful products.

Taken together, Fan said, these advancements bring Ohio State's chemical looping technology many steps closer to commercialization.

He calls the most recent advances ""significant and exciting,"" and they've been a long time coming. True innovations in science are uncommon, and when they do happen, they're not sudden. They're usually the result of decades of concerted effort—or, in Fan's case, the result of 40 years of research at Ohio State. Throughout some of that time, his work has been supported by the U.S. Department of Energy and the Ohio Development Services Agency.

""This is my life's work,"" Fan said.

His co-authors on the first paper include postdoctoral researcher Mandar Kathe; undergraduate researchers Abbey Empfield, Peter Sandvik, Charles Fryer, and Elena Blair; and doctoral student Yitao Zhang. Co-authors on the second paper include doctoral student Cheng Chung, postdoctoral researcher Lang Qin, and master's student Vedant Shah. Collaborators on the pressure adjustment assembly work include Tong, Kathe and senior research associate Dawei Wang.

The university would like to partner with industry to further develop the technology.

The Linde Group, a provider of hydrogen and synthesis gas supply and plants, has already begun collaborating with the team. Andreas Rupieper, the head of Linde Group R&D at Technology & Innovation said that the ability to capture carbon dioxide in hydrogen production plants and use it downstream to make products at a competitive cost ""could bridge the transition towards a decarbonized hydrogen production future."" He added that ""Linde considers Ohio State's chemical looping platform technology for hydrogen production to be a potential alternative technology for its new-built plants"".

The Babcock & Wilcox Company (B&W), which produces clean energy technologies for power markets, has been collaborating with Ohio State for the past 10 years on the development of the CDCL technology - an advanced oxy-combustion technology for electricity production from coal with nearly zero carbon emissions. David Kraft, Technical Fellow at B&W, stated ""The CDCL process is the most advanced and cost-effective approach to carbon capture we have reviewed to date and are committed to supporting its commercial viability through large-scale pilot plant design and feasibility studies. With the continued success of collaborative development program with Ohio State, B&W believes CDCL has potential to transform the power and petrochemical industries.""

More information: Mandar Kathe et al. Utilization of CO2 as a partial substitute for methane feedstock in chemical looping methane–steam redox processes for syngas production, Energy & Environmental Science (2017). Mandar Kathe et al. Utilization of CO2 as a partial substitute for methane feedstock in chemical looping methane–steam redox processes for syngas production,(2017). DOI: 10.1039/C6EE03701A Cheng Chung et al. Chemically and physically robust, commercially-viable iron-based composite oxygen carriers sustainable over 3000 redox cycles at high temperatures for chemical looping applications, Energy & Environmental Science (2017). DOI: 10.1039/C7EE02657A Journal information: Energy & Environmental Science"
65,Regus Two new business centres opened in Liverpool,"Shared office provider Regus has added two new business centres to its portfolio in Liverpool. The new sites are located at Merchants Court, on Derby Square, and at 1 Mann Island, alongside the docks – both close to its Exchange Flags city-centre offices. Regus UK Chief Executive Richard Morris said the expansion is in response to growing demand for flexible office space in Liverpool, which is attracting investors from around the world.
",https://www.insidermedia.com/insider/northwest/regus-opens-two-new-business-centres,2018-01-03 11:03:38.037000,"North West Property Richard Frost

A typical Regus workspace

Workspace provider Regus has opened two new business centres in Liverpool, meaning that it now has three locations in the city.

The centres are located on Merchants Court on Derby Square and 1 Mann Island.

Regus also has a site in Exchange Flags.

Richard Morris, UK chief executive of Regus, said: ""Demand for flexible workspace in Liverpool is booming so the city was a natural choice for our expansion plans.

""The city is well-connected and offers excellent value and it's increasingly attracting investment and visitors from across the world.

""We expect our new centres to be popular with a wide range of users including local small businesses, start-ups and remote workers as well as national firms opening satellite offices and global businesses establishing a footprint in the area."""
66,Policy setback hits legal cannabis market in US,"US Attorney General Jeff Sessions has announced he is dismantling recent policies that help states legalise recreational cannabis, causing pot stocks to plunge, just as California began cannabis sales on 1 January. The legal cannabis market in the US had been predicted to be worth $40bn and to create over 400,000 jobs by 2021. That figure was made up of $20bn in direct sales and the same amount in indirect revenues for growers and others. Last year the market was worth $16bn.",https://phys.org/news/2018-01-legal-cannabis-40bln-jobs.html,2018-01-03 10:19:15.183000,"Credit: CC0 Public Domain

The legal cannabis sector is expected to generate $40 billion and more than 400,000 jobs by 2021 in the United States, according to a study released Tuesday.

The estimate by consulting firm Arcview includes direct purchases by consumers of $20.8 billion and indirect revenue for growers and various subcontractors as well as money spent with businesses not affiliated with the sector, such as supermarkets.

The projection would represent a rise of 150 percent on the $16 billion revenue recorded in 2017, according to the study, released the day after recreational use of marijuana became legal in California.

Arcview and its partner in the research, BDS Analytics, expect $4 billion in taxes to be generated within three years.

The new regime will lead to the creation of nearly 100,000 cannabis industry jobs in California by 2021, about a third of the nationwide figure and 146,000 jobs overall when indirect effects are considered.

Customers and operators in California have complained however about the punitive sales taxes to be applied to cannabis and its derivative products, which can hit 35 percent when state, county and municipal levies are taken into account.

California, the most populous US state, became the largest legal market for marijuana in the world on Monday, and public reaction to the law change has been enthusiastic, with long lines and stock shortages reported at clinics already licensed and open.

Berkeley mayor Jesse Arreguin hailed the reforms at a ceremony on Monday at Berkeley Patients Group, one of the oldest dispensaries in the United States.

""I'm stoked about this historic moment, not just for Berkeley, but for the state of California,"" Arreguin said, praising the state for ""embracing this new economy.""

Cannabis possession remains illegal under federal law, and Arcview's Tom Adams said fewer than 100 out of the 3,000 outlets and delivery services operating in California were ready to go with the required local and state permits.

""Those that were generally report doing multiples of their typical day's business with a far more diverse and less experienced customer base that need a lot of hand-holding and educating from their bud-tenders,"" he added.

""We were very cautious in projecting revenue growth from $3 billion to $3.7 billion in this first year of adult-use legality in California, but we'll have to revise that upwards if, as now appears likely, San Francisco and Los Angeles are going to get permits issued more quickly than we expected.""

Explore further California issues first licenses for legal pot market

© 2018 AFP"
67,Redrow Second phase of Sandbach development to go before planning committee,"The second phase of a development by UK housebuilder Redrow Homes and Anwyl in Sandbach, Cheshire is set to be examined by Cheshire East’s Southern planning committee. The latest reserved matters application, covering 126 homes, has been recommended for approval by Cheshire East planning officers, but Sandbach town council has complained about the lack of green space and inadequate provision for downsizers.
",https://www.placenorthwest.co.uk/news/planning-sandbach-homes-on-cheshire-east-agenda/,2018-01-03 10:06:58.647000,"The second phase of a development by Anwyl and Redrow Homes, providing 151 homes off Middlewich Road in Sandbach, is set to be discussed by Cheshire East’s Southern planning committee next week.

Anwyl and Redrow have made two applications on the site: a reserved matters application for 126 homes, and a full planning application for 25 houses at the site’s southern end.

The wider 39-acre site was granted outline planning permission for up to 280 homes, alongside public open space and highways improvements, in 2012, and a reserved matters planning application for the first phase of 154 houses was approved in 2015.

The latest reserved matters application includes a mix of 74 four-bedroom homes; 26 three-beds; 21 two-beds; four one-beds; and one five-bed house. The four-bed units are expected to vary in price between £264,000 and £475,000.

Recommending the application for approval, Cheshire East planning officers said the proposals would “much needed affordable housing provision” and “would help in the Council’s delivery of five-year housing land supply”.

Cheshire East planning officers have stipulated that 30% of the homes – around 38 units – should be provided as affordable homes under the application’s Section 106 agreement.

A contribution of £514,000 towards education services was already secured as part of the outline planning permission, secured in 2012.

The recommendation to approve has been put forward despite objections from Sandbach Town Council, which argued the housing was “far too dense” in the second phase, and that it offered “no green space of any significance”.

The Town Council also criticised the application for not providing any bungalows “for older residents who wish to downsize”.

However, planning officers said the development’s open space was already covered under the outline application, which provides a six-acre park on the site.

Cheshire East planners also recommended the full planning application, covering 25 homes, for approval.

The homes on designated open countryside land are in addition to the 280 houses approved as part of 2012’s outline planning application.

These will provide a mix of 17 four-bed homes; four three-beds; three two-beds; and a single five-bedroom house, with prices for a four-bed house expected to be in a similar range as for the wider development.

Planning officers said the additional homes would “not have a detrimental impact upon residential amenity” in the area, and recommended the scheme for approval, subject to agreeing affordable homes on the site, as well as a £120,000 provision towards local education provision.

The professional team for the development includes Astle Planning & Design."
68,Regus Regus opens doors to second Redhill business centre,"Flexible office space provider Regus has opened its second centre in Redhill, Surrey. The company has leased the fifth, sixth and seventh floors of Kingsgate House, offering co-working space, meeting rooms, virtual office services and more than 180 workstations and hot desks. UK CEO Richard Morris said Rehill was a popular location for insurers, banks, oil companies and publishers.
",http://www.southeastbusiness.com/section/news/business-centre-opens,2018-01-03 10:05:08.320000,"Regus’ second Redhill centre opens for business.

Global workspace provider Regus has opened the doors to its second Redhill business centre in response to the increased demand for workspace on flexible terms from local professionals.

The centre occupies the fifth, sixth and seventh floors of Kingsgate House, located at the southern end of Redhill High Street. It houses over 180 workstations and offers a range of flexible working options including co-working space, offices of varying sizes and layouts, Virtual Office services, meeting rooms and hot desks.

Regus Redhill’s central location places it close to various local amenities and transport links. It is situated just five minutes’ walk from Redhill Station, which has direct rail links to London, Croydon, Guildford and Reading. The centre can also be accessed internationally via Gatwick Airport which is just a 20 minute drive away.

Richard Morris, UK CEO, Regus, said: “Redhill is already a popular location with insurers, banks, oil companies and publishers. Combined with its excellent transport links, this makes the town a great location for Regus. We expect the new centre to be popular with a wide range of users including those based in Redhill and those visiting for business”

Please enable JavaScript to view the comments."
69,Renewables dominates Britain's 2017 electricity production,"The UK had its greenest ever year in electricity production in 2017, breaking 13 different renewable energy records. Figures from BM Reports and Sheffield University showed that renewables produced more electricity than coal power stations on 315 days last year, and April saw the first day with no coal-fired power used in the UK. Coal now supplies less than 7% of the UK's electricity, and the government has a target to phase it out by 2025. The UK has halved carbon emissions in electricity production since 2012, and the increase in renewable power is expected to continue in 2018.",https://www.wwf.org.uk/updates/britain-set-greenest-year-ever-clean-energy-breaks-all-records-2017,2018-01-03 09:38:01.767000,"Since 2012, Britain has halved carbon emissions in the electricity sector making the power system the 4th cleanest in Europe and the 7th cleanest in the world. Meanwhile public support for renewable electricity production has hit record highs, with 82% of the UK public supporting green energy.

However, a lot more needs to be done to reduce our carbon emissions and tackle climate change. The UK is behind schedule to meet the 4th or 5th Carbon Budgets and there are no clear plans on how we are going to make up this shortfall.

2018 is the year of opportunity for clean energy, and is set to be even greener, but it must be backed up with Government action. Greater support needs to be given to renewable energy, to decarbonise our heat and make our buildings use less energy. On top of this greater ambition is needed to support electric vehicles by ending the sale of petrol and diesel cars by 2030. This will cut our carbon emissions, clean up our air and bolster the UK economy."
70,Renewables dominates Britain's 2017 electricity production,"The UK had its greenest ever year in electricity production in 2017, breaking 13 different renewable energy records. Figures from BM Reports and Sheffield University showed that renewables produced more electricity than coal power stations on 315 days last year, and April saw the first day with no coal-fired power used in the UK. Coal now supplies less than 7% of the UK's electricity, and the government has a target to phase it out by 2025. The UK has halved carbon emissions in electricity production since 2012, and the increase in renewable power is expected to continue in 2018.",https://www.theguardian.com/environment/2017/dec/28/renewables-power-coal-2017-uk-figures,2018-01-03 09:38:01.767000,"British wind farms generated more electricity than coal plants on more than 75% of days this year, an analysis of energy figures has shown.

Solar also outperformed coal more than half the time, the data provided by website MyGridGB revealed.

Overall, renewables provided more power than coal plants on 315 days in 2017, figures up to 12 December showed. Wind beat coal on 263 days, and solar outperformed the fossil fuel on 180 days.

Between April and August inclusive, coal generation exceeded solar on only 10 days.

In total, renewables generated more than three times the amount of electricity as coal over the year to 12 December.

The figures – provided by BM Reports and Sheffield University – reflect a year in which a number of green records have been set for the power sector, including the first full day without any coal power in the system, record solar generation and tumbling prices for new offshore wind farms.

The government has committed to phasing out coal power that does not have technology to capture and permanently store its carbon emissions by 2025, as part of efforts to meet targets on greenhouse gases.

The focus now turns to gas, with daily output from wind outstripping gas on only two days of the year, and renewables overall – including wind, solar, biomass and hydropower – beating the fossil fuel on just 23 days.

Dr Andrew Crossland from MyGridGB and the Durham Energy Institute said: “The government has focused on reducing coal use which now supplies less than 7% of our electricity. However, if we continue to use gas at the rate that we do, then Britain will miss carbon targets and be dangerously exposed to supply and price risks in the international gas markets.

“Clearly, refreshed government support for low-carbon alternatives is now needed to avoid price and supply shocks for our heat and electricity supplies.”

Emma Pinchbeck, executive director at industry body RenewableUK, said the decision to phase out coal was being made possible by a homegrown renewables industry “coming into its own”.

She added: “We want to see more boldness from the Conservative government. In 2018, the government should move to allow onshore wind, now the cheapest form of power for consumers, to be developed in parts of the UK where it is wanted, and agree an ambitious sector deal with the offshore wind industry.

“The new year could be the first in a golden age for UK renewables.”"
71,FarmWise raises $5.7m to commercialise AI-driven weeding robot,"California-based agriculture technology start-up FarmWise has raised $5.7m to bring its weeding robot to the market. The technology uses computer vision and artificial intelligence (AI) to identify and remove weeds from vegetable farms and gather data about crops. Users simply set the boundaries for the machine, and it gets on with its work. The funding comes from venture capital companies Playground Global, Felicis Ventures, Basis Set Ventures and Valley Oak Investments. The robot can be used throughout the year and can make organic food more affordable by reducing the use of pesticides, the company said.",https://agfundernews.com/farmwise-raises-seed-round-weeding-robot.html?utm_source=AgFunder&#43;Updates&amp;utm_campaign=fd6e32ef4c-Dec21_2017&amp;utm_medium=email&amp;utm_term=0_7b0bb00edf-fd6e32ef4c-98391793,2018-01-03 08:33:55.043000,"FarmWise, an agricultural robotics and IoT startup has raised a $5.7 million seed round to commercialize the company’s automated weeding robot.

FarmWise has developed a weeding robot that uses computer vision to identify weeds and robotics to remove them from vegetable farms without herbicides. While removing weeds with a mechanical motion similar to a garden hoe, the robot also gathers data about the plants.

“The machine can drive itself through the field and use cameras and computer vision to understand the field. It can analyze each plant, gathering info such as size, health status, and growth stage,” FarmWise founder and CEO Sebastien Boyer told AgFunderNews.

The round was led by hardware-focused VC Playground Global with Felicis Ventures, Basis Set Ventures, and food, agriculture and health investor Valley Oak Investments also participating.

“Farmwise is using the latest advances in computer vision and deep learning to build an autonomous platform that not only gathers crop-level data but also carries out actual tasks such as precision weeding and thinning, which are crucial to sustainable organic farming. The confluence of data-sensing, machine intelligence, and robotic actuation being smartly applied to an agricultural setting by a fantastic pair of entrepreneurs is what got us excited about partnering with them,” said Mario Malave of Playground Ventures.

Boyer said that he and his cofounder and CTO Thomas Palomares became interested in weeding because it is at the intersection of the farm labor crisis and moving away from chemical applications in agriculture.

Palomares has a family history of farming in France and he and Boyer have been working with growers in California for a year to hone their user interface.

‘We want to have a user interface as simple as a washing machine. On the inside, we want the machine to be as smart as possible. On the outside, it should be as simple as possible,” said Boyer.

The only element that needs to be entered by the user is a GPS “geofence” so that the machine does not go beyond the field it is supposed to be weeding. The rest of the robot’s activities are self-directed.

“One of the things that got us excited was that this is something that farms need to do throughout the year multiple times. so there’s always a use case as opposed to a picking robot,” Niki Pezeshki, vice president at Felicis Ventures.

Pezeshki said that finding a pesticide-free and cheaper method of weeding could be a game-changer for organic growers and consumers.

“Taking out herbicides really resonated with us too. If you can crack that nut in a scalable way that won’t make organic food more expensive, even better,” said Pezeshki.

Boyer is confident that the economics will work in growers’ favor sooner rather than later in part due to targetting vegetable farmers, who generally benefit from higher profit margins and suffer from higher labor costs — setting FarmWise further apart from well-known automated “see and spray” weeding robotics startup Blue River Technologies, which was acquired by John Deere earlier this year.

“The cost of weeding for vegetables on a per acre basis is between two and five times higher than for commodities. That makes our solution more economical at the beginning,” said Boyer, who anticipates selling the robots as a service, at least initially.

FarmWise investor Valley Oak Investments is also invested in Farmers Business Network, biological crop input company Marrone Bio Innovations, food delivery startup Caviar, dairy management software platform Farmeron, ag biotech company Hazel Technologies, and food waste reduction software Spoiler Alert.

Bruce Leak, inventor of QuickTime and cofounder of WebTV has joined FarmWises’s board of directors.

Farmwise is currently signing pilot contracts with some of the largest vegetable growers in CA beginning in summer 2018, according to Boyer.

Photo: FarmWise"
72,China estimates space lab Tiangong-1 will crash to Earth in March,"China's space lab Tiangong-1 is estimated to crash to Earth in March, with the Aerospace Corporation predicting it will re-enter between the latitudes of 43°N and 43°S. Although the area is largely covered by ocean, the US, Brazil and China also traverse it. China said it will release updated forecasts of the craft's descent; the expectation is that some debris will hit Earth, while the rest will burn up on re-entry. As we previously noted, many believe China's space authority lost control of the 8.5-tonne lab and are allowing it to enter Earth's atmosphere ""naturally"".
",https://qz.com/1169100/chinas-first-space-lab-is-falling-to-earth-will-probably-miss-you/?mc_cid=1bfc2b3b77&mc_eid=a37072368a,2018-01-03 08:33:06.673000,"China’s first space lab will crash down to earth in the coming months, but don’t worry: The odds of any debris hitting a person are astronomically small.

The Aerospace Corporation, a California nonprofit, estimates the Tiangong-1 will enter the atmosphere in mid-March. Sent up in 2011, the “Heavenly Palace,” as it’s also known, has witnessed a number of milestones as China races to become a space superpower. For example in 2012 the country sent its first female astronaut, Liu Yang, as part of the team behind the first successful manual docking with the lab.

The station was designed with a two-year lifespan, but authorities extended its service life by two and a half years to conduct more experiments.

In September 2016, China’s Manned Space Engineering Office announced that the Tiangong-1 would re-enter the atmosphere around the latter half of 2017, which many interpreted to mean the lab had fallen into an uncontrolled orbit. (Satellite trackers suggest the lab has been that way since at least June 2016, notes the Aerospace Corporation.)

The agency said most of the lab would burn up in the fall, adding that it would release its updated forecasts of the descent, internationally if necessary.

Although it’s hard to predict where surviving pieces might land, based on Tiangong-1’s inclination, the Aerospace Corporation estimates the station will re-enter somewhere between the latitudes of 43° N and 43° S, an area largely covered by ocean, but also traversing countries including the US, Brazil, and China itself. According to the latest available trajectory data (link in Chinese), the Tiangong-1 is orbiting at an average height of 287 km (178 miles), about 100 km lower than it was in September 2016.

Though it’s not uncommon for spacecraft and satellites to re-enter the atmosphere, rarely does it lead to injury or destruction of property. The largest manmade object to re-enter was Russia’s Mir space station, with surviving fragments falling into the Pacific east of New Zealand in 2001. Whereas the Tiangong-1 weighs 8,500 kg (18,739 lbs), the Mir weighed 120,000 kg.

The Mir, though, was still under control when it entered the atmosphere. Letting objects enter uncontrolled is not considered a best practice.

One (still remote) danger from the Tiangong-1, according to the Aerospace Corporation, is that someone will find and pick up a piece of its debris that’s covered in a corrosive substance.

While the odds of getting hit by space debris are absurdly remote, it happened to at least one woman. In 1997 Lottie Williams was strolling through a park in Tulsa, Oklahoma when a piece of light metal measuring about 6 inches (15.2 cm) glanced off her shoulder. NASA later confirmed the timing and location were consistent with the re-entry and breakup of a second-stage Delta rocket, the main wreckage of which was found a few hundred miles away in Texas. Williams wasn’t injured, but she’s thought to be the only person ever hit by space debris.

You won’t likely become the second.

Correction: An earlier version of this story stated the Mir space station re-entered the atmosphere in 2013 instead of 2001."
73,UK group that backs diversity in VC firms to launch in US,"Diversity VC, a UK initiative established last year by five young venture capitalists to tackle the lack of diversity within their industry, is set to launch in the US. The group released a report in May that revealed almost 50% of UK venture capital firms had no women working on their investment teams. Diversity VC in the US will get financial backing from law firm Cooley and support from Insight Venture Partners, Entrepreneur First and Female Founders Fund. Diversity VC said the US launch was in response to requests from workers in the US industry.
",http://uk.businessinsider.com/diversity-vc-is-expanding-to-the-us-to-challenge-venture-capitals-monoculture-2018-1?utm_source=feedburner&utm_medium=referral&r=US&IR=T,2018-01-03 00:00:00,"A British campaign group set up by young venture capitalists to boost diverse representation in VC is expanding to the US.

Diversity VC will be backed in the US by partners and associates at Female Founders Fund, Entrepreneur First, and financially by law firm Cooley.

2017 was a shocking year for US venture capital, with high-profile figures such as Uber investor Shervin Pishevar, 500startups founder Dave McClure, Binary Capital cofounder Justin Caldbeck and many others accused of sexual harassment.

A British initiative set up to tackle venture capital's monoculture will launch in the US, where the industry is currently reeling from multiple sexual harassment scandals.

Diversity VC was set up by five young venture capitalists in March to highlight the fact that most British venture capital investors are middle-class white men, with a knock-on effect on which startups get funding. It released a landmark report in May showing that almost half of British VC firms had no women on their investment teams.

Diversity VC founder and investor Check Warner told Business Insider that the group had decided to expand to the US after multiple requests from venture capital associates.

""What's been lacking in conversations in the US is that basis in fact, [research into] why we have such a big problem,"" Warner said. ""There wasn't anything comparable to what we had done.

""People got in touch to say 'It would be great to have something like this, a system and process to start uncovering this.'""

Warner and her cofounders set up Diversity VC only a few months before a wave of harassment scandals hit US venture firms. Six women accused Binary Capital cofounder Justin Caldbeck of inappropriate behaviour, resulting in his leave of absence from the firm. Subsequent scandals engulfed Uber investor Shervin Pishevar, 500 Startups founder Dave McClure, and Tesla investor Steve Jurvetson.

""It's been interesting to see these things come out,"" said Warner. ""It's reflective of the culture not including people, and having this sense of entitlement. You don't have checks and balances on behaviour.

""The lack of female diversity around the table is a contributing factor to people getting away with what they got away with.""

Like its UK counterpart, the US chapter of Diversity VC will promote greater diversity in venture capital through four initiatives, including helping underrepresented groups build a VC network, getting minority interns into VC firms, and publishing data about diverse representation.

The group has backing from Female Founders Fund partner Sutian Dong, Insight Venture Partners associate Juliet Bailin, and Entrepreneur First's US head of funding Matt Wichrowski. Law firm Cooley is giving financial backing."
74,Savills calls the bottom of prime housing slump,"British property company Savills has predicted that the slump in London's prime housing market has slowed, but says a return to growth will be slow. Values of the most expensive homes in Westminster have fallen by 15% over the last three years, while in areas such as Wandsworth and Clapham, the decrease has been 7.3%. Savills predict it will be two years before prices start to rise again, due to uncertainty over Brexit, added to the previous effect of a stamp duty increase on properties over £1m ($1.4m).",http://www.telegraph.co.uk/property/house-prices/bad-news-barnes-house-prices-south-west-london-slump-centre/?utm_source=dlvr.it&utm_medium=twitter,2018-01-02 17:52:15.583000,"Price falls that have hit London's most expensive property for years have slowed and rippled out to high-end family homes in the south-west of the capital, according to Savills.

The value of central London's 'prime' property, located in Westminster and Kensington & Chelsea, has been falling for three years, down 15pc since September 2014. Savills said that fall, sparked largely by a hike in stamp duty on homes worth more than £1m, appears to have bottomed out.

By contrast, increasingly fragile buyer sentiment due to uncertainty over Brexit and high property prices mean that these falls are accelerating in outer zones.

While prices of expensive property in the centre of the capital fell by 4pc last year, they sank by an average of 4.2pc in south-west London, in areas such as Barnes, Wandsworth and Clapham. This is the first time since June 2012 that such price falls in south-west London have outpaced those in the centre."
75,Amazon reportedly close to investment in Indian insurtech Acko,"Amazon is reportedly finalising an investment in Indian insurtech firm Acko. The arrangement would see Amazon acting as a distribution platform for the online-only insurer. Amazon's biggest rival in India, Flipkart, was also reportedly considering an investment in Acko. The start-up has raised $30m to date and has provisional approval to operate from India's financial markets regulator. 
",https://www.reinsurancene.ws/amazon-reported-verge-insurtech-investment-india/,2018-01-02 17:33:07.047000,"Online retail and e-commerce giant Amazon is reportedly on the verge of making its first investment in an insurtech start-up, with the company said close to finalising an investment in online-only insurance start-up Acko.

Acko wants to disrupt India’s insurance industry through a digital-only platform, having raised $30 million and recently received in-principal approval from the financial market regulators in India.

Amazon and Indian rival Flipkart had both been pursuing investing in Acko, it has been reported widely, but at this stage it is now thought that Amazon is close to signing a term-sheet for the investment and a partnership deal with Acko.

It’s said that the arrangement will see Amazon acting as an online distributor for Acko’s insurance products, selling a range of financial products.

The potential for Amazon to enter the insurance space has been much-discussed in recent months, including in our article from November, Incumbents could be relegated, if tech giants come for re/insurance.

Now it appears Amazon is close to taking a sensible step of investing in and partnering with an insurtech start-up, in order to gain the ability to add insurance products to its retail offering, seeing the firm stepping into the sale of financial products for the first time.

Targeting India first is also a smart move, as the burgeoning financial services market there has a strong focus on technology and take-up rates of insurance products are rising all the time.

If Amazon can crack selling insurance online to the Indian market, it will stand it in good stead to break into more established markets such as the United States and Europe.

Of course, if Amazon does move into insurance meaningfully it will likely only be a matter of time before other tech giants such as Google follow suit with their own integrated e-commerce offerings.

It’s also been reported that Flipkart is readying its own entry into insurance sales online, with the establishment of a new entity to focus on financial services and venture investing.

Also read: Incumbents could be relegated, if tech giants come for re/insurance."
76,Nuritas raises $16m for its AI-powered solutions to diabetes,"A Dublin-based biotech firm has raised €16m ($19.2m) in a series A funding round to support its work in discovering and making use of bioactive peptides to assist in the treatment of diseases. Nuritas, whose CEO is former Nestlé executive Emmet Browne, has now received a total of €25m since its launch in 2014, including $3m from the European Union for a project that explores using artificial intelligence (AI) to help to prevent diabetes. Bioactive peptides play a role in managing the condition, and the AI technology used by Nuritas speeds up the process of identifying those which may help. 
",https://www.foodnavigator.com/Article/2017/12/21/AI-investment-Nuritas-to-step-up-development-after-16m-Series-A-funding,2018-01-02 16:39:47.917000,"The latest round of funding, led by Chicago-based Cultivian Sandbox Ventures, takes the total invested in Nuritas to date to around €25 million since the firms’ launch in 2014.

Emmet Browne, CEO of Nuritas told NutraIngredients the new funding will allow it scale the business and ‘grasp the huge opportunity’ that its unique platform has created.

“Predominantly we will use the funds to triple our workforce, progress our research and development and improve our rate of prediction,” said Browne – adding that it will allow the company to create the capacity “to deal with the huge levels of customer demand that is building up for what we do.”​

Nuritas, founder and chief scientific officer Dr Nora Khaldi added that the investment will not only help it accelerate routes to market, explore new disease areas and grow its team, “but it will also push us even further in extracting the great potential of what our technology is capable of creating.”​

Nick Rosa, managing director of Cultivian Sandbox and Co-founder of Sandbox Industries, which led the latest round of investment, said the company’s unique platform delivers ‘truly life-changing health benefits’

“We are very pleased to be involved in its growth – it’s a brilliant team and such an exciting technology,”​ he said. “We expect Nuritas to quickly emerge as one of the most innovative companies in the world, effecting real change.”​

‘Huge opportunity’​

Nuritas’ platform combines DNA analysis and artificial intelligence (AI) to predict, unlock, and validate peptides from natural sources. The Irish firm have grown rapidly and gained much attention in recent years, with a list of awards and big-name backers – with initial seed investments from Singapore-based New Protein Capital (NPC) and Silicon Valley investor Ali Partovi​ and further funding coming from Irish rock legends Bono and The Edge in December 2016​.

Headed-up by ex-Nestlé regional president Emmet Browne​, Nuritas was recently given a €3 million EU grant​ to continue working on an unnamed peptide said to be 'a major breakthrough' for diabetes prevention, and has entered into collaboration with international ingredients giant BASF​ for the discovery and commercialisation of functional peptides.

“In an effort to expand our health solutions, we searched the globe trying to find such an innovative discovery technology and we eventually found it with Nuritas,”​ commented Michael De Marco, Global Head, Research & Development Human Nutrition and Pharma Solutions, BASF SE.

Looking to 2018 … and beyond​

The partnership between Nuritas and BASF will see its first commercial launch in 2018 as a new anti-inflammatory ingredient is launched into the US market. Nuritas CEO Browne said the launch of the anti-inflammatory ingredient will be ‘pivotal’ for the firm in the next year.

“What is so exciting is that the inflammation ingredient launching in the U.S. next year is actually the first healthcare ingredient that has been fully discovered through the use of Artificial intelligence,” ​added Khaldi.

Meanwhile, BASF’s De Marco said the launch of the ingredient into the US market ‘is only the beginning’.

“Our collaboration is progressing and on track to yield more groundbreaking products in the future.” ​

Another key area of focus for Nuritas is diabetes. According to the International Diabetes Federation, an estimated 352 million individuals globally are living with pre-diabetes which is considered an early warning sign for the condition.

“Bioactive peptides are known to play a role in managing diabetes and many other areas, but the current methods of identifying those that may work is time-consuming, inefficient and expensive,” ​said Browne – noting that when compared to traditional discovery methods, the Nuritas platform has been shown to identify peptides ten times faster and 500 times more accurately while significantly reducing costs.

“Our artificial intelligence platform has already disrupted this antiquated process by targeting, predicting and unlocking peptides that can positively impact in conditions like pre-diabetes while reducing the cost and time needed to find them,”​ he said.

Commenting on the possibility of further investment and financing rounds, the CEO told NutraIngredients that Nuritas will continue to grow and expand, but that the ‘huge’ round of investment that has just closed will provide a good runway to operate from.

“We are now focussed firmly on using it to maximise the many opportunities before us,”​ he said.

However, Browne added that the company is also “fortunate that we already have a number of different options identified when it comes to bringing in new funding if and when the need may arise.”​"
77,Japanese hospital to deploy night-shift delivery robots,"A hospital in Japan is deploying robots to deliver supplies around the building. The Nagoya University Hospital is to use four of the devices to run during the night shift from 5pm to 8am, when there are fewer people using the corridors. The robots have been developed in partnership with Toyota industries, and use radar and cameras to guide themselves around the hospital. They contain mobile refrigerators to carry medical supplies and staff can use a tablet device to summon them and set their destination.",https://www.engadget.com/2018/01/02/a-hospital-in-japan-will-use-robots-to-help-out-the-night-shift/,2018-01-02 16:31:04.417000,"In Nagoya, Japan, a city that once held an entire museum dedicated to robotics, a hospital will soon add robots developed by Toyota to its medical staff. No, they won't be scrubbing in for surgery: In February, the Nagoya University Hospital will deploy four bots to ferry medicine and test samples between floors for a year.

Nagoya hospital to use robots for deliveries of drugs, materials：The Asahi Shimbun：The Asahi Shimbun https://t.co/bhlkzM0Rnp — Asahi Shimbun AJW (@AJWasahi) January 1, 2018

The robots are essentially mobile refrigerators with a 90-liter capacity that rely on radar and cameras to zoom through the hospital. Should they run into humans, they're programmed to dodge them or politely voice 'Excuse me, please let me pass,' according to The Asahi Shimbun. Staff can summon the robots and assign a destination for their medical payload using a tablet.

Nagoya built the robot system in partnership with Toyota Industries, a subsidiary of the automaker that produces auto parts and electronics. The trial run will run the robots between 5pm and 8am during the night shift when fewer people are walking the floors. Should the trial go well, the facility may choose to deploy more units."
78,NASA considers allowing staff on commercial suborbital flights,"NASA is looking at allowing researchers from the agency on board commercial suborbital flights, according to Steve Jurczyk, NASA associate administrator for space technology. Blue Origin's New Shepard craft and Virgin Galactic’s SpaceShipTwo are among the vehicles that could carry researchers on suborbital flights. Liability issues on the relatively dangerous missions have so far deterred NASA from allowing staff aboard them.
",https://www.space.com/39247-nasa-researchers-commercial-suborbital-vehicles.html,2018-01-02 16:27:42.023000,"NASA's Flight Opportunities program is already flying experiments on Blue Origin's New Shepard vehicle, but researchers and companies alike want NASA to also fund experiments with people on board.

BROOMFIELD, Colo. — As commercial suborbital vehicles capable of carrying both payloads and people prepare to enter service, NASA officials say they're willing to consider allowing agency-funded researchers to fly on those vehicles.

In an interview after a speech at the Next-Generation Suborbital Researchers Conference here Dec. 19, Steve Jurczyk, NASA associate administrator for space technology, said the agency would be open to allowing researchers funded by NASA's Flight Opportunities program to fly on suborbital spacecraft to carry out their experiments.

""As principal investigators propose, both internal to NASA and external, we'll do the same kind of process that we do with Zero G,"" he said, referring to the company that performs parabolic aircraft flights. Zero G flies investigations as part of the Flight Opportunities program, with researchers flying on the aircraft with their experiments. [Watch Blue Origin's New Shepard 2.0 Spacecraft Soar in 1st Test Flight]

Zero G's aircraft, a Boeing 727, is regulated by the Federal Aviation Administration. Jurczyk said that, in addition to the FAA oversight, NASA's Armstrong Flight Research Center performs an evaluation of the aircraft for investigations selected by the Flight Opportunities program for flights on it. ""It just ensures that our grantees and contractors are safe to fly, and then we allow them to go fly,"" he said in a speech at the conference.

A similar procedure is not yet in place for suborbital vehicles, but Jurczyk said the agency would be open to finding some process analogous to that used for Zero G. ""Moving forward, as these capabilities start coming online, we’ll figure it out,"" he said in the interview.

His comments come four and a half years after another agency official opened the door to flying people on commercial suborbital vehicles through the Flight Opportunities program. Speaking at the same conference in June 2013, Lori Garver, NASA deputy administrator at the time, said that past prohibitions about flying people would be lifted.

""We absolutely do not want to rule out paying for research that could be done by an individual spaceflight participant — a researcher or payload specialist — on these vehicles in the future,"" Garver said then. ""That could open up a lot more opportunities.""

That announcement took the program by surprise, with the program's managers saying at the time they had yet to craft a policy for allowing people to fly with their experiments. Development of such a policy suffered years of delays, in part because of Garver's departure from NASA just a few months after her announcement as well as extended delays in the development of commercial suborbital vehicles capable of carrying people.

""It mostly resulted in a bunch of ostriches sticking heads in the sand for a few years,"" said Erika Wagner, business development manager at Blue Origin, during a panel discussion at the conference Dec. 18.

Blue Origin's New Shepard vehicle is already carrying research payloads, including for Flight Opportunities, but without people on board. However, the vehicle will be able to support missions carrying payloads and people in the future. Virgin Galactic’s SpaceShipTwo vehicle will also fly research payloads accompanied by a payload specialist.

Wagner said she has seen some progress as both companies' vehicles advance through flight testing. ""The heads are back out. They're looking around trying to understand what really are the barriers, what is the liability regime.""

Those liability issues today, she said, prevent NASA civil servants from flying on the Zero G aircraft, even though outside researchers whose experiments are funded by NASA are able to do so. Jurczyk, in his speech at the conference, said that’s because they would have to sign a liability waiver to do so. ""Right now, that’s just NASA policy. We don't have a strong mission need to do that,"" he said. ""That's current policy. I’m not saying it's going to be policy forever and ever."" [In Photos: Blue Origin's New Shepard 2.0 Aces Maiden Test Flight]

Scientists who would like to fly experiments on suborbital vehicles argue that such missions are analogous to fieldwork — oftentimes hazardous — performed in other fields. ""Marine biologists and marine geologists get to put themselves in that very same operationally risky environment by going to the bottom of the ocean, to a deep sea vent,"" said Dan Durda, a planetary scientist at the Southwest Research Institute, during the Dec. 18 panel. ""These vehicles offer us, as space scientists, that opportunity to get into the field the way that biologists and geologists do.""

Advocates of commercial suborbital research, such as the Commercial Spaceflight Federation’s Suborbital Applications Research Group, have been pushing to allow NASA to fund human-tended experiments.

""They're working quietly to get the word out that there are very definite needs for human-tended payloads,"" said Steven Collicott, a Purdue University professor, in a conference speech Dec. 19. ""We've heard some encouraging words and we’re working quietly to try and move that ahead.""

Others at the conference noted a decades-old precedent that suggests existing barriers to flying NASA-funded researchers on commercial suborbital vehicles can be overcome. In the 1980s, several payload specialists flew on the space shuttle, including Charles Walker, a McDonnell Douglas engineer who was part of three shuttle missions.

Walker, in the Dec. 18 panel discussion, noted that on those shuttle missions he and his family signed liability waivers. He supported similar approaches to allow researchers to fly on commercial suborbital vehicles.

""The environments opened up by suborbital flight and, at a greater scale, orbital flight, are laboratory environments,"" he said. ""You should be there to maximize the answers that are coming out of the conduct in that environment.""

This story was provided by SpaceNews, dedicated to covering all aspects of the space industry."
79,US and China set to launch Mars mission for sample retrieval,"NASA and China's national space agency are both aiming to retrieve samples from Mars for study on Earth by 2030. NASA's preliminary mission starts with the launch of its Mars 2020 rover, set to land in February 2021. The rover will core a sample, place it in a tube and leave it on the surface for collection by a later mission. Meanwhile, China's National Space Science Centre will scout the planet with its first spacecraft to orbit Mars. It will then use the data gathered from orbit to select a site for a follow-up sample return mission.
",http://www.popularmechanics.com/space/moon-mars/a14506608/united-states-china-racing-first-sample-from-mars/,2018-01-02 16:20:06.470000,"The best way to study martian rocks and soil would be to do it on Earth. While spacecraft-mounted instruments—such as the Curiosity rover's ChemCam that vaporizes surface material with a laser and then uses a spectrometer to determine the chemical composition—are invaluable to planetary scientists, they are no replacement for a sample in the lab. The amount of compositional and absolute age data that scientists can obtain with a laboratory full of state-of-the-art equipment and chemicals to test sample materials is unparalleled, as evidenced by research conducted on meteorites (including from Mars) and Apollo moon samples.



To continue this work, scientists need a pristine sample of martian rock and soil, which would help build a Rosetta Stone to unlock the history of the solar system. The potential knowledge to be obtained from such a sample ranges from the formation of Mars to the nature of the planet's ancient surface waters to possible habitability in the red planet's past, and in turn, perhaps the secret to the origin of life on Earth.

With so much to gain, both NASA and the Chinese national space agency are designing missions to retrieve a sample from Mars before the end of the 2020s. The missions are ambitious, incomplete, and reliant on yet-to-be developed technologies. They both start, however, with flights to Mars in 2020.

Scouting the Red Planet

Self-portrait of NASA’s Curiosity rover, taken at Namib Dune on January 19, 2016. NASA/JPL-Caltech/MSSS

Both American and Chinese Mars sample-return ambitions require preliminary missions to scout the planet and locate the best place to drill into the rock and core out a sample. NASA plans to do this with the Mars 2020 rover, slated to launch on an Atlas V rocket in July of that year and land on Mars in February 2021. The Chinese National Space Science Center (NSSC), meanwhile, is planning an orbiter, as well as a lander that will deploy a small rover, known as the Mars Global Remote Sensing Orbiter and Small Rover, to launch on a Long March 5 rocket in July or August of 2020.



Both start with flights to Mars in 2020.

NASA's Mars 2020, a car-sized twin to the Curiosity rover with an upgraded instrument suite, will not only locate the best places to collect a sample, but will also core the sample into a receptacle tube and deposit it on the surface to be picked up by a later mission. The Mars 2020 team is still considering three possible landing sites: Columbia Hills in Gusev Crater, a mineral-rich region near the Spirit rover's old exploration site; Jezero Crater, where surface water is thought to have flooded and receded at least twice in the past; and NE Syrtis, an area once characterized by volcanic activity and hot spring flows.

The Mars 2020 team will continue to analyze surface composition data from orbiters such as the Mars Reconnaissance Orbiter (MRO) to ultimately select one of these sites based on mineralogy and the potential for habitability. Once Mars 2020 arrives, however, where to sample will be determined on the fly.

""We'll be exploration driven once we get on the surface, so we'll have to make decisions,"" Katie Stack Morgan, deputy project scientist for Mars 2020, told Popular Mechanics during a visit to NASA's Jet Propulsion Laboratory in August. ""Okay, here's a rock outcrop, where would be the best place to sample? Or should we pass on this one and go to the next one?""



""That will be something we'll figure out on the surface.""

Once the rover has collected multiple samples in storage tubes, it will deposit the samples in a ""depot cache"" to be picked up by a yet-to-be-determined future mission. ""We will have a couple different, probably at least two, drop-off spots,"" says Morgan. As to what exactly Mars 2020 will sample, ""that will be something we'll figure out on the surface.""

Geological layers examined by NASA’s Curiosity rover on Mount Sharp, taken January 24, 2017. NASA/JPL-Caltech/MSSS

The NSSC's mission, on the other hand, will do most of the planet-scouting with China's first spacecraft to orbit Mars, or Huoxing in Chinese, meaning ""fire star."" Using data gathered from orbit, Chinese scientists plan to select a location for a follow-up sample return mission that could return to Earth before 2030.



According to Andrew Jones of the Planetary Society, who specializes in Chinese space program coverage, the current plan is to launch a large spacecraft that can carry out all phases of the mission, including atmospheric entry, descent and landing (EDL), sample collection, ascent from Mars, rendezvous in Mars orbit, and a flight back to Earth. Such a mission would require the super-heavy-lift Long March 9 rocket, which is to enter development in the near future, targeting a first flight in 2025.

Chinese Mars lander and rover concept. The country’s National Space Science Center hopes to launch the two spacecraft, as well as an accompanying orbiter, to Mars in 2020. Xinhua

The Chinese sample return mission has yet to receive formal approval, but national space officials and the NSSC's primary contractor, the China Aerospace Science and Technology Corporation (CASC), which is building the 2020 Mars spacecraft, have been discussing the mission publicly, according to Jones. In addition, China published a white paper in 2016 that discusses ""key technological research on the bringing back of samples from Mars.""



While the 2020 missions to Mars are approaching final preparations, aiming to launch in just over two years, the sample return missions to follow are still in early planning stages, and a number of questions about the missions have yet to be addressed. As 2020 looms, however, we could see the space agencies of the two global powers start to drill down exactly how they plan to bring home the most valuable scientific sample ever collected.



Fourth Rock from the Sun

ESA/MPS/Rosetta OSIRIS Team

The sample caches deposited by the Mars 2020 rover will be one of the most significant aspects of that mission, but NASA has yet to figure out how to pick them up. ""All options are on the table,"" says Morgan. ""One concept is a fetch rover.""

""The question we're going to have to ask is, how important is that sample return?""

Thomas Zurbuchen, head of NASA's Science Mission Directorate, discussed possibilities for fast tracking the sample return mission at a National Academies space studies board meeting in August. In the meeting, Zurbuchen proposed a mission to launch as soon as 2026 to send a multi-phase mission to collect the samples. A lander would touch down on Mars and deploy a fetch rover to collect the samples and return to deposit them in the lander. Alternatively, the Mars 2020 rover could deliver samples directly to the lander.

NASA concept of a Mars sample return mission. NASA/JPL

Once secured in the lander, a small rocket called the Mars Ascent Vehicle (MAV) would launch to carry the samples to a rendezvous with another spacecraft in Mars orbit. That orbiting craft would then return to Earth with the rocks from Mars, or possibly fly to an orbit in cislunar space between Earth and the moon to be picked up by a future lunar mission.



To expedite the Mars sample return mission, however, NASA would likely need to postpone a new Mars telecommunications and reconnaissance orbiter, already funded by Congress and currently planned for a 2022 launch. Postponing the launch of a new Mars communications relay satellite would require NASA to figure out how to use existing orbiters to continue support for surface missions, possibly by changing the orbit of the Mars Atmosphere and Volatile Evolution Mission (MAVEN) and repurposing that atmospheric science spacecraft to serve as a telecommunications link.

""At the end, the question we're going to have to ask is, how important is that sample return?"" Zurbuchen said. ""Do we want to tunnel-vision focus on that piece, because of the fact that we think it's so critical?""

While NASA's challenges establishing a sample return mission are largely logistical—deciding which method is best and figuring out how to fund the mission among a multitude of high-priority interplanetary missions—the NSSC's challenges are mostly technical. The global success rate for missions to Mars is only about 50 percent, and to date, just one institution has successfully landed on the red planet: NASA's Jet Propulsion Laboratory.

Overview of NASA’s proposed Mars sample return mission. NASA

China's first attempt to launch an orbiter to Mars, Yinghuo-1, was lost after the Russian spacecraft it was hitching a ride with failed to conduct two engine burns to fly to Mars. The two craft were stranded in Earth orbit in late 2011 and burned up in the atmosphere a couple months later.

While NASA's challenges are mostly logistical, China's are mostly technical.

However, China's series of successful lunar missions provides reason to believe the country could pull off its ambitious Mars mission in 2020. Chinese missions Chang'e 1 and Chang'e 2 successfully orbited the moon in 2007 and 2010, respectively. Chang'e 3 successfully landed on the moon and deployed a small rover called Yutu (Jade Rabbit) in 2013, similar to the lander/rover design China plans to send to Mars.

The Chang’e 3 lander photographed by the small Yutu rover that it deployed. A blurred reflection of the rover is visible in the reflective surface of the lander. Chinese Academy of Sciences

The Chinese space program's next big test will come in 2018, when the People's Republic plans to launch Chang'e 4 to land and deploy a rover on the far side of the moon, a feat that has yet to be accomplished by humanity. Putting a lander and rover on the far side of the moon will require a telecommunications relay satellite to launch to an orbit beyond the moon about six months prior. The Chang'e missions will culminate with 5 in late 2018 or early 2019, the first lunar sample return mission since the Soviet's Luna 24 in 1976.

Racing to Mars and Back

A Long March 5 rocket at Wenchang Spacecraft Launch Center in June 2017. 篁竹水声/Wikimedia

The completion of the Chang'e program will be a vital test for crucial technologies required for both the Chinese 2020 Mars mission and a Mars sample return mission to follow. However, the biggest question mark on the Chinese schedule to Mars sample return is certainly the Long March 9 rocket. A Chinese Long March 5 heavy-lift rocket failed after launch earlier this year, postponing the Chang'e 5 mission. The even larger, more powerful Long March 9, which is still in early planning phases, will undoubtedly present a significant challenge to the Chinese space program. Rockets of this size and power, such as NASA's Space Launch System (SLS), are notorious for running up against schedule delays and high cost overruns.

If China pushes ahead with plans for a Mars sample return mission to launch on one rocket, it will need the Long March 9. However, it seems possible that the country could pivot to a multi-phase mission using the Long March 5, similar to NASA's current sample return mission outline.

The technical, logistical and budgetary challenges for either country to pull off a Mars sample return mission before 2030 are significant and manifold. The first step for both is successfully launching a flagship mission to Mars in 2020.

Diagram of science instruments on NASA’s Mars 2020 rover. NASA

However, at the breakneck pace that China's space program has been advancing, with multiple space stations and lunar missions launched in the past decade, the wealthy and powerful nation is clearly committed to aggressively advancing space sciences. As for NASA, you have to wonder how long the agency's scientists will be able to bear having martian samples carefully drilled out and stored in a receptacle on the surface of the planet, just waiting to be picked up.



If the two nations are starting to feel pressure to beat one another in the race to return home with a tantalizing sample of red planet rock, good. Getting a sample of Mars material into the hands of laboratory scientists on Earth could lead to some of the most profound scientific discoveries in the history of humankind. It's time to kick Mars sample return into high gear and bring that red dirt home.

This content is created and maintained by a third party, and imported onto this page to help users provide their email addresses. You may be able to find more information about this and similar content at piano.io"
80,Tiny Cubesat exploration satellites could 'swarm' across space,"Engineers are racing to create a propulsion method to allow Cubesats, 10 cm-sided satellite cubes, to move independently and in large numbers through space, enabling cheaper exploration of asteroids and planets. Among the projects is Paulo Lozano's chemical propulsion system, which uses electricity and tiny drops of salt water to create speeds of 111,000 mph. The device from Lozano, who is the director of the Space Propulsion Lab at the Massachusetts Institute of Technology, will undergo another demonstration in the coming months and is one of three to have been tested in space.
",https://www.pbs.org/newshour/science/these-tiny-satellites-equipped-with-ion-thrusters-could-change-how-we-explore-space,2018-01-02 15:59:46.557000,"CubeSats, low-cost, bite-sized satellites inspired by the tubes used to hold Beanie Babies, were invented in 1999 as educational tools. Their creators — engineering professors Bob Twiggs and Jordi Puig-Suari — hoped building satellites the size and shape of Rubik’s Cubes would help students of all ages how to design and engineer efficiently.

Now, aerospace suppliers and governments across the globe see the tools as the future of space commercialization and deep space exploration. They want to turn CubeSats into tools for low Earth orbit activities like telecommunications and reconnaissance. Companies like SpaceX, Virgin Galactic, Boeing and Airbus, for instance, want to create a space internet — a network of thousands of CubeSats that provide high speed broadband to remote parts of the world.

And people like Paulo Lozano, director of the Space Propulsion Lab at the Massachusetts Institute of Technology, say sending the tiny satellites to asteroids could help improve space research (or even save the planet from an asteroid attack, he said).

“Instead of going to an asteroid every five, 10 years the traditional way, release a fleet of these tiny little CubeSats and visit 100 asteroids because it’s so cheap,” he said. “Because some of these asteroids, especially the very small ones, have the potential to collide with the Earth. Detecting them in time is important [for stopping them], but also knowing their composition.”

Over the first decade of the CubeSat era, universities dominated the landscape, sending two of every three devices into space. Today, commercial companies and militaries have taken over, launching 70 percent of CubeSats in the last five years.

But there’s still one big problem: CubeSats can’t move once they’re in space — which limits their survival to months or years and makes them dangerous.

“One of the big limitations in CubeSats is that they are launched as secondary payload. Once they are in space, they cannot move,” Lozano said.

Of the 750 or so CubeSats sent into space so far, almost all have lacked their own propulsion systems. The tiny satellites are transported alongside regular cargo, and then flung into space.

But without their own rockets, the CubeSats cannot maneuver on their own. Most fall slowly back to Earth, but some remain in orbit for years, where they join the other 100 million pieces of space debris that are at risk of colliding with other satellites and space stations.

The U.S. Air Force, whose Joint Space Operation Center monitors more than 23,000 orbiting objects larger than four inches in diameter, issues about 700,000 of these collision warnings to satellite owners per year. Imagine what would happen if thousands of CubeSats were added to the fray.

What CubeSats need to stay in space are mini boosters, and scientists like Lozano are racing to build them.

Moving with static

Lozano’s early work focused on big chemical rockets — the kind that you see strapped to space shuttles or on SpaceX missions. He knew these conventional rockets require huge fuel tanks — too big to be carried by CubeSats. Meanwhile, government standards limit how much chemical propellant can be carried by secondary cargo like CubeSats in order to prevent accidental explosions.

“You don’t have a lot of leeway in what you’re allowed to bring up because if your satellite blows up and you’re the secondary payload, the primary people are going to be really angry,” said Kristina Lemmer, a mechanical and aerospace engineer at Western Michigan University, who isn’t involved with Lozano’s research.

So, Lozano needed an alternative. His inspiration: static electricity and tiny drops of salt water.

Static electricity is caused by an imbalance between positive and negative charges in an object. Rub a balloon on your sweater, and its rubber surface becomes covered in negative charge (electrons). Place the balloon near your positively charged hair, and it tugs on the strands until you have a misshapen mohawk.

Lozano’s team designed a set of mini thrusters that rely on the same principle. The devices create an electric field that tugs on the charged particles in salt water until they peel off. The result is a spray made of charged molecules called ions.

This ion spray doesn’t create a lot force. It’s always less than a millinewton, which is akin to the force produced when a mosquito lands on your arm. But the spray moves very fast, and even a small action creates a reaction in the frictionless vacuum of outer space. Use this to move ions in one direction, and a CubeSat will move uber fast in the other.

By launching a fleet of CubeSats, scientists could learn the chemical compositions of asteroids, which could be the key to destroying or redirecting them.

Lozano said the best chemical rockets produce a fiery exhaust that moves at about 9,000 miles per hour. His electrospray thrusters can go more than 111,000 miles per hour, he said.

The thrusters, which look like computer microchips, are the size of quarters. The chips contain a grid of 500 needles — each a custom-built nozzle for spewing ions. His team tests them inside a large vacuum chamber at their lab in Boston.

“In an ideal situation, all of the ions would have the same energy, but the physics of these ion beams makes it so that some ions have less energy than others,” said Catherine Miller, an MIT doctoral and NASA Space Technology Research Fellow. By studying how energy is distributed among the ion beams, she can calculate and standardize how each thruster will perform.

Leading the space charge

Only three propulsion boosters for CubeSats have had successfully demoed in space, Lemmer said. Lozano’s system was one of them, through a partnership with the Aerospace Corporation in 2015. But Lemmer, who published a comprehensive review of CubeSat propulsion systems last year, said Lozano’s ion engines stand out because each one can produce so much thrust.

“Dr. Lozano’s system is probably the frontrunner for the possibility for deep space missions,” Lemmer said. “In order to go interplanetary, you’re going to have to have an electric propulsion system because they are so much more efficient.”

Folks like NASA have counted on the high efficiency of ion engines in the past, such as with the Dawn mission to the asteroids Vesta and Ceres. That journey would have been impossible without the Dawn’s high velocity ion engine. But the Dawn mission cost half a billion dollars. Commercial CubeSats can cost as little as $100,000 — and this price is dropping. Even children are building CubeSats at their elementary schools.

While Lozano’s electrospray thrusters don’t work exactly like the Dawn’s ion engines, Lemmer said the advantages are the same. You can carry less propellant — Lozano’s fuel tanks are the size of sugar cubes — but move more efficiently.

What’s next?

With another demonstration scheduled in early 2018, Lozano is dreaming big. He hopes his tiny thrusters can help CubeSats reach Mars or send them on asteroid scouting voyages. “Since they are so small, you can actually land on the asteroid with these rockets and take off again,” Lozano said.

Though nobody over the last 1,000 years has died because of an asteroid strike, as far as anyone knows, the chances are still disturbing. Your odds of being killed by an asteroid in your lifetime — one in 700,000 — rival death by flood or earthquake in the U.S.

By launching a fleet of CubeSats, scientists could learn the chemical compositions of asteroids, which could be the key to destroying or redirecting them. An asteroid made of silicon, for instance, would be much tougher to stop than one made of iron. Lemmer said CubeSats with propulsion could also provide a cheaper way to test new space technologies.

“Right now, if you want to put a new technology on a NASA satellite, it’s years in the making to run sufficient tests,” Lemmer said. Instead, “if you launch a new technology on a CubeSat and show that it works in space without bad things happening, then you can more easily translate into a NASA mission down the road.”"
81,Genes in Space-3 team identifies microbes on space station,"Scientists on the International Space Station (ISS) have successfully identified microbes in the first-ever sample-to-sequence process on board the ship. NASA astronaut Peggy Whitson worked with the Houston-based Genes in Space-3 team to collect microbial samples from around the ISS, before a MinION device was used to sequence the samples' DNA, enabling it to be identified. Back on Earth, the results were retested and proved to exactly match the ISS outcomes. Being able to identify microbes on the ISS would assist in diagnosing illnesses and infections among astronauts.",https://phys.org/news/2017-12-genes-space-successfully-unknown-microbes.html,2018-01-02 15:55:21.457000,"Sarah Wallace (L), NASA microbiologist and Genes in Space-3 principal investigator, and Sarah Stahl (R), microbiologist, are seen in their Johnson Space Center lab with the in-flight sample from the Genes in Space-3 investigation. Credit: Rachel Barry

Being able to identify microbes in real time aboard the International Space Station, without having to send them back to Earth for identification first, would be revolutionary for the world of microbiology and space exploration. The Genes in Space-3 team turned that possibility into a reality this year, when it completed the first-ever sample-to-sequence process entirely aboard the space station. Results from their investigation were published in Scientific Reports.

The ability to identify microbes in space could aid in the ability to diagnose and treat astronaut ailments in real time, as well as assisting in the identification of DNA-based life on other planets. It could also benefit other experiments aboard the orbiting laboratory. Identifying microbes involves isolating the DNA of samples, and then amplifying - or making many copies - of that DNA that can then be sequenced, or identified.

The investigation was broken into two parts: the collection of the microbial samples and amplification by Polymerase Chain Reaction (PCR), then sequencing and identification of the microbes. NASA astronaut Peggy Whitson conducted the experiment aboard the orbiting laboratory, with NASA microbiologist and the project's Principal Investigator Sarah Wallace and her team watching and guiding her from Houston.

As part of regular microbial monitoring, petri plates were touched to various surfaces of the space station. Working within the Microgravity Science Glovebox (MSG) about a week later, Whitson transferred cells from growing bacterial colonies on those plates into miniature test tubes, something that had never been done before in space.

Once the cells were successfully collected, it was time to isolate the DNA and prepare it for sequencing, enabling the identification of the unknown organisms - another first for space microbiology. An historic weather event, though, threatened the ground team's ability to guide the progress of the experiment.

""We started hearing the reports of Hurricane Harvey the week in between Peggy performing the first part of collecting the sample and gearing up for the actual sequencing,"" said Wallace.

When JSC became inaccessible due to dangerous road conditions and rising flood waters, the team at Marshall Space Flight Center's Payload Operations Integration Center in Huntsville, Alabama, who serve as ""Mission Control"" for all station research, worked to connect Wallace to Whitson using Wallace's personal cell phone.

With a hurricane wreaking havoc outside, Wallace and Whitson set out to make history. Wallace offered support to Whitson, a biochemist, as she used the MinION device to sequence the amplified DNA. The data were downlinked to the team in Houston for analysis and identification.

NASA astronaut Peggy Whitson performed the Genes in Space-3 investigation aboard the space station using the miniPCR and MinION, developed for previously flown investigations. Credit: NASA

""Once we actually got the data on the ground we were able to turn it around and start analyzing it,"" said Aaron Burton, NASA biochemist and the project's co-investigator. ""You get all these squiggle plots and you have to turn that into As, Gs, Cs and Ts.""

Those As, Gs, Cs and Ts are Adenine, Guanine, Cytosine and Thymine - the four bases that make up each strand of DNA and can tell you what organism the strand of DNA came from.

""Right away, we saw one microorganism pop up, and then a second one, and they were things that we find all the time on the space station,"" said Wallace. ""The validation of these results would be when we got the sample back to test on Earth.""

Soon after, the samples returned to Earth, along with Whitson, aboard the Soyuz spacecraft. Biochemical and sequencing tests were completed in ground labs to confirm the findings from the space station. They ran tests multiple times to confirm accuracy. Each time, the results were exactly the same on the ground as in orbit.

The Genes in Space-3 team worked throughout Hurricane Harvey to ensure operations continued on the space station. Pictured are Aaron Burton, Kristen John, Sarah Stahl and Sarah Wallace as they watch NASA astronaut Peggy Whitson work within the Microgravity Science Glovebox (MSG) during part one of the investigation. Credit: Sarah Wallace

""We did it. Everything worked perfectly,"" said Sarah Stahl, microbiologist.

Developed in partnership by NASA's Johnson Space Center and Boeing, this National Lab sponsored investigation is managed by the Center for the Advancement of Science in Space.

Genes in Space-1 marked the first time the PCR was used in space to amplify DNA with the miniPCR thermal cycler, followed shortly after by Biomolecule Sequencer, which used the MinION device to sequence DNA. Genes in Space-3 married these two investigations to create a full microbial identification process in microgravity.

""It was a natural collaboration to put these two pieces of technology together because individually, they're both great, but together they enable extremely powerful molecular biology applications,"" said Wallace.

Explore further Sequencing the station: Investigation aims to identify unknown microbes in space

More information: Sarah L. Castro-Wallace et al. Nanopore DNA Sequencing and Genome Assembly on the International Space Station, Scientific Reports (2017). Journal information: Scientific Reports Sarah L. Castro-Wallace et al. Nanopore DNA Sequencing and Genome Assembly on the International Space Station,(2017). DOI: 10.1038/s41598-017-18364-0"
82,Panasonic develops autonomous robotic tomato picker,"A robot that can autonomously harvest tomatoes has been developed by Japanese electronics firm Panasonic. The device uses a camera, range-image sensors and artificial intelligence (AI) systems to detect which fruit are ripe for picking and cut them from the plant. Mounted on a rail alongside the vine, the company said the robot can perform as well as a human picker, averaging a pace of 10 tomatoes per minute. The device has been publicly demonstrated but Panasonic has made no announcement about its likely cost or plans for its production.",https://internetofbusiness.com/panasonic-robot-tomato-picker/,2018-01-02 15:50:20.853000,"The automation of repetitive tasks in construction and manufacturing has been around for some time. Last month, Panasonic introduced an agricultural robot at Tokyo’s International Robot Exhibition that could have implications for workers in the fruit-picking business.

Harvesting tomatoes is more complicated than you might think. Each fruit has to be plucked from the vine once it is ripe enough, not before. It’s also a delicate operation: tomatoes bruise easily and a single scratch in one can lead to a whole box going bad, fast.

Read more: Robot tax could ease drawbacks of automation

Panasonic robot harvests tomatoes

To handle the perception and dexterity-related challenges that come with fruit picking, Panasonic’s new robot relies on a combination of camera, range image sensor and artificial intelligence technologies.

First, it recognizes which tomatoes are ready to be picked. Then, it performs a precise cut-and-catch technique to move each tomato from vine to bucket.

The robot can be mounted on a rail, enabling it to slide along one vine from start to finish. In terms of speed, Panasonic expects the robot to perform at least as well as a human, harvesting at an average pace of 10 tomatoes per minute.

However, as the robot doesn’t need breaks, pay rises or sick days, it’s easy to see where the attraction might lie in terms of wider efficiency gains.

Panasonic has so far only demonstrated its harvesting robot and no announcement has yet been made regarding its readiness for market or cost.

Read more: Tarzan robot swings above crops for automated agriculture

With great dexterity comes great responsibility

The rise of computer vision and faster, more agile robots has made complex tasks accessible to automation. Tomato picking is just one example.

Last month, Ocado released footage of a new bagging robot, capable of picking products and carefully placing them into shopping bags based on the shape and weight of each item.

This level of processing and dexterity could pave the way for applications that go far beyond monotonous tasks in agriculture and retail.

Read more: Italian start-up Evja launches smart agriculture platform for salad growers"
83,Chocolate may disappear due to effect of climate change on cacao,"Chocolate could disappear by 2050 because of higher temperatures and less rainfall in regions where cacao trees are grown, according to the US's National Oceanic and Atmospheric Administration. Cacao plants only grow well in rainforest regions that are near the equator, with half of the world's chocolate being produced in Côte d’Ivoire and Ghana at up to 850 ft above sea level. Rising temperatures are pushing growing regions to about 1,000 ft above sea level and into mountainous terrain. UC Berkeley is working with Mars to generate new strains of cacao plants using CRISPR gene-editing technology.
",http://www.grubstreet.com/2018/01/research-says-chocolate-could-go-extinct-by-2050.html?utm_campaign=grub-street&utm_source=tw&utm_medium=s1,2018-01-02 15:46:39.780000,"Maybe start hoarding now. Photo: Diana Miller/Getty Images/Cultura Exclusive

Here’s some climate-change news that President Trump will have trouble ignoring: Earth’s junk food is in danger of losing a crucial ingredient. Scientists now predict that chocolate — which POTUS will sometimes eat to celebrate making important military decisions — could become impossible to grow in the coming decades because of hotter temperatures and less rain in regions where cacao plants are cultivated. The year 2050 is when they predict that people will be forced to satisfy a sweet tooth with toffee or caramels seasoned with tears.

Like coffee plants and wine grapvines, cacao is a finicky tree. It only grows well in rain-forest land that’s within 20 degrees of the equator. Half of the world’s chocolate is produced in Côte d’Ivoire and Ghana, where the plants thrive at around 300 to 850 feet above sea level and under dependably humid weather conditions. But by 2050, researchers say that rising temperatures could push the optimal cultivation zone “uphill,” to as high as 1,500 feet.

Thankfully, a team from UC Berkeley is working on a possible fix. It’s actually part of a new partnership that Mars announced last year. The M&M’s and Snickers maker is investing $1 billion into a variety of efforts to fight climate change, and the scientists in a plant-genomics program at Berkeley hope to develop hardier cacao plants that won’t wilt or rot at their current altitude.

Berkeley’s gene-editing technology, called CRISPR, has been in the works for a while, though when it gets attention, it’s almost always for the potential to eliminate genetic diseases or (sort of on the extreme end of this) build “designer babies.” But creator Jennifer Doudna tells Business Insider that the “most profound” application will likely be saving food."
84,Netherlands introduces tradeable phosphate rights for dairy cattle,"The European Commission has given the go-ahead to a trading system for phosphate rights for dairy cattle in the Netherlands, aimed at improving the country's water quality by limiting phosphate production from dairy cattle manure and encouraging a move to land-based farming. Dairy farmers will receive phosphate rights for free and will be obligated each year to prove they have sufficient rights to justify the quantity of phosphate produced by their manure. Phosphate rights can be obtained on the market, with 10% of the traded rights held back to promote the development of more land-based dairy farming.",https://www.government.nl/latest/news/2017/12/19/european-commission-gives-green-light-for-dairy-cattle-phosphate-system,2018-01-02 15:34:08.223000,"European Commission gives green light for dairy cattle phosphate system

The European Commission has agreed to the introduction of phosphate rights in Dutch dairy farming. This system and the legislation in which it is enshrined satisfy the applicable guidelines for state aid, European Commissioner for Competition Margrethe Vestager reported to the Minister of Agriculture, Nature and Food Quality Carola Schouten. The House of Representatives and the Senate previously assented to the phosphate system. This system will ensure that the quantity of phosphate produced by cattle as a constituent of manure is kept below the European maximum. The system is set to come into force on 1 January 2018.

Schouten꞉ “Now that Brussels has confirmed that the legislation does not constitute state aid, we can be certain that the phosphate rights system will go ahead. Moreover, this decision is also key for the purpose of obtaining a new derogation, a special exemption on the basis of which the Netherlands will be entitled to use more animal manure. Dairy farmers are waiting anxiously for this decision. I can now say to the entire sector: ‘We’re not there yet, but this achievement is already significant’”.

From 1 January 2018, dairy farms will be allocated an amount of phosphate rights based on the number of cattle kept as at 2 July 2015 (the date on which the system was announced), less the previously announced generic reduction of 8.3%. Land-based farms with plenty of land in proportion to the number of cattle are exempt from this reduction, which is necessary to keep phosphate production below the European maximum. The phosphate rights are tradable. Farmers wishing to keep more cattle will have to purchase rights to this end from dairy farmers who are reducing their livestock or terminating their company.

The system of phosphate rights follows the Phosphate Reduction Scheme, which saw trade associations and the Government agree to curb phosphate production in 2017. Considerable reductions in livestock have already been made over the past year through this plan. The most recent figures (October 2017) indicate that the Netherlands is on course with its ambition to reduce phosphate production below the national ceiling again by the end of this year.

The Netherlands is striving to secure a decision from the European Commission granting a new derogation for the 2018–2021 period around April 2018. In this regard, it is important for phosphate production to be brought back below the European maximum by the end of 2017 and for the phosphate system to come into force on 1 January 2018. It is also imperative that agreement has been reached on the Sixth Action Programme of the EU Nitrates Directive. In the unfortunate event that the new derogation is not granted, farmers would be forced to incur additional expenses such as for the responsible disposal of manure and for the supply of extra fertiliser."
85,Netherlands introduces tradeable phosphate rights for dairy cattle,"The European Commission has given the go-ahead to a trading system for phosphate rights for dairy cattle in the Netherlands, aimed at improving the country's water quality by limiting phosphate production from dairy cattle manure and encouraging a move to land-based farming. Dairy farmers will receive phosphate rights for free and will be obligated each year to prove they have sufficient rights to justify the quantity of phosphate produced by their manure. Phosphate rights can be obtained on the market, with 10% of the traded rights held back to promote the development of more land-based dairy farming.",https://www.dairyreporter.com/Article/2018/01/02/Tradable-phosphate-rights-introduced-in-the-Netherlands,2018-01-02 15:34:08.223000,"The system came into effect January 1, 2018.

The measure has been introduced to improve water quality in the country by limiting phosphate production from dairy cattle manure and promote a shift to land-based farming.

The EC said that given the high density of dairy cattle in the Netherlands, the phosphate contained in dairy cattle manure represents a significant environmental concern.

In addition to the main environmental objectives, the system also provides support for young farmers and is intended to have a positive effect on grazing and grassland.

Trading rights

Dairy farms will be awarded phosphate rights for free and will only be allowed to produce phosphate from dairy cattle manure corresponding to the phosphate production rights they hold. At the end of each calendar year, farms will be required to demonstrate that they have sufficient phosphate rights to justify the amount of phosphate produced by their dairy cattle manure.

Dairy farms, including new entrants, can acquire phosphate rights on the market, as phosphate rights will be traded.

When a transaction occurs, 10% of the traded rights will be withheld and kept in a ‘phosphate bank.’

This is intended to encourage the development of more land-based dairy farming by providing temporary, non-tradable rights to ""land-based farms,"" which can fully absorb on their land all the phosphate from their own manure production.

Based on the environmental objectives the system aims to achieve, the European Commission concluded that the system is in line with the EU rules for environmental State aid."
86,"3,000-metre high Chinese salt lake holds vast lithium deposit","A remote salt lake in the Chinese inland province of Qinghai is home to 83% of the country's lithium deposit. The metal is crucial for the development of electric vehicle (EV) batteries and China is the second richest country in terms of reserves, after Chile. The majority of its lithium can be found within salt sediment on the floor of Chaerhan Salt Lake. Companies such as major Chinese EV maker BYD have begun setting up factories in the area after securing a concession for recovering lithium.",https://asia.nikkei.com/Business/Trends/Future-of-electric-cars-is-at-the-bottom-of-a-Chinese-salt-lake,2018-01-02 15:33:39.597000,"GOLMUD, China -- The world is on the brink of an electric vehicle revolution. The widespread use of electric cars will depend on the availability of lithium, which is crucial for electric vehicle batteries. China is the world's second-richest country in lithium reserves, after Chile.

Countries are now scrambling to secure supplies of the valuable metal, but where is it found? I visited a remote area of China, about 3,000 meters above sea level, that is one of the world's largest lithium-producing areas and which the Chinese government considers a strategic region. ""Huge amounts of capital are rapidly flowing into the town,"" a local resident said.

In late November, I took a full day to travel from Guangzhou, Guangdong Province, in southern China, to Golmud in the country's inland province of Qinghai. The air is thin in the area, located high in the mountains between the Tibet Autonomous Region and the Xinjiang Uighur Autonomous Region, and I could walk only a little way before I was short of breath.

""This airport was completed only a little over a month ago. It's brand-new,"" said an official. I assumed it had been built in anticipation of rapidly growing lithium demand. I got into the four-wheel-drive Land Cruiser that had come to pick me up, and, soon after leaving the airport, I saw a vast expanse of salt marsh extending as far as the eye could see. ""The elevation here is high, but in ancient times all this area was under the sea. Crustal movements lifted it up,"" the driver, a local man, told me.

World of salt

While I marveled at the grand scale of the landscape, we traveled about an hour and a half to Chaerhan Salt Lake, which takes its name from a Mongolian word meaning ""world of salt."" At an elevation of nearly 3,000 meters, in the freezing cold and clear air, the vast lake sparkled in the sunlight. Surrounded by large volumes of dried salt, it looked like it was wearing snow makeup. The locals call the lake the ""mirror of the sky.""

The area centering on Chaerhan Salt Lake, is home to 83% of China's lithium deposits, found within several meters of salt sediment on the lake floor.

No living things inhabit the lake or its surroundings, so silence reigned. However, soon 10-ton trucks loaded with heaps of salt recovered from the lake began rumbling by, and the lithium-producing area appeared to suddenly come to life.

After traveling a little farther along a bumpy road built of pressed salt, I spoke to a man named Li Jingwei, 47. He said he had worked for a plant of a state-owned company by the lake since he was 16, and called himself an experienced old hand. ""The salt lake provides lots of precious resources. Attention is now on lithium, which is used in electric vehicles,"" he said. ""Small developers have been driven out, and over the past three years, state-owned enterprises have come in and investment has become active. This is such a remote place, but many dignitaries come here.""

Lithium recovered from Chaerhan Salt Lake is used to make electric vehicle batteries. The area around the lake appeared to have come to life. (Photo by Yu Nakamura)

I wondered which dignitaries visited such a remote place, and was surprised to see the photographs on a wall of the plant where Li was working.

Electric car maker moves in

The photos showed Wen Jiabao, Zhang Dejiang, Li Changchun, Zhao Leji, Li Keqiang and other high-level dignitaries. They even included one that showed President Xi Jinping encouraging the employees during his visit in August last year. ""Jiang Zemin also came, though there is no photograph of him here. This is a front-line base for China's resources,"" Li said proudly.

The auto industry is already ramping up for what is expected to be a rapid shift from gasoline to electric vehicles, and as a result, lithium prices have already soared. On Shanghai's metals market, lithium carbonate is trading at around 170,000 yuan ($25,700) per ton, more than three times the level two years ago.

""The price of lithium has risen, and business is good. We expect even better times,"" said a factory worker of BYD, China's biggest electric vehicle manufacturer. BYD moved into the area a year ago, realizing that Chaerhan Salt Lake holds the key to electric vehicle growth, and has succeeded in securing a concession for recovering lithium. It jointly set up the factory with a local state-owned enterprise, and is hurriedly preparing to start production.

In China, the world's biggest automobile market, electric vehicles still account for only about 2% of new car sales. However, the country's electric car market is expected to grow rapidly and reach 5 million vehicles by 2025 -- comparable to Japan's entire market for new cars. Preparations are already underway in this remote area of China. A massive wave of business activity that will influence the world is about to spread from the quiet city of Golmud."
87,Publishers prepare for Facebook feeds without news,"Online publishers are considering strategies for a future in which Facebook removes news stories from its main pages, called newsfeeds. The social network last year experimented with a separate newsfeed called Explore in six countries outside the US, but said it didn't plan to roll this out further. However, a number of other online platforms including Twitter and Snapchat have separated news stories from user-generated content to some degree, leading to speculation Facebook will follow suit. Publishers have already seen their direct traffic from Facebook decline in recent months as user content and videos have taken precedence.
",https://digiday.com/media/2018-year-facebook-banishes-news-feed/?utm_medium=email&utm_campaign=digidaydis&utm_source=uk&utm_content=180102,2018-01-02 15:27:08.127000,"Publishers have a lot to gripe about when it comes to Facebook, from the platform choking off their referral traffic, dominating digital advertising and giving them whiplash with its constantly changing video strategy. But what if it got even worse?

In 2018, Facebook could take a step further and separate news from the news feed. It’s not a crazy idea. The platform tested a newsless news feed, called the Explore Feed, in six countries outside the U.S., causing a major publisher freakout. (Facebook said it didn’t expect to roll out the test further.) In the past year, Facebook also launched Watch, a TV-like video tab; and prioritized Facebook Groups, communities for people who share interests or characteristics — also underscoring the idea of separating user interaction from other media content.

Other platforms have made moves to separate users’ messages from media and brands’ content. Snapchat redesigned its app to separate users’ feeds from brands’ content. Instagram is testing a private messaging app, which would take peer-to-peer chat out of the main app. Twitter has its Moments tab, a dedicated home for news and entertainment stories.

Fundamental to the success of platforms like Twitter and Facebook is keeping users happy, and as such, they’re always running experiments to see if changes will get people to return more often and stay longer. Given a lot of news is negative or controversial, a feed with no news (unless it’s shared by a user) could be less contentious and more enjoyable for users. And another group that likes less controversy, of course, is another important Facebook constituency: advertisers.

“Sometimes people get really annoyed and confused when they’re reading about their cousin’s bar mitzvah or whatever and they see a very serious story afterward,” said Andrew Montalenti, CTO and co-founder of web analytics firm Parsely. “All of the platforms, what they’re really concerned about with fake news is that I think you kind of draw on a bank account of trust with the user. If you come across that stuff too much, you declare it to be a problem, and you stop using it. So they have to play this delicate balance — ‘We can’t show you too many ads or show you too much spammy content.’”

Another factor is the fake-news imbroglio that blew up in Facebook’s face in the past year, leading lawmakers to threaten regulation. Facebook responded by trying to police fake news, which has proved to be a challenge. Further de-emphasizing news or taking it out of the feed altogether is one way to deal with the problem.

As to the Explore test, Facebook said: “There is no current plan to roll this out beyond these test countries or to charge pages on Facebook to pay for all their distribution in News Feed or Explore.” That was cold comfort to those publishers who depend on the news feed to reach audiences, though. As much as Facebook has declined in reach, it’s still a significant source of traffic for many publishers, which have already seen their direct traffic from Facebook decline in recent months, if not years, as Facebook has prioritized users’ posts and video content in the news feed.



Some publishers whose audience strategy is closely tied to Facebook and follow the company closely are starting to consider the possibility of a newsless news feed. An executive at a traditional publishing company said this is “definitely on our minds” given the company gets a “ton of traffic from Facebook,” and it’s a risk the company has to think about in the next few years. “It would be seismic shift,” said another publishing exec.

“There’s good reason to be concerned if publishers’ content becomes separated out of the main news feed,” said Vivian Schiller, a former Twitter news executive. “Their criteria [for the Explore test] was about user experience. That’s their business. But it’s hard to imagine this not having a deleterious effect on publishers.”

There are other reasons for Facebook to go in this direction. Facebook could make an exception for publishers and other commercial content providers that pay to be in the news feed, which could mean more revenue for Facebook. Separating news from the feed also could give Facebook a way to test a potential new product, similar to how it took Messenger out of the site and made it its own app, Schiller said.

Of course, none of this is a fait accompli. There’s good reason to think Facebook will keep news in the feed. Scrolling through the news feed is the core daily habit for most Facebook users. It’s what Facebook uses to promote its many other products, like the Watch video tab and Marketplace. It’s hard to get people to toggle from the news feed to other places on Facebook.

That said, even if a newsless news feed doesn’t materialize, publishers have to adapt. Facebook, and Google, are here to stay, and Facebook has proven time and time again that it’s not always going to act in publishers’ interests. Publishers have to take matters into their own hands, and take advantage of other audience and revenue opportunities."
88,Researchers create graphene battery that charges in seconds,"Scientists from Zhejiang University have developed an aluminium-graphene battery that can be charged in seconds, rather than hours. The battery is able to maintain 91% of its original capacity after 250,000 charges and could be fully charged in 1.1 seconds. The researchers also claimed the battery can work in temperatures ranging from minus 40C to 120C. However, they added that the battery does not have the energy density of lithium-ion batteries and is at present too expensive for commercial production.",http://www.xinhuanet.com/english/2017-12/23/c_136847756.htm,2018-01-02 15:20:14.633000,"Source: Xinhua| 2017-12-23 21:15:29|Editor: Zhou Xin

Video Player Close

HANGZHOU, Dec. 23 (Xinhua) -- A team of researchers from Zhejiang University have developed a new type of aluminum-graphene battery that can be charged in seconds, instead of hours.

The team, led by professor Gao Chao, from Department of Polymer Science and Engineering of Zhejiang University, designed a battery using graphene films as anode and metallic aluminum as cathode.

The battery could work well after quarter-million cycles and can be fully charged in seconds.

Experiments show that the battery retains 91 percent of its original capacity after 250,000 recharges, surpassing all the previous batteries in terms of cycle life.

In quick charge mode, the battery can be fully charged in 1.1 seconds, according to Gao. The finding was detailed in a paper recently published in Science Advances.

The assembled battery also works well in temperatures range of minus 40 to 120 degrees Celsius. It can be folded, and does not explode when exposed to fire.

However, the aluminum-ion battery cannot compete with commonly-used Li-ion batteries in terms of energy density, or the amount of power you can store in a battery in relation to the size, according to Gao.

""It is still costly to make such battery. Commercial production of the battery can only be possible until we can find cheaper electrolyte,"" Gao said."
89,Insurtech Urban Jungle raises €1.1m in funding round,"UK fintech Urban Jungle has raised €1.1m ($1.3m) in seed funding towards its tech-driven insurance service for young renters. The investment, led by Rob Devey, former CEO of Prudential UK and HBOS insurance, will help the London-based start-up increase the choice of products available to its customers, such as one targeted at house and flat shares. Urban Jungle was founded in 2016 by Jimmy Williams and Greg Smyth, who felt young people were excluded from the insurance market as it failed to keep up with their changing lifestyles.
",http://www.eu-startups.com/2018/01/london-based-urban-jungle-raises-e1-1-million-to-build-a-tech-enabled-home-insurer/,2018-01-02 14:45:47.320000,"Urban Jungle, a London-based FinTech startup revamping insurance for young people, has secured €1.1 million seed funding from a group of top angel investors, in order to build a better home insurance experience for the growing population of renters. The financing round was led by Rob Devey, ex CEO of Prudential UK and HBOS insurance. This capital injection will support the startup’s plans to launch several new products, including one targeted at the growing number of house and flat shares.

Urban Jungle was founded in 2016. The founders Jimmy Williams and Greg Smyth looked at the insurance industry, and noted both that poor tech was making it difficult to buy and use, and that young people were being excluded from insurance because it was failing to keep pace with their changing lifestyles. They believe they can transform the insurance experience by completely rebuilding the tech stack, using modern technologies, for example through smartphone-first design, and using machine learning to improve risk scoring.

Jimmy Williams commented: “This investment marks a big step towards our mission to build an insurance provider that customers love. Through 2017, we focused on building a business that our customers can rely on 100% when something goes wrong. In 2018 we’ll vastly increase the choice of products for our customers, and make all of our products even easier to use.”

Rob Devey, who led both this, and the business’ previous round of funding commented: “I’m very excited about the huge opportunity to transform the insurance industry using technology, and think this team is superbly well placed to capitalise on that. They have proven their ability to execute to a very highly quality over the past year, and I’m confident that they will only accelerate from here.”"
90,Five major office sites due to open in central London,"Five major London office projects are due to open this year. Goldman Sachs's 825,000 sq ft London headquarters at 70 Farringdon Street was the subject of subletting speculation after rumours circulated that the firm would move staff to Frankfurt post-Brexit, while the International Quarter in Stratford represents a location gamble for Deutsche Asset Management. M&G Investments has leased 11 of the 13 floors at 10 Fenchurch Avenue. However, The Scalpel at 52 Lime Street is barely more than one-third leased and 70 St Mary Axe, also known as The Can of Ham, has yet to secure a tenant.",https://www.bisnow.com/london/news/office/mixed-fortunes-the-five-biggest-office-schemes-opening-in-london-in-2018-83078?single-page,2018-01-02 14:38:48.060000,"Courtesy of the City of London Corp. The City of London skyline in 2026 London's skyline is rapidly changing, as this image from the City of London Corp., forecasting the City skyline in 2026, shows. In the shorter term, 2018 is set to be a pivotal year for the London office market. Supply of new space will begin to drop after 2017's cyclical peak, according to Deloitte Real Estate, and 44% of the space set to be completed this year is already leased. But with Brexit negotiations at their most delicate, demand will be at its most skittish. Here are the five biggest office schemes opening in 2018 and who is occupying them, according to Deloitte. In terms of lettings, some are doing significantly better than others. Begin slideshow

70 Farringdon St. — Goldman's new HQ Courtesy of Goldman Sachs Goldman's new London HQ Goldman Sachs broke ground on its 825K SF London headquarters before the U.K. voted to leave the EU. As Brexit has been negotiated, there has been speculation Goldman would move staff to Frankfurt and thus not occupy the entire building — especially after Chief Executive Lloyd Blankfein started trolling the U.K. government on Twitter. There is no sign of it looking to sublease any space yet ahead of the building completing in the third quarter.

The International Quarter — the new home of the Financial Conduct Authority Courtesy of Lendlease Building S5 at Lendlease's International Quarter at Stratford The Financial Conduct Authority pre-let 425K of the 515K SF building at the International Quarter in Stratford. Lendlease and LCR are delivering the scheme in the middle of 2018. The FCA's pre-let in building S5 convinced Deutsche Asset Management to pay £370M for a building in what is still an emerging office location.

10 Fenchurch Ave. — a new HQ for M&G Investments Courtesy of Greycoat 10 Fenchurch Ave. M&G, the investment division of insurer Prudential, in 2014 signed for 11 of the 13 floors at the 398K SF 10 Fenchurch Ave., which is being developed by Greycoat and CORE on behalf of Italian insurance company Generali.

The Scalpel — two-thirds still to be leased Courtesy of WR Berkley The Scalpel The Scalpel at 52 Lime St. in the City is being built by the development arm of U.S. insurance company WR Berkeley. The company will occupy 81K SF of the 387K SF building, and financial and insurance firms Axis and BPL have taken space, but 61% of the building is unlet. The 35-storey building is scheduled to complete in the second quarter."
91,Researchers train robots to respect human personal space,Researchers in Argentina have developed systems to teach robots to respect humans' personal space. The team at the National University of San Juan devised techniques to replicate the social signals given by humans when they feel their personal space is being invaded. These were then set as defined physical fields for the robot as it followed a human's movements. The experiments showed the robot was able to successfully emulate the behaviour and movement of a human. Scientists claim this will help increase future social acceptance of robots.,http://www.robodaily.com/reports/Dont_step_on_my_heels_Scientists_teach_robots_how_to_respect_personal_space_999.html,2018-01-02 14:33:20.433000,"Scientists teach robots how to respect personal space



by Staff Writers



Beijing, China (SPX) Jan 02, 2018



Robots have a lot to learn about humans, including how to respect their personal space. Scientists at the Institute of Automatics of the National University of San Juan in Argentina are giving mobile robots a crash course in avoiding collisions with humans.

The researchers published their methods in IEEE/CAA Journal of Automatica Sinica (JAS), a joint publication of the IEEE and Chinese Association of Automation.

""Humans respect social zones during different kind[s] of interactions,"" wrote Daniel Herrera, a postdoctoral researcher at the Institute of Automatics of the National University of San Juan and an author on the study. He notes how the specifics of a task and situation, as well as cultural expectations and personal preferences, influence the distance of social zones.

""When a robot follows a human as part of a formation, it is supposed that it must also respect these social zones to improve its social acceptance.""

Using impedance control, the researchers aimed to regulate the social dynamics between the robot's movements and the interactions of the robot's environment. They did this by first analyzing how a human leader and a human follower interact on a set track with well-defined borders.

The feedback humans use to adjust their behaviors - letting someone know they're following too closely, for example - was marked as social forces and treated as defined physical fields.

The human interactions (leading and following), including the estimated social forces, were fed to a mobile robot. The programmed robot then followed the human within the same defined borders, but without impeding on the social forces defined by the human interactions.

""Under the hypothesis that moving like human will be acceptable by humans, it is believed that the proposed control improves the social acceptance of the robot for this kind of interaction,"" wrote Herrera.

The researchers posit that robots are more likely to be accepted if they can be programmed to respect and respond like humans in social interactions. In this experiment, the robot mimicked the following human, and avoided the leader's personal space.

""The results show that the robot is capable of emulating the previously identified impedance and, consequently, it is believed that the proposed control can improve the social acceptance by being able to imitate this human-human dynamic behavior.""

Research paper

Tokyo, Japan (SPX) Dec 15, 2017





Lockheed Martin and NEC Corp have announced that Lockheed Martin will use NEC's System Invariant Analysis Technology (SIAT) in the space domain. SIAT's advanced analytics engine uses data collected from sensors to learn the behavior of systems, including computer systems, power plants, factories and buildings, enabling the system itself to automatically detect inconsistencies and prescribe resol ... read more

Related Links

"
92,UK campaign urges parents to reduce children’s sugar intake,"Public Health England (PHE) has called for parents to limit their children to two sugary snacks of no more than 100 calories each per day, as part of a new Change4Life campaign. At present, children in England are consuming a daily average of at least three high-calorie sweet snacks and drinks, with a third of children eating four or more. PHE said that children’s food intake can contain three times the recommended amount of sugar on average. The campaign will include money-off vouchers for healthier snacks, a TV advert by Aardman Animations and leaflets in schools.
",https://www.theguardian.com/science/2018/jan/02/childrens-snacking-habits-setting-them-up-for-obesity-in-later-life,2018-01-02 14:27:23.737000,"Children’s snacking habits are setting them up for obesity and poor health, Public Health England has warned, calling on parents to take a tougher line on sweets and cakes and fizzy drinks between meals.

Children in England are eating on average at least three unhealthy high-calorie sugary snacks and drinks every day, says PHE, and about a third of children eat four or more. It is urging parents to draw the line at two and make sure they are not more than 100 calories each.

The diet of the average child can contain three times more sugar than recommended, says PHE. Half the equivalent of seven sugar cubes a day they consume comes from unhealthy snacks and drinks. Each year that includes almost 400 biscuits, more than 120 cakes, 100 sweets, 70 chocolate bars and 70 ice creams, washed down with more than 150 juice drink pouches and cans of fizzy drink.

The slogan of a new campaign under the Change4Life banner is: “Look for 100 calorie snacks, two a day max”.

That could lead to a significant change in diet. An ice-cream contains about 175 calories, a pack of crisps contains about 190 calories, a chocolate bar contains about 200 calories and a pastry contains about 270 calories, says PHE.

There will be a drive to encourage healthier snacking, with signposting at supermarkets and special offers on fruit and vegetables. Parents can sign up on the Change4Life website to get vouchers for money off snacks PHE identifies as healthier, such as malt loaf, lower sugar fromage frais, and drinks with no added sugar.

Other snack foods PHE says are healthier include fresh or tinned fruit salad, chopped vegetables and lower fat hummus, plain rice cakes, crackers, lower fat cheese, small low-fat, lower sugar yoghurt, sugar free jelly, crumpets and Scotch pancakes.

Dr Alison Tedstone, chief nutritionist at Public Health England, said: “The true extent of children’s snacking habits is greater than the odd biscuit or chocolate bar. Children are having unhealthy snacks throughout the day and parents have told us they’re concerned.

“To make it easier for busy families, we’ve developed a simple rule of thumb to help them move towards healthier snacking – look for 100 calorie snacks, two a day max.”

The campaign will include a new TV advert from Aardman Animations as well as leaflets in schools.

Justine Roberts, CEO and founder of Mumsnet, said: “The volume of sugar kids are getting from snacks and sugary drinks alone is pretty mindblowing, and it can often be difficult to distinguish which snacks are healthy and which aren’t.

A third of children are leaving primary school obese or overweight. Recent figures from the National Child Measurement Programme in schools show the number of obese children in reception year has risen for the second consecutive year (to 9.6%) and has shown no improvement in year 6 (20%).

A quarter of children (24.7%) suffer from tooth decay by the time they turn five. Tooth extraction is the most common cause of hospital admissions in children aged 5 to 9 years."
93,Halal and kosher meat could face additional regulation after Brexit amid cruelty concerns,"The UK government could introduce food labels on halal and kosher meat indicating that the animal was not stunned before being slaughtered following its withdrawal from the European Union, according to farming minister George Eustice. Current EU legislation does not require such information on meat packaging, but Conservative MP Laurence Robertson said its introduction would allow consumers to make an informed choice, adding: ""Brexit… makes it easier for us to legislate without an eye looking over our shoulders"".
",http://www.telegraph.co.uk/news/2018/01/01/halal-meat-could-labelled-method-slaughter-brexit-amid-animal/,2018-01-02 14:10:54.410000,"Halal and Kosher meat could be labelled by method of slaughter after Brexit amid concerns that animals are suffering needlessly before being killed.

Tory MPs and leading vets have for years raised concerns that the failure to stun animals before killing them under some methods of slaughter is cruel.

George Eustice, the farming minister, has now given a clear indication that the Government will consider introducing labelling after the UK leaves the European Union.

It comes after Michael Gove, the Environment Secretary, vowed to ensure that animal welfare standards will be even higher after Britain leaves the EU.

Mr Eustice said: that consumers should be able to make an ""informed choice"" about what they decide to buy.

He said: “There is no national or EU requirement to display the method of slaughter on meat products but where this is included it must be accurate.

“The Government believes that consumers should have the necessary information available to them to make an informed choice about their food, and this is something we can consider in the context of leaving the EU.”"
94,"'Big food' is acting like big tobacco, says campaigner","A former advertising executive who promoted Coca-Cola and McDonald's has claimed that ""big food"" is copying the tactics of large tobacco companies. In an interview with the Guardian, Dan Parker, the founder of the Living Loud charity, which supports anti-obesity campaigns including those by the Jamie Oliver Food Foundation, said that voluntary codes would not succeed and that government regulation is required. Parker cited the example of companies reducing the size of chocolate bars without lowering the price. Such moves led to a 7.6% increase in sales of larger 100g bars and a 5% decrease in single bars in 2016.
",https://www.theguardian.com/society/2018/jan/02/former-advertising-executive-reveals-big-foods-junk-food-pushing-tactics,2018-01-02 14:04:15.963000,"A former advertising executive who spent two decades working with “big food” corporations has revealed how they are still working to persuade us to eat more sugar and junk food in spite of the obesity epidemic.

Dan Parker, who was a successful advertising executive earning his living promoting Coca Cola and McDonalds, told the Guardian in his first interview that the food industry is behaving like Big Tobacco. “I think what the food industry does now will define where it lands. If it behaves like tobacco it will end up being treated like tobacco. And I think it is behaving like tobacco,” said the former industry insider.

Parker’s life changed when he was diagnosed with obesity-related type 2 diabetes, the disease that killed his father. In a “lightbulb moment”, he realised he could help save people’s lives by using his skills to try to help curb the junk food we eat.

Parker founded a charity called Living Loud, bringing on board others from marketing and advertising. In their first year of existence, they have helped anti-obesity campaigners like the Jamie Oliver Food Foundation understand the industry and communicate their messages.



Q&A Share your tips on managing snacks for children Show We’d like to hear how you are trying to limit the amount of snacks and sugary foods your children eat.

You can share your story using our encrypted form here. We will feature some of your contributions in our reporting. Was this helpful? Thank you for your feedback.

Asking the industry, supermarkets and advertising agencies to voluntarily dial down what they do will not work, he says. They need limits imposed by government so that everyone is on a level playing field. Parker cites the shrinking size of chocolate bars to illustrate how voluntarism is not working. Manufacturers have produced smaller portion sizes, but they have not cut the prices.

“This has made people angry. People are howling with rage about the fact that their single chocolate bar is smaller but the same price,” he told the Guardian. And their response is to buy the bigger bar which looks like better value for money – while the industry is now advertising the family size as something one person can eat by themselves.

“What you’re seeing is a lot of advertising for the bigger bar. You are seeing a lot of promotion of the bigger bar at point of purchase,” he said. “In WH Smiths you get thrust a £1 chocolate bar if you go in there for anything.”

Figures published in The Grocer magazine showed that single chocolate bar sales were down year on year in 2016 by about 5% to around £130m whereas sales of the tablet bars of around 100g were up 7.6% to £420m and the share bags were up 2.7% to about £300m. “What’s happening is this massive migration from the single bar to the bigger bar,” said Parker.

The chocolate companies are promoting this choice with adverts like the Audrey Hepburn Galaxy video, showing a digitally recreated Hepburn figure deciding to sit in the back seat of the car of a handsome admirer so she can eat a large bar of chocolate by herself, says Parker.

Sales of single chocolate bar are down – but that has been countered by a rise in sales for larger bars. Photograph: Lefteris Pitarakis/AP

“What that’s doing is normalising the idea that 100g bar is an individual portion of chocolate – although it will say on it you shouldn’t eat more than 30g in tiny little writing on the bottom, the advert says that for Audrey it’s a single portion,” he said. The same is true of an advert showing Gary Lineker in hospital with a large bag of crisps, which he refuses to share with his children.

“Both those adverts are formalising a larger portion size. I don’t think that’s very healthy,” he said.

Parker was diagnosed with type 2 diabetes four years ago, a result of obesity and long working hours spent at a desk. “I closed down my business and I decided that I would try and do something about it. I felt a need to redeem myself in some ways but also I guess I felt an immense sadness that the suffering that’s happened in my own family probably should have been avoided,” he said.

He reversed his diabetes through a strict diet with the help of Dr David Cavan, with whom he now has a commercial venture, offering a programme called “Diabetes Turnaround”.

He was on Brighton beach when he realised he could be part of the solution. “ If you want to boil obesity down, the single most important issue is what we put in our mouth. And nobody knows more about why people put in their mouth what they put in their mouth than the people who sit around the table at Coca Cola and McDonalds and Asda and these companies. I sat round that table. I got paid a lot of money because I was pretty good at this. So I suddenly realised that not only did I have this huge great desire to do something but I kind of went – you know what – I think I might be part of the answer here.”

The food industry, Parker says, is at a crossroads. “If [the food industry] continues to sit there saying we’re great, there’s no problem, it’s all to do with everything else, eventually suddenly there will be a switch in public will and then there will be an awful lot of bad regulation happening,” he said.

“What’s clearly happening at the moment is the food industry’s working hard to drag its heels,” he said. It funds research showing obesity is about lack of exercise or other factors. “It’s all about deflecting it away from being about what we eat.

“The very inconvenient truth that nobody wants to talk about is that to resolve the obesity crisis, we need to eat less food. And we need to particularly eat less unhealthy food which generally comes in a packet and has a logo on it and is generally owned by a very large multinational corporation.”

As a nation, we probably need to reduce the total amount of food we consume by 10-20% and we need to reduce the amount of unhealthy food we eat by 20-30%, he believes.

“There are an awful lot of people not very interested in seeing the size of the packaged food industry drop by those kind of figures. The amount of money involved is billions of pounds.” That includes the food industry, the supermarkets, the exchequer and also the media. “Parts are almost entirely propped up by advertising for those unhealthy products. Early Saturday night TV, for example, would struggle without pizzas and fishfingers.”

He thinks his charity can help bridge the gap between the academics and institutions who know about obesity but argue over nutrients and technicalities on the one hand, and the large numbers of people suffering poor health on the other, who are told by the NHS they are in trouble and are in despair – which is the space, he says, between fear and knowledge.

They are not communicating, Parker says, and the messages are uninspiring. He thinks he and his colleagues can change that. “We’ve taken pension plans and credit cards and cars and all sorts of complicated and dull and boring things and we turn them into simple and persuasive messages. That’s what we do.”"
95,US firms vie to create next generation supersonic passenger jets,"Supersonic commercial aviation may be about to make a comeback, as we highlighted last year, with three planes being developed by relatively new companies. Colorado-based Boom Technology is developing a 55-seat Mach 2.2 commercial jet to go on the market by 2023, and has been backed by Japan Airlines and Virgin Group. Boston-based Spike Aerospace is working on an 18-seat business jet capable of Mach 1.6 and Lockheed Martin and Aerion are planning to release a 12-passenger business jet that could reach Mach 1.4. All the planes are being designed to be much quieter than previous supersonic aircraft.",http://www.thedrive.com/sheetmetal/17234/supersonic-travel-is-coming-back-with-a-boom,2018-01-02 13:50:26.550000,"As noted in The Drive earlier this month, Lockheed Martin is now working with Aerion Corporation to develop a 12-passenger business jet that could fly at Mach 1.4—nearly twice the speed of most commercial airliners. The plane, dubbed the AS2, is expected to begin deliveries in 2025 at a reported cost of $120 million. Aerion announced that, with the right atmospheric conditions, it'll be able to fly at speeds approaching Mach 1.2 without a sonic boom reaching the ground.

The jet is in the vanguard of a rebirth in civilian supersonic travel with promises of speed up to Mach 2.2.

Mach is a measure of the speed of sound in air and it generally decreases with altitude. At sea level, the sound barrier (Mach 1) equates to 761 mph, but at 50,000 feet of elevation, it's 660 mph. Though military planes have broken the sound barrier since the late 1940s, civilian supersonic travel took a couple of decades to catch up. It was achieved from 1976 to 2003 by the Concorde, which topped out at Mach 2.04, and by the Soviet Union's similar-looking Tupolev TU-144, which flew from 1977 to 1983 and could reach Mach 2.15.

Earlier supersonic passenger planes failed because of cost and sound levels. They used more fuel and were much more expensive to operate than sub-sonic planes their size. Plus, the sonic boom they created breaking the sound barrier made them abrasively loud to operate over land, leading a ban on commercial flights hitting supersonic speeds over the continental U.S.

There are currently several business jets that can approach Mach 1, including Gulfstream's flagship G650ER, which hits Mach .925, and Cessna's Citation X+, the fastest business jet currently in production with a top speed of Mach .935. Most commercial airliners top out below Mach .9, including Boeing's 747-8, the largest commercial aircraft built in the U.S.

The return of viable supersonic flight is great news for business travelers. It'd make the journey from New York to London or Paris into a viable day trip, for example, and would facilitate business between Asia and North America by knocking more than a dozen hours off round-trip travel times.

If claimed speeds are accurate, the AS2 is actually the slowest of the three supersonic civilian planes currently under development."
96,JPMorgan tool ensures its ads avoid unsavoury YouTube content,"US bank JPMorgan Chase has developed a solution that prevents its ads being placed near unsuitable content on YouTube. A proprietary algorithm with 17 layers of filters that plugs into YouTube's programming system enables JPMorgan to whitelist or pre-approve the channels on which its ads are placed. Launched in October amid dissatisfaction with YouTube's own filters, the in-house software has reduced the number of pre-approved sites from five million to 3,000, with a 99.9% success rate. Aaron Smolick, executive director of paid-media analytics and optimisation, said the problem facing companies wasn't brand safety, but brand appropriateness.
",http://uk.businessinsider.com/jpmorgan-frustrated-youtube-brand-safety-ads-2017-12?r=US&IR=T,2018-01-02 13:47:38.813000,"JPMorgan Chase has created an internal tool to make sure its ads don't end up next to unsavory content on YouTube.

The company's proprietary algorithm plugs into YouTube's API to select ""safe"" channels for it to advertise on.

From more than 5 million channels the brand has winnowed the list down to 3,000 YouTube channels that its ads appear on.

""The model that Google has built to monetize YouTube may work for it, but it doesn't work for us,"" said Aaron Smolick, executive director of paid-media analytics and optimization at JPMorgan Chase.



When some brands' ads ended up next to troubling videos on YouTube last March, JPMorgan Chase responded by pulling its ads from the platform.

Dissatisfied with YouTube's slow response, the bank decided to take matters into its own hands and create an internal tool to make sure that its ads don't end up next to unsavory content on YouTube, only going back to YouTube after testing the tool.

The company developed its own proprietary algorithm in-house, and it plugs into YouTube's application programming interface (API) to select ""safe"" channels for its ads to appear on at scale. The algorithm was built by its internal programmatic and media-buying teams.

""When news broke about ads finding their way next to horrific pieces of content, we paused our efforts and pulled our ads from YouTube,"" Jake Davidow, the executive director of media and channel strategy at JPMorgan Chase, told Business Insider. ""We wanted to figure out a scalable solution and make sure we got it right.""

The technology consists of 17 layers or filters, which allow the bank to separate what it deems as good or safe YouTube channels from the bad or unsafe ones. One of the filters, for example, looks at the total video count on a channel, which automatically sifts out channels with one-off viral videos. The bank also looks at channels' subscriber counts, the general topics channels focus on, language, and even the comments on different channels' videos.

""The model that Google has built to monetize YouTube may work for it, but it doesn't work for us,"" said Aaron Smolick, executive director of paid-media analytics and optimization at JPMorgan Chase. ""The attention of protecting a brand has to fall on the actual people within the brand itself.""

The move reflects an increasing skepticism among major marketers regarding digital advertising, with issues of ad fraud, transparency and brand safety coming to a head in 2017. With their ads ending up next to dicey videos or being viewed by bots instead of humans, many advertisers have begun taking charge themselves by bringing advertising in-house and slashing the number of sites and channels they advertise on.

YouTube in particular has been a conundrum for advertisers. The platform came under fire in 2017 for a spate of incidents where ads ended up next to questionable videos, and brands were not exactly happy with its tackling of the problem. But YouTube is too big for most global advertisers to turn away from, even if they don’t have full confidence in it. Users watch an average of 40 minutes a day on YouTube globally.

The bank started white-listing — or pre-approving sites that its ads run on — in March, culling the number from 400,000 down to 5,000, and about 10,000 at present.

It began working on the YouTube algorithm in August and rolled it out in October. From over 5 million channels, it says that it has winnowed the list down to 3,000 channels on YouTube that its ads appear on. The bank says the algorithm has a success rate of 99.9%. The brand also continues to conduct regular manual checks on its channels as well as develop the tool further to make sure that it is foolproof.

""The biggest lesson for us was that we realized that it wasn't a black-and-white conversation with good guys or bad guys, but a gradient,"" Smolick said. ""It isn't necessarily about brand safety, but rather brand appropriateness. That's the next evolution of the debate, with each brand deciding what's appropriate for them and what's not."""
97,Sweden welcomes automation while the US fears it,"A majority of Swedish people have a positive view of the rise of robots and artificial intelligence, while the majority of Americans are concerned about it. The European Commission published a survey which found that 80% of respondents from Sweden had a positive view of such technology, higher than the European average of 61%, and lower only than Denmark (82%) and the Netherlands (81%). A separate survey from the Pew Research Centre in the United States found that 72% of adults there were worried about the technology.
",https://www.technologyreview.com/the-download/609857/while-us-workers-fear-automation-swedish-employees-welcome-it/,2018-01-02 13:46:41.037000,"Two-thirds of Americans believe robots will soon take over the majority of tasks currently done by humans. Swedes, on the other hand, are not concerned about new technology. “No, I’m afraid of old technology,” the Swedish minister for employment and integration, Ylva Johansson, told the New York Times. “The jobs disappear, and then we train people for new jobs. We won’t protect jobs. But we will protect workers.”

A recent survey by the European Commision found that 80 percent of Swedes have a positive view of robots and AI. Why such enthusiasm? Swedish citizens tend to trust that their government and the companies they work for will take care of them, and they see automation as a way to improve business efficiency. Since Swedish employees actually do benefit from increased profits by getting higher wages, a win for companies is a win for workers.

As the Times points out, the American tendency to worry about robots’ replacing human workers is driven by the severe consequences of losing a job in the U.S. The risk of losing health insurance and a steady income makes people reluctant to leave jobs in favor of new career options or training.

Sweden’s free health care, education, and job transition programs dampen the risk of such undertakings—which may be why people in the country are mostly happy to pay income tax rates of up to nearly 60 percent. The U.S., by contrast, provides almost none of these services. The difference is especially stark in the area of employment assistance: the U.S. spends only about 0.1 percent of GDP on programs designed to help people deal with changes in the workplace (see “The Relentless Pace of Automation”)."
98,"Huawei, Baidu sign AI mobile agreement","Technology firm Huawei has agreed to construct an open mobile artificial intelligence (AI) ecosystem in partnership with Chinese search engine company Baidu. The strategic cooperation will encompass AI technology, internet services, including search, and content ecosystems. The product will be made using the Huawei HiAI platform and corresponding neural network processing unit, as well as Baidu's deep-learning framework Baidu Brain. The partnership will also include the creation of better image and voice recognition for smart devices, as well as the generation of software for augmented reality.
",http://www.zdnet.com/article/huawei-signs-ai-mobile-agreement-with-baidu/,2018-01-02 13:36:18.917000,"Huawei has announced signing a strategic agreement to build an open mobile artificial intelligence (AI) ecosystem with Chinese search engine giant Baidu.

The strategic cooperation agreement covers AI platforms, technology, internet services, and content ecosystems, Huawei said.

The open ecosystem will be built using Huawei's HiAI platform and neural network processing unit (NPU); and Baidu's PaddlePaddle deep-learning framework and Baidu Brain, which contains Baidu's AI services and assets. It will allow AI developers to make use of the technology.

Under the partnership, Baidu and Huawei will also work on improved voice and image recognition on smart devices, and will built a consumer augmented reality (AR) software and hardware ecosystem.

Content and internet services being explored by the two companies will ""strengthen cooperation in areas such as search and feed to bring consumers a wealth of quality content with a more intuitive and convenient service experience"", Huawei added.

""The future is all about smart devices that will actively serve us, not just respond to what we tell them to do,"" said Huawei Consumer Business Group CEO Richard Yu.

""With a strong background in R&D, Huawei will work with Baidu to accelerate innovation in the industry, develop the next generation of smartphones, and provide global consumers with AI that knows you better.""

Baidu CEO Robin Li said the search giant is ""dedicated"" to exploring AI, having last week announced the availability of its AMD EPYC-powered AI, big data, and cloud computing (ABC) services.

Also working on developing autonomous driving and autonomous vehicles, Baidu is hoping to utilise Huawei's large customer base for the mobile AI project.

""Interactive technologies including voice, machine vision, and AI will drive the [mobile phone] industry forward. Originally developed to be personal tools, mobile phones will become a natural extension of the human body and AI-powered assistants for consumers,"" Huawei added.

""Huawei and Baidu will continue to prioritise consumer needs and leverage each other's strengths to form a partnership that benefits everyone.""

Huawei head of Consumer Software Engineering and director of Intelligence Engineering Felix Zhang had last month said the addition of AI capabilities to smartphones will bring the next shift in technology, comparing AI to the advent of steam engines in terms of its capacity to fundamentally change people's lives.

Mobile AI will change two key aspects of the smartphone, he said: User-machine interaction, and ""context-personalised openness"".

The first aspect will improve efficiencies between the user and their phone across text, voice, image, video, and sensors, while the second will actively provide services and aggregated information across apps, content, third-party features, and native features, he explained.

""If you look at the whole ecosystem, the AI will fundamentally change the phone from the smartphone to the intelligent phone,"" Zhang said.

Huawei had unveiled its Kirin 970 chipset with built-in AI in September, at the time calling it the ""future of smartphones"". Its new mobile AI is made up of a combination of on-device AI and cloud AI.

""Huawei is committed to developing smart devices into intelligent devices by building end-to-end capabilities that support coordinated development of chips, devices, and the cloud,"" Yu said at the time.

""The ultimate goal is to provide a significantly better user experience. The Kirin 970 is the first in a series of new advances that will bring powerful AI features to our devices and take them beyond the competition.""

Limitations in cloud AI necessitated improvements across latency, stability, and privacy, Huawei said, with on-device AI providing this as well as adding sensor data to the offering.

Its new flagship smartphones, the Mate 10 and Mate 10 Pro, come kitted out with the AI-focused Kirin processor, which has the dedicated NPU that is able to process 2,000 images per minute via image-recognition technology.

Huawei additionally provided the Kirin 970 as an open platform for mobile AI developers and partners in order to drive further developments.

This followed Huawei saying in August that AI would play a critical role in driving its smartphone innovation, with the tech giant predicting the advent of the ""superphone"" two years ago, saying it would be developed by 2020 and take advantage of advancements in AI, big data, and cloud computing.

Related Coverage

Nokia, Huawei strike multi-year patent licensing deal

Specifics of the deal aren't being disclosed, but Huawei will now have access to a slew of mobile patents.

5G standards approved as tech industry signals accelerated deployment

Networking, technology, and mobile giants worldwide have signalled their intent to accelerate 5G trials, development, and deployment as 3GPP has set NSA 5G New Radio specs.

Android lockdown: Google urges phone makers to support Oreo's rollback protection

Google sheds light on a new Oreo security feature that prevents attackers from downgrading a device to insecure versions of Android.

73 percent of developers who don't use AI plan to learn how in 2018 (TechRepublic)

Only 17 percent of developers worked with artificial intelligence or machine learning in 2017, according to a DigitalOcean report.

How AI will transform the future of work (TechRepublic)

Mikhail Naumov of DigitalGenius explains how machine learning is used by enterprise companies like KLM and BMW to improve customer service.

Mobile device computing policy (Tech Pro Research)

Mobile devices offer convenience and flexibility for the modern workforce-but they also bring associated risks and support issues. This policy establishes guidelines to help ensure safe and productive mobility."
99,Property tech to go mainstream in 2018: RealComm,"2018 will be the breakout year for technology such as artificial intelligence and blockchain in the commercial and corporate real estate (CRE) industry, according to Realcomm CEO Jim Young. Technological trends including robotic automation and augmented reality will continue to become more widely accepted and integrated into the daily routines of CRE organisations throughout 2018, he predicted. Finding ways to adopt emerging technological capabilities into existing business models will be the ""secret to success"" for the industry, Young added.",https://www.realcomm.com/advisory/advisory.asp?AdvisoryID=852,2018-01-02 13:32:24.290000,"Partner Content

Over the past year, the world has changed in many ways – from the need to work remotely to employees asking for more from their employers. Many have asked: is the traditional office dead? Thankfully, our physical office is here to stay. But how will it evolve moving forward?..."
